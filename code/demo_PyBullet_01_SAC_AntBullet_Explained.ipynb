{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SAC on Ant Bullet <font color='grey'> (*Self-Contained*) </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/l5vd5/opt/anaconda3/envs/tsv1/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/Users/l5vd5/opt/anaconda3/envs/tsv1/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/Users/l5vd5/opt/anaconda3/envs/tsv1/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/Users/l5vd5/opt/anaconda3/envs/tsv1/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/Users/l5vd5/opt/anaconda3/envs/tsv1/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/Users/l5vd5/opt/anaconda3/envs/tsv1/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Packaged loaded. TF version is [1.14.0].\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/l5vd5/opt/anaconda3/envs/tsv1/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/Users/l5vd5/opt/anaconda3/envs/tsv1/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/Users/l5vd5/opt/anaconda3/envs/tsv1/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/Users/l5vd5/opt/anaconda3/envs/tsv1/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/Users/l5vd5/opt/anaconda3/envs/tsv1/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/Users/l5vd5/opt/anaconda3/envs/tsv1/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import datetime,gym,os,pybullet_envs,time,os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "np.set_printoptions(precision=2)\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)\n",
    "print (\"Packaged loaded. TF version is [%s].\"%(tf.__version__))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Replay Buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "    \"\"\"\n",
    "    A simple FIFO experience replay buffer for SAC agents.\n",
    "    \"\"\"\n",
    "    def __init__(self, odim, adim, size):\n",
    "        self.obs1_buf = np.zeros([size, odim], dtype=np.float32)\n",
    "        self.obs2_buf = np.zeros([size, odim], dtype=np.float32)\n",
    "        self.acts_buf = np.zeros([size, adim], dtype=np.float32)\n",
    "        self.rews_buf = np.zeros(size, dtype=np.float32)\n",
    "        self.done_buf = np.zeros(size, dtype=np.float32)\n",
    "        self.ptr, self.size, self.max_size = 0, 0, size\n",
    "    def store(self, obs, act, rew, next_obs, done):\n",
    "        self.obs1_buf[self.ptr] = obs\n",
    "        self.obs2_buf[self.ptr] = next_obs\n",
    "        self.acts_buf[self.ptr] = act\n",
    "        self.rews_buf[self.ptr] = rew\n",
    "        self.done_buf[self.ptr] = done\n",
    "        self.ptr = (self.ptr+1) % self.max_size\n",
    "        self.size = min(self.size+1, self.max_size)\n",
    "    def sample_batch(self, batch_size=32):\n",
    "        idxs = np.random.randint(0, self.size, size=batch_size)\n",
    "        return dict(obs1=self.obs1_buf[idxs],\n",
    "                    obs2=self.obs2_buf[idxs],\n",
    "                    acts=self.acts_buf[idxs],\n",
    "                    rews=self.rews_buf[idxs],\n",
    "                    done=self.done_buf[idxs])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Soft Actor Critic Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SAC model ready.\n"
     ]
    }
   ],
   "source": [
    "def create_sac_model(odim=10,adim=2,hdims=[256,256]):\n",
    "    \"\"\"\n",
    "    Soft Actor Critic Model (compatible with Ray)\n",
    "    \"\"\"\n",
    "    import tensorflow as tf # make it compatible with Ray actors\n",
    "    \n",
    "    def mlp(x,hdims=[256,256],actv=tf.nn.relu,out_actv=tf.nn.relu):\n",
    "        ki = tf.truncated_normal_initializer(stddev=0.1)\n",
    "        for hdim in hdims[:-1]:\n",
    "            x = tf.layers.dense(x,units=hdim,activation=actv,kernel_initializer=ki)\n",
    "        return tf.layers.dense(x,units=hdims[-1],activation=out_actv,kernel_initializer=ki)\n",
    "    def gaussian_loglik(x,mu,log_std):\n",
    "        EPS = 1e-8\n",
    "        pre_sum = -0.5*(\n",
    "            ( (x-mu)/(tf.exp(log_std)+EPS) )**2 +\n",
    "            2*log_std + np.log(2*np.pi)\n",
    "        )\n",
    "        return tf.reduce_sum(pre_sum, axis=1)\n",
    "    def mlp_gaussian_policy(o,adim=2,hdims=[256,256],actv=tf.nn.relu):\n",
    "        net = mlp(x=o,hdims=hdims,actv=actv,out_actv=actv) # feature \n",
    "        mu = tf.layers.dense(net,adim,activation=None) # mu\n",
    "        log_std = tf.layers.dense(net,adim,activation=None) # log_std\n",
    "        LOG_STD_MIN,LOG_STD_MAX = -10.0,+2.0\n",
    "        log_std = tf.clip_by_value(log_std, LOG_STD_MIN, LOG_STD_MAX) \n",
    "        std = tf.exp(log_std) # std \n",
    "        pi = mu + tf.random_normal(tf.shape(mu)) * std  # sampled\n",
    "        logp_pi = gaussian_loglik(x=pi,mu=mu,log_std=log_std) # log lik\n",
    "        return mu,pi,logp_pi\n",
    "    def squash_action(mu,pi,logp_pi):\n",
    "        # Squash those unbounded actions\n",
    "        logp_pi -= tf.reduce_sum(2*(np.log(2) - pi -\n",
    "                                    tf.nn.softplus(-2*pi)), axis=1)\n",
    "        mu,pi = tf.tanh(mu),tf.tanh(pi)\n",
    "        return mu, pi, logp_pi\n",
    "    def mlp_actor_critic(o,a,hdims=[256,256],actv=tf.nn.relu,out_actv=None,\n",
    "                         policy=mlp_gaussian_policy):\n",
    "        adim = a.shape.as_list()[-1]\n",
    "        with tf.variable_scope('pi'): # policy\n",
    "            mu,pi,logp_pi = policy(o=o,adim=adim,hdims=hdims,actv=actv)\n",
    "            mu,pi,logp_pi = squash_action(mu=mu,pi=pi,logp_pi=logp_pi)\n",
    "        def vf_mlp(x): return tf.squeeze(\n",
    "            mlp(x=x,hdims=hdims+[1],actv=actv,out_actv=None),axis=1)\n",
    "        with tf.variable_scope('q1'): q1 = vf_mlp( tf.concat([o,a],axis=-1))\n",
    "        with tf.variable_scope('q2'): q2 = vf_mlp( tf.concat([o,a],axis=-1))\n",
    "        return mu,pi,logp_pi,q1,q2\n",
    "    \n",
    "    def placeholder(dim=None):\n",
    "        return tf.placeholder(dtype=tf.float32,shape=(None,dim) if dim else (None,))\n",
    "    def placeholders(*args):\n",
    "        \"\"\"\n",
    "        Usage: a_ph,b_ph,c_ph = placeholders(adim,bdim,None)\n",
    "        \"\"\"\n",
    "        return [placeholder(dim) for dim in args]\n",
    "    def get_vars(scope):\n",
    "        return [x for x in tf.compat.v1.global_variables() if scope in x.name]\n",
    "    \n",
    "    # Have own session\n",
    "    config = tf.ConfigProto()\n",
    "    config.gpu_options.allow_growth = True\n",
    "    sess = tf.Session(config=config)\n",
    "    \n",
    "    # Placeholders\n",
    "    o_ph,a_ph,o2_ph,r_ph,d_ph = placeholders(odim,adim,odim,None,None)\n",
    "    # Actor critic \n",
    "    ac_kwargs = {'hdims':hdims,'actv':tf.nn.relu,'out_actv':None,'policy':mlp_gaussian_policy}\n",
    "    with tf.variable_scope('main'):\n",
    "        mu,pi,logp_pi,q1,q2 = mlp_actor_critic(o=o_ph,a=a_ph,**ac_kwargs)\n",
    "    with tf.variable_scope('main',reuse=True):\n",
    "        _,_,_,q1_pi,q2_pi = mlp_actor_critic(o=o_ph,a=pi,**ac_kwargs)\n",
    "        _,pi_next,logp_pi_next,_,_ = mlp_actor_critic(o=o2_ph,a=a_ph,**ac_kwargs)\n",
    "    # Target value\n",
    "    with tf.variable_scope('target'):\n",
    "        _,_,_,q1_targ,q2_targ = mlp_actor_critic(o=o2_ph,a=pi_next,**ac_kwargs)\n",
    "\n",
    "    # Get variables\n",
    "    main_vars,q_vars,pi_vars,target_vars = \\\n",
    "        get_vars('main'),get_vars('main/q'),get_vars('main/pi'),get_vars('target')\n",
    "    \n",
    "    model = {'o_ph':o_ph,'a_ph':a_ph,'o2_ph':o2_ph,'r_ph':r_ph,'d_ph':d_ph,\n",
    "             'mu':mu,'pi':pi,'logp_pi':logp_pi,'q1':q1,'q2':q2,\n",
    "             'q1_pi':q1_pi,'q2_pi':q2_pi,\n",
    "             'pi_next':pi_next,'logp_pi_next':logp_pi_next,\n",
    "             'q1_targ':q1_targ,'q2_targ':q2_targ,\n",
    "             'main_vars':main_vars,'q_vars':q_vars,'pi_vars':pi_vars,'target_vars':target_vars}\n",
    "        \n",
    "    return model,sess\n",
    "\n",
    "def create_sac_graph(model,lr=1e-3,gamma=0.98,alpha=0.1,polyak=0.995):\n",
    "    \"\"\"\n",
    "    SAC Computational Graph\n",
    "    \"\"\"\n",
    "    # Double Q-learning\n",
    "    min_q_pi = tf.minimum(model['q1_pi'],model['q2_pi'])\n",
    "    min_q_targ = tf.minimum(model['q1_targ'],model['q2_targ'])\n",
    "    \n",
    "    # Entropy-regularized Bellman backup\n",
    "    q_backup = tf.stop_gradient(\n",
    "        model['r_ph'] + \n",
    "        gamma*(1-model['d_ph'])*(min_q_targ - alpha*model['logp_pi_next'])\n",
    "    )\n",
    "    \n",
    "    # Soft actor-critic losses\n",
    "    pi_loss = tf.reduce_mean(alpha*model['logp_pi'] - min_q_pi)\n",
    "    q1_loss = 0.5 * tf.reduce_mean((q_backup - model['q1'])**2)\n",
    "    q2_loss = 0.5 * tf.reduce_mean((q_backup - model['q2'])**2)\n",
    "    value_loss = q1_loss + q2_loss\n",
    "    \n",
    "    # Policy train op\n",
    "    pi_optimizer = tf.train.AdamOptimizer(learning_rate=lr)\n",
    "    train_pi_op = pi_optimizer.minimize(pi_loss,var_list=model['pi_vars'])\n",
    "\n",
    "    # Value train op \n",
    "    value_optimizer = tf.train.AdamOptimizer(learning_rate=lr)\n",
    "    \n",
    "    with tf.control_dependencies([train_pi_op]):\n",
    "        train_value_op = value_optimizer.minimize(value_loss,var_list=model['q_vars'])\n",
    "        \n",
    "    # Polyak averaging for target variables\n",
    "    with tf.control_dependencies([train_value_op]):\n",
    "        target_update = tf.group([tf.assign(v_targ, polyak*v_targ + (1-polyak)*v_main)\n",
    "                                  for v_main, v_targ in \n",
    "                                      zip(model['main_vars'], model['target_vars'])]\n",
    "                                )\n",
    "    \n",
    "    # Tensorboard\n",
    "    pl, pi_loss_update_op = tf.metrics.mean(pi_loss)\n",
    "    vl, val_loss_update_op = tf.metrics.mean(value_loss)\n",
    "    summary = tf.summary.merge([tf.summary.scalar('policy_loss', pl), tf.summary.scalar('value_loss', vl)])\n",
    "\n",
    "    # All ops to call during one training step\n",
    "    step_ops = [pi_loss, q1_loss, q2_loss, model['q1'], model['q2'], model['logp_pi'], model['logp_pi_next'],model['q1_targ'],model['q2_targ'],model['q_vars'], model['pi_vars'],\n",
    "                train_pi_op, train_value_op, target_update]\n",
    "    \n",
    "    # Initializing targets to match main variables\n",
    "    target_init = tf.group([tf.assign(v_targ, v_main)\n",
    "                            for v_main, v_targ in \n",
    "                                zip(model['main_vars'], model['target_vars'])]\n",
    "                          )\n",
    "\n",
    "    return step_ops,target_init, pi_loss_update_op, val_loss_update_op, summary\n",
    "    \n",
    "def get_action(model,sess,o,deterministic=False):\n",
    "    act_op = model['mu'] if deterministic else model['pi']\n",
    "    return sess.run(act_op, feed_dict={model['o_ph']:o.reshape(1,-1)})[0]\n",
    "\n",
    "print (\"SAC model ready.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[AntBulletEnv-v0] ready.\n",
      "odim:[28] adim:[8].\n"
     ]
    }
   ],
   "source": [
    "gym.logger.set_level(40)\n",
    "env_name = 'AntBulletEnv-v0'\n",
    "env,test_env = gym.make(env_name),gym.make(env_name)\n",
    "_ = test_env.render(mode='human') # enable rendering on test_env\n",
    "_ = test_env.reset()\n",
    "for _ in range(3): # dummy run for proper rendering \n",
    "    a = test_env.action_space.sample()\n",
    "    o,r,d,_ = test_env.step(a)\n",
    "    time.sleep(0.01)\n",
    "print (\"[%s] ready.\"%(env_name))\n",
    "observation_space = env.observation_space\n",
    "action_space = env.action_space # -1.0 ~ +1.0\n",
    "odim,adim = observation_space.shape[0],action_space.shape[0]\n",
    "print (\"odim:[%d] adim:[%d].\"%(odim,adim))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize SAC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "model,sess = create_sac_model(odim=odim,adim=adim)\n",
    "step_ops,target_init,pi_loss_update_op,val_loss_update_op,summary = create_sac_graph(model,lr=1e-3,gamma=0.98,alpha=0.1,polyak=0.995)\n",
    "# Replay buffers\n",
    "replay_buffer = ReplayBuffer(odim=odim,adim=adim,size=int(1e6))\n",
    "replay_buffer_short = ReplayBuffer(odim=odim,adim=adim,size=int(1e5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training configuration \n",
    "total_steps,start_steps = 1e6,1e4\n",
    "update_every,update_count,batch_size,max_ep_len_train = 1,2,128,1e3\n",
    "evaluate_every,num_eval,max_ep_len_test = 1e4,3,1e3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fix random seed and initialize the model\n",
    "seed = 0\n",
    "tf.set_random_seed(seed)\n",
    "np.random.seed(seed)    \n",
    "log_path = \"./log/\" + datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "summary_writer = tf.summary.FileWriter(log_path + \"/summary/\", graph=sess.graph)\n",
    "sess.run(tf.global_variables_initializer())\n",
    "sess.run(tf.local_variables_initializer())\n",
    "sess.run(target_init)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Evaluate] step:[10000/1000000][1.0%] time:00:00:09.\n",
      "[Evaluate] [0/3] ep_ret:[683.6030] ep_len:[1000]\n",
      "[Evaluate] [1/3] ep_ret:[666.5510] ep_len:[1000]\n",
      "[Evaluate] [2/3] ep_ret:[724.8439] ep_len:[1000]\n",
      "[Evaluate] step:[20000/1000000][2.0%] time:00:02:33.\n",
      "[Evaluate] [0/3] ep_ret:[625.0801] ep_len:[1000]\n",
      "[Evaluate] [1/3] ep_ret:[562.9352] ep_len:[1000]\n",
      "[Evaluate] [2/3] ep_ret:[627.9405] ep_len:[1000]\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "o,ep_ret,ep_len = env.reset(),0,0\n",
    "pi_loss, q1_loss, q2_loss, q1_val, q2_val, q1_targ, q2_targ, logp_pi_next, logp_pi, feed_dict= 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
    "for t in range(int(total_steps)):\n",
    "    zero_to_one = (t/total_steps)\n",
    "    one_to_zero = 1.0-zero_to_one\n",
    "    esec = time.time()-start_time\n",
    "    \n",
    "    # Get action \n",
    "    if t > start_steps: a = get_action(model,sess,o,deterministic=False)\n",
    "    else: a = env.action_space.sample()\n",
    "        \n",
    "    # Step the env\n",
    "    o2,r,d,_ = env.step(a)\n",
    "    ep_ret += r\n",
    "    ep_len += 1\n",
    "    d = False if ep_len==max_ep_len_train else d # ignore done if it maxed out \n",
    "    \n",
    "    # Store experience to replay buffers\n",
    "    replay_buffer.store(o, a, r, o2, d) # save obs, action, reward, next obs\n",
    "    replay_buffer_short.store(o, a, r, o2, d) # save obs, action, reward, next obs\n",
    "    o = o2 # easy to overlook\n",
    "    \n",
    "    # End of trajectory handling - reset env\n",
    "    if d or (ep_len == max_ep_len_train):\n",
    "        o, ep_ret, ep_len = env.reset(), 0, 0\n",
    "\n",
    "    # Update\n",
    "    if (t>=start_steps) and (t%update_every == 0):\n",
    "        for _ in range(update_count):\n",
    "            batch = replay_buffer.sample_batch(batch_size//2) \n",
    "            batch_short = replay_buffer_short.sample_batch(batch_size//2) \n",
    "            feed_dict = {model['o_ph']: np.concatenate((batch['obs1'],batch_short['obs1'])),\n",
    "                         model['o2_ph']: np.concatenate((batch['obs2'],batch_short['obs2'])),\n",
    "                         model['a_ph']: np.concatenate((batch['acts'],batch_short['acts'])),\n",
    "                         model['r_ph']: np.concatenate((batch['rews'],batch_short['rews'])),\n",
    "                         model['d_ph']: np.concatenate((batch['done'],batch_short['done']))\n",
    "                        }\n",
    "            # Tensorboard\n",
    "            sess.run(pi_loss_update_op, feed_dict=feed_dict)\n",
    "            sess.run(val_loss_update_op, feed_dict=feed_dict)\n",
    "            summary_value = sess.run(summary, feed_dict=feed_dict)\n",
    "            summary_writer.add_summary(summary_value, global_step=t)\n",
    "            outs = sess.run(step_ops,feed_dict=feed_dict) # train \n",
    "            pi_loss, q1_loss, q2_loss, q1_val, q2_val, logp_pi, logp_pi_next, q1_targ, q2_targ, q_vars, pi_vars = outs[0],outs[1],outs[2],outs[3],outs[4],outs[5],outs[6],outs[7],outs[8], outs[9], outs[10]\n",
    "\n",
    "    # Evaluate\n",
    "    if (((t+1)%evaluate_every) == 0): \n",
    "        print (\"[Evaluate] step:[%d/%d][%.1f%%] time:%s.\"%\n",
    "               (t+1,total_steps,zero_to_one*100,\n",
    "                time.strftime(\"%H:%M:%S\", time.gmtime(time.time()-start_time)))\n",
    "              )\n",
    "        #print(pi_loss, q1_loss, q2_loss, q1_val, q2_val, logp_pi, logp_pi_next, q1_targ, q2_targ)\n",
    "        #print('q_vars', q_vars)\n",
    "        #print('pi_vars', pi_vars)\n",
    "        #print(model)\n",
    "\n",
    "\n",
    "        for eval_idx in range(num_eval): \n",
    "            o,d,ep_ret,ep_len = test_env.reset(),False,0,0\n",
    "            _ = test_env.render(mode='human') \n",
    "            while not(d or (ep_len == max_ep_len_test)):\n",
    "                a = get_action(model,sess,o,deterministic=True)\n",
    "                o,r,d,_ = test_env.step(a)\n",
    "                _ = test_env.render(mode='human') \n",
    "                ep_ret += r # compute return \n",
    "                ep_len += 1\n",
    "            print (\"[Evaluate] [%d/%d] ep_ret:[%.4f] ep_len:[%d]\"\n",
    "                %(eval_idx,num_eval,ep_ret,ep_len))\n",
    "    \n",
    "print (\"Done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(q_vars)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Close "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()\n",
    "test_env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gym.logger.set_level(40)\n",
    "env_name = 'AntBulletEnv-v0'\n",
    "test_env = gym.make(env_name)\n",
    "_ = test_env.render(mode='human') # enable rendering on test_env\n",
    "_ = test_env.reset()\n",
    "for _ in range(3): # dummy run for proper rendering \n",
    "    a = test_env.action_space.sample()\n",
    "    o,r,d,_ = test_env.step(a)\n",
    "    time.sleep(0.01)\n",
    "print (\"[%s] ready.\"%(env_name))\n",
    "o,d,ep_ret,ep_len = test_env.reset(),False,0,0\n",
    "_ = test_env.render(mode='human') \n",
    "while not(d or (ep_len == max_ep_len_test)):\n",
    "    a = get_action(model,sess,o,deterministic=True)\n",
    "    o,r,d,_ = test_env.step(a)\n",
    "    _ = test_env.render(mode='human') \n",
    "    ep_ret += r # compute return \n",
    "    ep_len += 1\n",
    "print (\"[Evaluate] ep_ret:[%.4f] ep_len:[%d]\"\n",
    "    %(ep_ret,ep_len))\n",
    "test_env.close() # close env "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Video\n",
    "Video('../vid/SAC_PyBullet_Ant.mp4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "_draft": {
   "nbviewer_url": "https://gist.github.com/3f583727383beb34adca11b6dfea5d4e"
  },
  "gist": {
   "data": {
    "description": "Self-contained SAC on PyBullet Ant",
    "public": true
   },
   "id": "3f583727383beb34adca11b6dfea5d4e"
  },
  "interpreter": {
   "hash": "d6bfe750365b01d3f946bc60445dd9d4b06e4702844b6c71fde1d9a7dc656614"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
