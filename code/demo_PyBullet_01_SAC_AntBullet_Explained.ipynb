{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SAC on Ant Bullet <font color='grey'> (*Self-Contained*) </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Packaged loaded. TF version is [1.14.0].\n"
     ]
    }
   ],
   "source": [
    "import datetime,gym,os,pybullet_envs,time,os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "np.set_printoptions(precision=2)\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)\n",
    "print (\"Packaged loaded. TF version is [%s].\"%(tf.__version__))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Replay Buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "    \"\"\"\n",
    "    A simple FIFO experience replay buffer for SAC agents.\n",
    "    \"\"\"\n",
    "    def __init__(self, odim, adim, size):\n",
    "        self.obs1_buf = np.zeros([size, odim], dtype=np.float32)\n",
    "        self.obs2_buf = np.zeros([size, odim], dtype=np.float32)\n",
    "        self.acts_buf = np.zeros([size, adim], dtype=np.float32)\n",
    "        self.rews_buf = np.zeros(size, dtype=np.float32)\n",
    "        self.done_buf = np.zeros(size, dtype=np.float32)\n",
    "        self.ptr, self.size, self.max_size = 0, 0, size\n",
    "    def store(self, obs, act, rew, next_obs, done):\n",
    "        self.obs1_buf[self.ptr] = obs\n",
    "        self.obs2_buf[self.ptr] = next_obs\n",
    "        self.acts_buf[self.ptr] = act\n",
    "        self.rews_buf[self.ptr] = rew\n",
    "        self.done_buf[self.ptr] = done\n",
    "        self.ptr = (self.ptr+1) % self.max_size\n",
    "        self.size = min(self.size+1, self.max_size)\n",
    "    def sample_batch(self, batch_size=32):\n",
    "        idxs = np.random.randint(0, self.size, size=batch_size)\n",
    "        return dict(obs1=self.obs1_buf[idxs],\n",
    "                    obs2=self.obs2_buf[idxs],\n",
    "                    acts=self.acts_buf[idxs],\n",
    "                    rews=self.rews_buf[idxs],\n",
    "                    done=self.done_buf[idxs])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Soft Actor Critic Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SAC model ready.\n"
     ]
    }
   ],
   "source": [
    "def create_sac_model(odim=10,adim=2,hdims=[256,256]):\n",
    "    \"\"\"\n",
    "    Soft Actor Critic Model (compatible with Ray)\n",
    "    \"\"\"\n",
    "    import tensorflow as tf # make it compatible with Ray actors\n",
    "    \n",
    "    def mlp(x,hdims=[256,256],actv=tf.nn.relu,out_actv=tf.nn.relu):\n",
    "        ki = tf.truncated_normal_initializer(stddev=0.1)\n",
    "        for hdim in hdims[:-1]:\n",
    "            x = tf.layers.dense(x,units=hdim,activation=actv,kernel_initializer=ki)\n",
    "        return tf.layers.dense(x,units=hdims[-1],activation=out_actv,kernel_initializer=ki)\n",
    "    def gaussian_loglik(x,mu,log_std):\n",
    "        EPS = 1e-8\n",
    "        pre_sum = -0.5*(\n",
    "            ( (x-mu)/(tf.exp(log_std)+EPS) )**2 +\n",
    "            2*log_std + np.log(2*np.pi)\n",
    "        )class ReplayBuffer:\n",
    "    \"\"\"\n",
    "    A simple FIFO experience replay buffer for SAC agents.\n",
    "    \"\"\"\n",
    "    def __init__(self, odim, adim, size):\n",
    "        self.obs1_buf = np.zeros([size, odim], dtype=np.float32)\n",
    "        self.obs2_buf = np.zeros([size, odim], dtype=np.float32)\n",
    "        self.acts_buf = np.zeros([size, adim], dtype=np.float32)\n",
    "        self.rews_buf = np.zeros(size, dtype=np.float32)\n",
    "        self.done_buf = np.zeros(size, dtype=np.float32)\n",
    "        self.ptr, self.size, self.max_size = 0, 0, size\n",
    "    def store(self, obs, act, rew, next_obs, done):\n",
    "        self.obs1_buf[self.ptr] = obs\n",
    "        self.obs2_buf[self.ptr] = next_obs\n",
    "        self.acts_buf[self.ptr] = act\n",
    "        self.rews_buf[self.ptr] = rew\n",
    "        self.done_buf[self.ptr] = done\n",
    "        self.ptr = (self.ptr+1) % self.max_size\n",
    "        self.size = min(self.size+1, self.max_size)\n",
    "    def sample_batch(self, batch_size=32):\n",
    "        idxs = np.random.randint(0, self.size, size=batch_size)\n",
    "        return dict(obs1=self.obs1_buf[idxs],\n",
    "                    obs2=self.obs2_buf[idxs],\n",
    "                    acts=self.acts_buf[idxs],\n",
    "                    rews=self.rews_buf[idxs],\n",
    "                    done=self.done_buf[idxs])\n",
    "        return tf.reduce_sum(pre_sum, axis=1)\n",
    "    def mlp_gaussian_policy(o,adim=2,hdims=[256,256],actv=tf.nn.relu):\n",
    "        net = mlp(x=o,hdims=hdims,actv=actv,out_actv=actv) # feature \n",
    "        mu = tf.layers.dense(net,adim,activation=None) # mu\n",
    "        log_std = tf.layers.dense(net,adim,activation=None) # log_std\n",
    "        LOG_STD_MIN,LOG_STD_MAX = -10.0,+2.0\n",
    "        log_std = tf.clip_by_value(log_std, LOG_STD_MIN, LOG_STD_MAX) \n",
    "        std = tf.exp(log_std) # std \n",
    "        pi = mu + tf.random_normal(tf.shape(mu)) * std  # sampled\n",
    "        logp_pi = gaussian_loglik(x=pi,mu=mu,log_std=log_std) # log lik\n",
    "        return mu,pi,logp_pi\n",
    "    def squash_action(mu,pi,logp_pi):\n",
    "        # Squash those unbounded actions\n",
    "        logp_pi -= tf.reduce_sum(2*(np.log(2) - pi -\n",
    "                                    tf.nn.softplus(-2*pi)), axis=1)\n",
    "        mu,pi = tf.tanh(mu),tf.tanh(pi)\n",
    "        return mu, pi, logp_pi\n",
    "    def mlp_actor_critic(o,a,hdims=[256,256],actv=tf.nn.relu,out_actv=None,\n",
    "                         policy=mlp_gaussian_policy):\n",
    "        adim = a.shape.as_list()[-1]\n",
    "        with tf.variable_scope('pi'): # policy\n",
    "            mu,pi,logp_pi = policy(o=o,adim=adim,hdims=hdims,actv=actv)\n",
    "            mu,pi,logp_pi = squash_action(mu=mu,pi=pi,logp_pi=logp_pi)\n",
    "        def vf_mlp(x): return tf.squeeze(\n",
    "            mlp(x=x,hdims=hdims+[1],actv=actv,out_actv=None),axis=1)\n",
    "        with tf.variable_scope('q1'): q1 = vf_mlp( tf.concat([o,a],axis=-1))\n",
    "        with tf.variable_scope('q2'): q2 = vf_mlp( tf.concat([o,a],axis=-1))\n",
    "        return mu,pi,logp_pi,q1,q2\n",
    "    \n",
    "    def placeholder(dim=None):\n",
    "        return tf.placeholder(dtype=tf.float32,shape=(None,dim) if dim else (None,))\n",
    "    def placeholders(*args):\n",
    "        \"\"\"\n",
    "        Usage: a_ph,b_ph,c_ph = placeholders(adim,bdim,None)\n",
    "        \"\"\"\n",
    "        return [placeholder(dim) for dim in args]\n",
    "    def get_vars(scope):\n",
    "        return [x for x in tf.compat.v1.global_variables() if scope in x.name]\n",
    "    \n",
    "    # Have own session\n",
    "    config = tf.ConfigProto()\n",
    "    config.gpu_options.allow_growth = True\n",
    "    sess = tf.Session(config=config)\n",
    "    \n",
    "    # Placeholders\n",
    "    o_ph,a_ph,o2_ph,r_ph,d_ph = placeholders(odim,adim,odim,None,None)\n",
    "    # Actor critic \n",
    "    ac_kwargs = {'hdims':hdims,'actv':tf.nn.relu,'out_actv':None,'policy':mlp_gaussian_policy}\n",
    "    with tf.variable_scope('main'):\n",
    "        mu,pi,logp_pi,q1,q2 = mlp_actor_critic(o=o_ph,a=a_ph,**ac_kwargs)\n",
    "    with tf.variable_scope('main',reuse=True):\n",
    "        _,_,_,q1_pi,q2_pi = mlp_actor_critic(o=o_ph,a=pi,**ac_kwargs)\n",
    "        _,pi_next,logp_pi_next,_,_ = mlp_actor_critic(o=o2_ph,a=a_ph,**ac_kwargs)\n",
    "    # Target value\n",
    "    with tf.variable_scope('target'):\n",
    "        _,_,_,q1_targ,q2_targ = mlp_actor_critic(o=o2_ph,a=pi_next,**ac_kwargs)\n",
    "        \n",
    "    # Get variables\n",
    "    main_vars,q_vars,pi_vars,target_vars = \\\n",
    "        get_vars('main'),get_vars('main/q'),get_vars('main/pi'),get_vars('target')\n",
    "    \n",
    "    model = {'o_ph':o_ph,'a_ph':a_ph,'o2_ph':o2_ph,'r_ph':r_ph,'d_ph':d_ph,\n",
    "             'mu':mu,'pi':pi,'logp_pi':logp_pi,'q1':q1,'q2':q2,\n",
    "             'q1_pi':q1_pi,'q2_pi':q2_pi,\n",
    "             'pi_next':pi_next,'logp_pi_next':logp_pi_next,\n",
    "             'q1_targ':q1_targ,'q2_targ':q2_targ,\n",
    "             'main_vars':main_vars,'q_vars':q_vars,'pi_vars':pi_vars,'target_vars':target_vars}\n",
    "        \n",
    "    return model,sess\n",
    "\n",
    "def create_sac_graph(model,lr=1e-3,gamma=0.98,alpha=0.1,polyak=0.995):\n",
    "    \"\"\"\n",
    "    SAC Computational Graph\n",
    "    \"\"\"\n",
    "    # Double Q-learning\n",
    "    min_q_pi = tf.minimum(model['q1_pi'],model['q2_pi'])\n",
    "    min_q_targ = tf.minimum(model['q1_targ'],model['q2_targ'])\n",
    "    \n",
    "    # Entropy-regularized Bellman backup\n",
    "    q_backup = tf.stop_gradient(\n",
    "        model['r_ph'] + \n",
    "        gamma*(1-model['d_ph'])*(min_q_targ - alpha*model['logp_pi_next'])\n",
    "    )\n",
    "    \n",
    "    # Soft actor-critic losses\n",
    "    pi_loss = tf.reduce_mean(alpha*model['logp_pi'] - min_q_pi)\n",
    "    q1_loss = 0.5 * tf.reduce_mean((q_backup - model['q1'])**2)\n",
    "    q2_loss = 0.5 * tf.reduce_mean((q_backup - model['q2'])**2)\n",
    "    value_loss = q1_loss + q2_loss\n",
    "    \n",
    "    # Policy train op\n",
    "    pi_optimizer = tf.train.AdamOptimizer(learning_rate=lr)\n",
    "    train_pi_op = pi_optimizer.minimize(pi_loss,var_list=model['pi_vars'])\n",
    "    \n",
    "    # Value train op \n",
    "    value_optimizer = tf.train.AdamOptimizer(learning_rate=lr)\n",
    "    with tf.control_dependencies([train_pi_op]):\n",
    "        train_value_op = value_optimizer.minimize(value_loss,var_list=model['q_vars'])\n",
    "        \n",
    "    # Polyak averaging for target variables\n",
    "    with tf.control_dependencies([train_value_op]):\n",
    "        target_update = tf.group([tf.assign(v_targ, polyak*v_targ + (1-polyak)*v_main)\n",
    "                                  for v_main, v_targ in \n",
    "                                      zip(model['main_vars'], model['target_vars'])]\n",
    "                                )\n",
    "    \n",
    "    # All ops to call during one training step\n",
    "    step_ops = [pi_loss, q1_loss, q2_loss, model['q1'], model['q2'], model['logp_pi'],\n",
    "                train_pi_op, train_value_op, target_update]\n",
    "    \n",
    "    # Initializing targets to match main variables\n",
    "    target_init = tf.group([tf.assign(v_targ, v_main)\n",
    "                            for v_main, v_targ in \n",
    "                                zip(model['main_vars'], model['target_vars'])]\n",
    "                          )\n",
    "\n",
    "    return step_ops,target_init\n",
    "    \n",
    "def get_action(model,sess,o,deterministic=False):\n",
    "    act_op = model['mu'] if deterministic else model['pi']\n",
    "    return sess.run(act_op, feed_dict={model['o_ph']:o.reshape(1,-1)})[0]\n",
    "\n",
    "print (\"SAC model ready.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[AntBulletEnv-v0] ready.\n",
      "odim:[28] adim:[8].\n"
     ]
    }
   ],
   "source": [
    "gym.logger.set_level(40)\n",
    "env_name = 'AntBulletEnv-v0'\n",
    "env,test_env = gym.make(env_name),gym.make(env_name)\n",
    "_ = test_env.render(mode='human') # enable rendering on test_env\n",
    "_ = test_env.reset()\n",
    "for _ in range(3): # dummy run for proper rendering \n",
    "    a = test_env.action_space.sample()\n",
    "    o,r,d,_ = test_env.step(a)\n",
    "    time.sleep(0.01)\n",
    "print (\"[%s] ready.\"%(env_name))\n",
    "observation_space = env.observation_space\n",
    "action_space = env.action_space # -1.0 ~ +1.0\n",
    "odim,adim = observation_space.shape[0],action_space.shape[0]\n",
    "print (\"odim:[%d] adim:[%d].\"%(odim,adim))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize SAC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "model,sess = create_sac_model(odim=odim,adim=adim)\n",
    "step_ops,target_init = create_sac_graph(model,lr=1e-3,gamma=0.98,alpha=0.1,polyak=0.995)\n",
    "# Replay buffers\n",
    "replay_buffer = ReplayBuffer(odim=odim,adim=adim,size=int(1e6))\n",
    "replay_buffer_short = ReplayBuffer(odim=odim,adim=adim,size=int(1e5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training configuration \n",
    "total_steps,start_steps = 1e6,1e4\n",
    "update_every,update_count,batch_size,max_ep_len_train = 1,2,128,1e3\n",
    "evaluate_every,num_eval,max_ep_len_test = 1e4,3,1e3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fix random seed and initialize the model\n",
    "seed = 0\n",
    "tf.set_random_seed(seed)\n",
    "np.random.seed(seed)\n",
    "sess.run(tf.global_variables_initializer())\n",
    "sess.run(target_init)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Evaluate] step:[10000/1000000][1.0%] time:00:00:07.\n",
      "[Evaluate] [0/3] ep_ret:[221.1132] ep_len:[1000]\n",
      "[Evaluate] [1/3] ep_ret:[237.8531] ep_len:[1000]\n",
      "[Evaluate] [2/3] ep_ret:[69.1929] ep_len:[180]\n",
      "[Evaluate] step:[20000/1000000][2.0%] time:00:03:15.\n",
      "[Evaluate] [0/3] ep_ret:[547.6536] ep_len:[1000]\n",
      "[Evaluate] [1/3] ep_ret:[561.0090] ep_len:[1000]\n",
      "[Evaluate] [2/3] ep_ret:[575.7181] ep_len:[1000]\n",
      "[Evaluate] step:[30000/1000000][3.0%] time:00:06:23.\n",
      "[Evaluate] [0/3] ep_ret:[488.5191] ep_len:[1000]\n",
      "[Evaluate] [1/3] ep_ret:[506.4437] ep_len:[1000]\n",
      "[Evaluate] [2/3] ep_ret:[419.2753] ep_len:[1000]\n",
      "[Evaluate] step:[40000/1000000][4.0%] time:00:09:32.\n",
      "[Evaluate] [0/3] ep_ret:[539.0713] ep_len:[1000]\n",
      "[Evaluate] [1/3] ep_ret:[777.1833] ep_len:[1000]\n",
      "[Evaluate] [2/3] ep_ret:[517.9417] ep_len:[1000]\n",
      "[Evaluate] step:[50000/1000000][5.0%] time:00:12:40.\n",
      "[Evaluate] [0/3] ep_ret:[733.7249] ep_len:[1000]\n",
      "[Evaluate] [1/3] ep_ret:[806.7745] ep_len:[1000]\n",
      "[Evaluate] [2/3] ep_ret:[698.7821] ep_len:[1000]\n",
      "[Evaluate] step:[60000/1000000][6.0%] time:00:15:49.\n",
      "[Evaluate] [0/3] ep_ret:[865.3736] ep_len:[1000]\n",
      "[Evaluate] [1/3] ep_ret:[787.5916] ep_len:[1000]\n",
      "[Evaluate] [2/3] ep_ret:[813.1506] ep_len:[1000]\n",
      "[Evaluate] step:[70000/1000000][7.0%] time:00:18:57.\n",
      "[Evaluate] [0/3] ep_ret:[806.8718] ep_len:[1000]\n",
      "[Evaluate] [1/3] ep_ret:[802.8945] ep_len:[1000]\n",
      "[Evaluate] [2/3] ep_ret:[732.4886] ep_len:[1000]\n",
      "[Evaluate] step:[80000/1000000][8.0%] time:00:22:06.\n",
      "[Evaluate] [0/3] ep_ret:[734.8379] ep_len:[1000]\n",
      "[Evaluate] [1/3] ep_ret:[796.3818] ep_len:[1000]\n",
      "[Evaluate] [2/3] ep_ret:[734.9004] ep_len:[1000]\n",
      "[Evaluate] step:[90000/1000000][9.0%] time:00:25:16.\n",
      "[Evaluate] [0/3] ep_ret:[822.5432] ep_len:[1000]\n",
      "[Evaluate] [1/3] ep_ret:[787.6301] ep_len:[1000]\n",
      "[Evaluate] [2/3] ep_ret:[730.0119] ep_len:[1000]\n",
      "[Evaluate] step:[100000/1000000][10.0%] time:00:28:24.\n",
      "[Evaluate] [0/3] ep_ret:[839.6137] ep_len:[1000]\n",
      "[Evaluate] [1/3] ep_ret:[945.6401] ep_len:[1000]\n",
      "[Evaluate] [2/3] ep_ret:[943.6320] ep_len:[1000]\n",
      "[Evaluate] step:[110000/1000000][11.0%] time:00:31:33.\n",
      "[Evaluate] [0/3] ep_ret:[848.8228] ep_len:[1000]\n",
      "[Evaluate] [1/3] ep_ret:[955.4163] ep_len:[1000]\n",
      "[Evaluate] [2/3] ep_ret:[935.8035] ep_len:[1000]\n",
      "[Evaluate] step:[120000/1000000][12.0%] time:00:34:42.\n",
      "[Evaluate] [0/3] ep_ret:[942.6585] ep_len:[1000]\n",
      "[Evaluate] [1/3] ep_ret:[867.9932] ep_len:[1000]\n",
      "[Evaluate] [2/3] ep_ret:[951.9670] ep_len:[1000]\n",
      "[Evaluate] step:[130000/1000000][13.0%] time:00:37:51.\n",
      "[Evaluate] [0/3] ep_ret:[986.9257] ep_len:[1000]\n",
      "[Evaluate] [1/3] ep_ret:[927.8071] ep_len:[1000]\n",
      "[Evaluate] [2/3] ep_ret:[1003.4057] ep_len:[1000]\n",
      "[Evaluate] step:[140000/1000000][14.0%] time:00:41:00.\n",
      "[Evaluate] [0/3] ep_ret:[876.6582] ep_len:[1000]\n",
      "[Evaluate] [1/3] ep_ret:[893.0542] ep_len:[1000]\n",
      "[Evaluate] [2/3] ep_ret:[971.5108] ep_len:[1000]\n",
      "[Evaluate] step:[150000/1000000][15.0%] time:00:44:09.\n",
      "[Evaluate] [0/3] ep_ret:[972.3527] ep_len:[1000]\n",
      "[Evaluate] [1/3] ep_ret:[883.4183] ep_len:[1000]\n",
      "[Evaluate] [2/3] ep_ret:[973.8026] ep_len:[1000]\n",
      "[Evaluate] step:[160000/1000000][16.0%] time:00:47:18.\n",
      "[Evaluate] [0/3] ep_ret:[882.1992] ep_len:[1000]\n",
      "[Evaluate] [1/3] ep_ret:[762.8587] ep_len:[1000]\n",
      "[Evaluate] [2/3] ep_ret:[766.7973] ep_len:[1000]\n",
      "[Evaluate] step:[170000/1000000][17.0%] time:00:50:27.\n",
      "[Evaluate] [0/3] ep_ret:[929.9704] ep_len:[1000]\n",
      "[Evaluate] [1/3] ep_ret:[1021.1552] ep_len:[1000]\n",
      "[Evaluate] [2/3] ep_ret:[983.9182] ep_len:[1000]\n",
      "[Evaluate] step:[180000/1000000][18.0%] time:00:53:35.\n",
      "[Evaluate] [0/3] ep_ret:[1123.1069] ep_len:[1000]\n",
      "[Evaluate] [1/3] ep_ret:[1004.6298] ep_len:[1000]\n",
      "[Evaluate] [2/3] ep_ret:[940.0157] ep_len:[1000]\n",
      "[Evaluate] step:[190000/1000000][19.0%] time:00:56:44.\n",
      "[Evaluate] [0/3] ep_ret:[1026.7088] ep_len:[1000]\n",
      "[Evaluate] [1/3] ep_ret:[913.5368] ep_len:[1000]\n",
      "[Evaluate] [2/3] ep_ret:[1117.0515] ep_len:[1000]\n",
      "[Evaluate] step:[200000/1000000][20.0%] time:00:59:53.\n",
      "[Evaluate] [0/3] ep_ret:[1599.2201] ep_len:[1000]\n",
      "[Evaluate] [1/3] ep_ret:[1651.9382] ep_len:[1000]\n",
      "[Evaluate] [2/3] ep_ret:[1632.9364] ep_len:[1000]\n",
      "[Evaluate] step:[210000/1000000][21.0%] time:01:03:01.\n",
      "[Evaluate] [0/3] ep_ret:[1775.3966] ep_len:[1000]\n",
      "[Evaluate] [1/3] ep_ret:[1777.5738] ep_len:[1000]\n",
      "[Evaluate] [2/3] ep_ret:[1772.0957] ep_len:[1000]\n",
      "[Evaluate] step:[220000/1000000][22.0%] time:01:06:09.\n",
      "[Evaluate] [0/3] ep_ret:[1744.2249] ep_len:[1000]\n",
      "[Evaluate] [1/3] ep_ret:[1739.3074] ep_len:[1000]\n",
      "[Evaluate] [2/3] ep_ret:[1738.7729] ep_len:[1000]\n",
      "[Evaluate] step:[230000/1000000][23.0%] time:01:09:16.\n",
      "[Evaluate] [0/3] ep_ret:[1716.5974] ep_len:[1000]\n",
      "[Evaluate] [1/3] ep_ret:[1731.1148] ep_len:[1000]\n",
      "[Evaluate] [2/3] ep_ret:[1752.1153] ep_len:[1000]\n",
      "[Evaluate] step:[240000/1000000][24.0%] time:01:12:24.\n",
      "[Evaluate] [0/3] ep_ret:[1934.1503] ep_len:[1000]\n",
      "[Evaluate] [1/3] ep_ret:[1984.3133] ep_len:[1000]\n",
      "[Evaluate] [2/3] ep_ret:[1960.3207] ep_len:[1000]\n",
      "[Evaluate] step:[250000/1000000][25.0%] time:01:15:32.\n",
      "[Evaluate] [0/3] ep_ret:[2054.7290] ep_len:[1000]\n",
      "[Evaluate] [1/3] ep_ret:[2061.8832] ep_len:[1000]\n",
      "[Evaluate] [2/3] ep_ret:[2076.4639] ep_len:[1000]\n",
      "[Evaluate] step:[260000/1000000][26.0%] time:01:18:40.\n",
      "[Evaluate] [0/3] ep_ret:[1975.0110] ep_len:[1000]\n",
      "[Evaluate] [1/3] ep_ret:[1965.6213] ep_len:[1000]\n",
      "[Evaluate] [2/3] ep_ret:[2015.9368] ep_len:[1000]\n",
      "[Evaluate] step:[270000/1000000][27.0%] time:01:21:48.\n",
      "[Evaluate] [0/3] ep_ret:[2274.4807] ep_len:[1000]\n",
      "[Evaluate] [1/3] ep_ret:[2236.2614] ep_len:[1000]\n",
      "[Evaluate] [2/3] ep_ret:[2199.9503] ep_len:[1000]\n",
      "[Evaluate] step:[280000/1000000][28.0%] time:01:24:55.\n",
      "[Evaluate] [0/3] ep_ret:[2319.3397] ep_len:[1000]\n",
      "[Evaluate] [1/3] ep_ret:[2309.3300] ep_len:[1000]\n",
      "[Evaluate] [2/3] ep_ret:[2289.3763] ep_len:[1000]\n",
      "[Evaluate] step:[290000/1000000][29.0%] time:01:28:03.\n",
      "[Evaluate] [0/3] ep_ret:[2378.9860] ep_len:[1000]\n",
      "[Evaluate] [1/3] ep_ret:[2432.7955] ep_len:[1000]\n",
      "[Evaluate] [2/3] ep_ret:[2396.5112] ep_len:[1000]\n",
      "[Evaluate] step:[300000/1000000][30.0%] time:01:31:10.\n",
      "[Evaluate] [0/3] ep_ret:[2462.0739] ep_len:[1000]\n",
      "[Evaluate] [1/3] ep_ret:[2498.3189] ep_len:[1000]\n",
      "[Evaluate] [2/3] ep_ret:[2447.0756] ep_len:[1000]\n",
      "[Evaluate] step:[310000/1000000][31.0%] time:01:34:18.\n",
      "[Evaluate] [0/3] ep_ret:[2342.4908] ep_len:[1000]\n",
      "[Evaluate] [1/3] ep_ret:[2348.4172] ep_len:[1000]\n",
      "[Evaluate] [2/3] ep_ret:[2330.7192] ep_len:[1000]\n",
      "[Evaluate] step:[320000/1000000][32.0%] time:01:37:25.\n",
      "[Evaluate] [0/3] ep_ret:[2504.6447] ep_len:[1000]\n",
      "[Evaluate] [1/3] ep_ret:[2455.4100] ep_len:[1000]\n",
      "[Evaluate] [2/3] ep_ret:[2471.0521] ep_len:[1000]\n",
      "[Evaluate] step:[330000/1000000][33.0%] time:01:40:32.\n",
      "[Evaluate] [0/3] ep_ret:[2530.5393] ep_len:[1000]\n",
      "[Evaluate] [1/3] ep_ret:[2570.9806] ep_len:[1000]\n",
      "[Evaluate] [2/3] ep_ret:[2422.3089] ep_len:[1000]\n",
      "[Evaluate] step:[340000/1000000][34.0%] time:01:43:39.\n",
      "[Evaluate] [0/3] ep_ret:[2423.0394] ep_len:[1000]\n",
      "[Evaluate] [1/3] ep_ret:[2332.5934] ep_len:[1000]\n",
      "[Evaluate] [2/3] ep_ret:[2443.3195] ep_len:[1000]\n",
      "[Evaluate] step:[350000/1000000][35.0%] time:01:46:46.\n",
      "[Evaluate] [0/3] ep_ret:[2529.0137] ep_len:[1000]\n",
      "[Evaluate] [1/3] ep_ret:[2521.7963] ep_len:[1000]\n",
      "[Evaluate] [2/3] ep_ret:[2489.6490] ep_len:[1000]\n",
      "[Evaluate] step:[360000/1000000][36.0%] time:01:49:54.\n",
      "[Evaluate] [0/3] ep_ret:[2542.2667] ep_len:[1000]\n",
      "[Evaluate] [1/3] ep_ret:[2464.8438] ep_len:[1000]\n",
      "[Evaluate] [2/3] ep_ret:[2550.5877] ep_len:[1000]\n",
      "[Evaluate] step:[370000/1000000][37.0%] time:01:53:00.\n",
      "[Evaluate] [0/3] ep_ret:[2622.0297] ep_len:[1000]\n",
      "[Evaluate] [1/3] ep_ret:[2638.1856] ep_len:[1000]\n",
      "[Evaluate] [2/3] ep_ret:[2640.9279] ep_len:[1000]\n",
      "[Evaluate] step:[380000/1000000][38.0%] time:01:56:07.\n",
      "[Evaluate] [0/3] ep_ret:[326.2049] ep_len:[1000]\n",
      "[Evaluate] [1/3] ep_ret:[224.6879] ep_len:[1000]\n",
      "[Evaluate] [2/3] ep_ret:[2718.2186] ep_len:[1000]\n",
      "[Evaluate] step:[390000/1000000][39.0%] time:01:59:15.\n",
      "[Evaluate] [0/3] ep_ret:[2607.0791] ep_len:[1000]\n",
      "[Evaluate] [1/3] ep_ret:[2620.2067] ep_len:[1000]\n",
      "[Evaluate] [2/3] ep_ret:[2619.5295] ep_len:[1000]\n",
      "[Evaluate] step:[400000/1000000][40.0%] time:02:02:22.\n",
      "[Evaluate] [0/3] ep_ret:[2727.8537] ep_len:[1000]\n",
      "[Evaluate] [1/3] ep_ret:[2732.9427] ep_len:[1000]\n",
      "[Evaluate] [2/3] ep_ret:[2755.4042] ep_len:[1000]\n",
      "[Evaluate] step:[410000/1000000][41.0%] time:02:05:28.\n",
      "[Evaluate] [0/3] ep_ret:[2800.6930] ep_len:[1000]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Evaluate] [1/3] ep_ret:[2767.9099] ep_len:[1000]\n",
      "[Evaluate] [2/3] ep_ret:[2814.7225] ep_len:[1000]\n",
      "[Evaluate] step:[420000/1000000][42.0%] time:02:08:35.\n",
      "[Evaluate] [0/3] ep_ret:[2731.0187] ep_len:[1000]\n",
      "[Evaluate] [1/3] ep_ret:[2634.4614] ep_len:[1000]\n",
      "[Evaluate] [2/3] ep_ret:[2698.0933] ep_len:[1000]\n",
      "[Evaluate] step:[430000/1000000][43.0%] time:02:11:43.\n",
      "[Evaluate] [0/3] ep_ret:[2795.0351] ep_len:[1000]\n",
      "[Evaluate] [1/3] ep_ret:[2772.4156] ep_len:[1000]\n",
      "[Evaluate] [2/3] ep_ret:[2769.1423] ep_len:[1000]\n",
      "[Evaluate] step:[440000/1000000][44.0%] time:02:14:50.\n",
      "[Evaluate] [0/3] ep_ret:[2884.1316] ep_len:[1000]\n",
      "[Evaluate] [1/3] ep_ret:[2894.4042] ep_len:[1000]\n",
      "[Evaluate] [2/3] ep_ret:[2893.8562] ep_len:[1000]\n",
      "[Evaluate] step:[450000/1000000][45.0%] time:02:17:56.\n",
      "[Evaluate] [0/3] ep_ret:[2687.1298] ep_len:[1000]\n",
      "[Evaluate] [1/3] ep_ret:[2703.9625] ep_len:[1000]\n",
      "[Evaluate] [2/3] ep_ret:[2740.3868] ep_len:[1000]\n",
      "[Evaluate] step:[460000/1000000][46.0%] time:02:21:03.\n",
      "[Evaluate] [0/3] ep_ret:[2881.5669] ep_len:[1000]\n",
      "[Evaluate] [1/3] ep_ret:[2849.3727] ep_len:[1000]\n",
      "[Evaluate] [2/3] ep_ret:[2888.2894] ep_len:[1000]\n",
      "[Evaluate] step:[470000/1000000][47.0%] time:02:24:10.\n",
      "[Evaluate] [0/3] ep_ret:[2887.1938] ep_len:[1000]\n",
      "[Evaluate] [1/3] ep_ret:[2881.2499] ep_len:[1000]\n",
      "[Evaluate] [2/3] ep_ret:[2785.7897] ep_len:[1000]\n",
      "[Evaluate] step:[480000/1000000][48.0%] time:02:27:17.\n",
      "[Evaluate] [0/3] ep_ret:[2790.8916] ep_len:[1000]\n",
      "[Evaluate] [1/3] ep_ret:[2850.0546] ep_len:[1000]\n",
      "[Evaluate] [2/3] ep_ret:[2792.0487] ep_len:[1000]\n",
      "[Evaluate] step:[490000/1000000][49.0%] time:02:30:24.\n",
      "[Evaluate] [0/3] ep_ret:[2774.6423] ep_len:[1000]\n",
      "[Evaluate] [1/3] ep_ret:[2797.6177] ep_len:[1000]\n",
      "[Evaluate] [2/3] ep_ret:[2821.1692] ep_len:[1000]\n",
      "[Evaluate] step:[500000/1000000][50.0%] time:02:33:30.\n",
      "[Evaluate] [0/3] ep_ret:[2684.0677] ep_len:[1000]\n",
      "[Evaluate] [1/3] ep_ret:[2780.4873] ep_len:[1000]\n",
      "[Evaluate] [2/3] ep_ret:[2761.5153] ep_len:[1000]\n",
      "[Evaluate] step:[510000/1000000][51.0%] time:02:36:37.\n",
      "[Evaluate] [0/3] ep_ret:[2614.6522] ep_len:[1000]\n",
      "[Evaluate] [1/3] ep_ret:[2724.0797] ep_len:[1000]\n",
      "[Evaluate] [2/3] ep_ret:[2739.8377] ep_len:[1000]\n",
      "[Evaluate] step:[520000/1000000][52.0%] time:02:39:44.\n",
      "[Evaluate] [0/3] ep_ret:[2493.5760] ep_len:[1000]\n",
      "[Evaluate] [1/3] ep_ret:[2585.0546] ep_len:[1000]\n",
      "[Evaluate] [2/3] ep_ret:[2501.1833] ep_len:[1000]\n",
      "[Evaluate] step:[530000/1000000][53.0%] time:02:42:51.\n",
      "[Evaluate] [0/3] ep_ret:[2788.2226] ep_len:[1000]\n",
      "[Evaluate] [1/3] ep_ret:[2826.6497] ep_len:[1000]\n",
      "[Evaluate] [2/3] ep_ret:[2894.0201] ep_len:[1000]\n",
      "[Evaluate] step:[540000/1000000][54.0%] time:02:45:58.\n",
      "[Evaluate] [0/3] ep_ret:[2728.4887] ep_len:[1000]\n",
      "[Evaluate] [1/3] ep_ret:[2755.4414] ep_len:[1000]\n",
      "[Evaluate] [2/3] ep_ret:[2793.6685] ep_len:[1000]\n",
      "[Evaluate] step:[550000/1000000][55.0%] time:02:49:05.\n",
      "[Evaluate] [0/3] ep_ret:[3050.7252] ep_len:[1000]\n",
      "[Evaluate] [1/3] ep_ret:[3039.6540] ep_len:[1000]\n",
      "[Evaluate] [2/3] ep_ret:[3061.2233] ep_len:[1000]\n",
      "[Evaluate] step:[560000/1000000][56.0%] time:02:52:12.\n",
      "[Evaluate] [0/3] ep_ret:[2456.4405] ep_len:[1000]\n",
      "[Evaluate] [1/3] ep_ret:[2535.5897] ep_len:[1000]\n",
      "[Evaluate] [2/3] ep_ret:[2575.0034] ep_len:[1000]\n",
      "[Evaluate] step:[570000/1000000][57.0%] time:02:55:19.\n",
      "[Evaluate] [0/3] ep_ret:[3075.2919] ep_len:[1000]\n",
      "[Evaluate] [1/3] ep_ret:[3090.5839] ep_len:[1000]\n",
      "[Evaluate] [2/3] ep_ret:[3085.1649] ep_len:[1000]\n",
      "[Evaluate] step:[580000/1000000][58.0%] time:02:58:26.\n",
      "[Evaluate] [0/3] ep_ret:[2800.4349] ep_len:[1000]\n",
      "[Evaluate] [1/3] ep_ret:[2816.0106] ep_len:[1000]\n",
      "[Evaluate] [2/3] ep_ret:[2826.6861] ep_len:[1000]\n",
      "[Evaluate] step:[590000/1000000][59.0%] time:03:01:33.\n",
      "[Evaluate] [0/3] ep_ret:[3029.5402] ep_len:[1000]\n",
      "[Evaluate] [1/3] ep_ret:[3033.4485] ep_len:[1000]\n",
      "[Evaluate] [2/3] ep_ret:[3053.8989] ep_len:[1000]\n",
      "[Evaluate] step:[600000/1000000][60.0%] time:03:04:40.\n",
      "[Evaluate] [0/3] ep_ret:[2844.8231] ep_len:[1000]\n",
      "[Evaluate] [1/3] ep_ret:[2922.1770] ep_len:[1000]\n",
      "[Evaluate] [2/3] ep_ret:[2852.0739] ep_len:[1000]\n",
      "[Evaluate] step:[610000/1000000][61.0%] time:03:07:47.\n",
      "[Evaluate] [0/3] ep_ret:[3018.5750] ep_len:[1000]\n",
      "[Evaluate] [1/3] ep_ret:[3051.8516] ep_len:[1000]\n",
      "[Evaluate] [2/3] ep_ret:[2996.7378] ep_len:[1000]\n",
      "[Evaluate] step:[620000/1000000][62.0%] time:03:10:54.\n",
      "[Evaluate] [0/3] ep_ret:[3081.0495] ep_len:[1000]\n",
      "[Evaluate] [1/3] ep_ret:[3082.1108] ep_len:[1000]\n",
      "[Evaluate] [2/3] ep_ret:[3060.8443] ep_len:[1000]\n",
      "[Evaluate] step:[630000/1000000][63.0%] time:03:14:00.\n",
      "[Evaluate] [0/3] ep_ret:[3043.5467] ep_len:[1000]\n",
      "[Evaluate] [1/3] ep_ret:[3103.9086] ep_len:[1000]\n",
      "[Evaluate] [2/3] ep_ret:[3078.3161] ep_len:[1000]\n",
      "[Evaluate] step:[640000/1000000][64.0%] time:03:17:07.\n",
      "[Evaluate] [0/3] ep_ret:[374.4869] ep_len:[1000]\n",
      "[Evaluate] [1/3] ep_ret:[193.3010] ep_len:[1000]\n",
      "[Evaluate] [2/3] ep_ret:[252.3950] ep_len:[1000]\n",
      "[Evaluate] step:[650000/1000000][65.0%] time:03:20:16.\n",
      "[Evaluate] [0/3] ep_ret:[2767.1544] ep_len:[1000]\n",
      "[Evaluate] [1/3] ep_ret:[2722.9460] ep_len:[1000]\n",
      "[Evaluate] [2/3] ep_ret:[2781.0897] ep_len:[1000]\n",
      "[Evaluate] step:[660000/1000000][66.0%] time:03:23:22.\n",
      "[Evaluate] [0/3] ep_ret:[2953.3864] ep_len:[1000]\n",
      "[Evaluate] [1/3] ep_ret:[2923.3179] ep_len:[1000]\n",
      "[Evaluate] [2/3] ep_ret:[2940.2304] ep_len:[1000]\n",
      "[Evaluate] step:[670000/1000000][67.0%] time:03:26:29.\n",
      "[Evaluate] [0/3] ep_ret:[2921.1379] ep_len:[1000]\n",
      "[Evaluate] [1/3] ep_ret:[2936.8713] ep_len:[1000]\n",
      "[Evaluate] [2/3] ep_ret:[2945.2281] ep_len:[1000]\n",
      "[Evaluate] step:[680000/1000000][68.0%] time:03:29:36.\n",
      "[Evaluate] [0/3] ep_ret:[2958.9171] ep_len:[1000]\n",
      "[Evaluate] [1/3] ep_ret:[2938.2562] ep_len:[1000]\n",
      "[Evaluate] [2/3] ep_ret:[2913.2809] ep_len:[1000]\n",
      "[Evaluate] step:[690000/1000000][69.0%] time:03:32:42.\n",
      "[Evaluate] [0/3] ep_ret:[2828.4125] ep_len:[1000]\n",
      "[Evaluate] [1/3] ep_ret:[2827.6043] ep_len:[1000]\n",
      "[Evaluate] [2/3] ep_ret:[2823.1158] ep_len:[1000]\n",
      "[Evaluate] step:[700000/1000000][70.0%] time:03:35:49.\n",
      "[Evaluate] [0/3] ep_ret:[2971.1157] ep_len:[1000]\n",
      "[Evaluate] [1/3] ep_ret:[2983.9015] ep_len:[1000]\n",
      "[Evaluate] [2/3] ep_ret:[2970.5565] ep_len:[1000]\n",
      "[Evaluate] step:[710000/1000000][71.0%] time:03:38:56.\n",
      "[Evaluate] [0/3] ep_ret:[3026.4563] ep_len:[1000]\n",
      "[Evaluate] [1/3] ep_ret:[3008.6178] ep_len:[1000]\n",
      "[Evaluate] [2/3] ep_ret:[3076.4312] ep_len:[1000]\n",
      "[Evaluate] step:[720000/1000000][72.0%] time:03:42:03.\n",
      "[Evaluate] [0/3] ep_ret:[3039.0942] ep_len:[1000]\n",
      "[Evaluate] [1/3] ep_ret:[3029.7595] ep_len:[1000]\n",
      "[Evaluate] [2/3] ep_ret:[3035.6955] ep_len:[1000]\n",
      "[Evaluate] step:[730000/1000000][73.0%] time:03:45:09.\n",
      "[Evaluate] [0/3] ep_ret:[3053.4365] ep_len:[1000]\n",
      "[Evaluate] [1/3] ep_ret:[3069.1829] ep_len:[1000]\n",
      "[Evaluate] [2/3] ep_ret:[3075.1257] ep_len:[1000]\n",
      "[Evaluate] step:[740000/1000000][74.0%] time:03:48:16.\n",
      "[Evaluate] [0/3] ep_ret:[3025.6348] ep_len:[1000]\n",
      "[Evaluate] [1/3] ep_ret:[3006.2009] ep_len:[1000]\n",
      "[Evaluate] [2/3] ep_ret:[3044.3640] ep_len:[1000]\n",
      "[Evaluate] step:[750000/1000000][75.0%] time:03:51:22.\n",
      "[Evaluate] [0/3] ep_ret:[2945.4785] ep_len:[1000]\n",
      "[Evaluate] [1/3] ep_ret:[2938.7286] ep_len:[1000]\n",
      "[Evaluate] [2/3] ep_ret:[2929.5092] ep_len:[1000]\n",
      "[Evaluate] step:[760000/1000000][76.0%] time:03:54:29.\n",
      "[Evaluate] [0/3] ep_ret:[2998.7923] ep_len:[1000]\n",
      "[Evaluate] [1/3] ep_ret:[2923.0124] ep_len:[1000]\n",
      "[Evaluate] [2/3] ep_ret:[2931.3458] ep_len:[1000]\n",
      "[Evaluate] step:[770000/1000000][77.0%] time:03:57:36.\n",
      "[Evaluate] [0/3] ep_ret:[2898.3877] ep_len:[1000]\n",
      "[Evaluate] [1/3] ep_ret:[3002.3588] ep_len:[1000]\n",
      "[Evaluate] [2/3] ep_ret:[2994.1674] ep_len:[1000]\n",
      "[Evaluate] step:[780000/1000000][78.0%] time:04:00:42.\n",
      "[Evaluate] [0/3] ep_ret:[2892.5151] ep_len:[1000]\n",
      "[Evaluate] [1/3] ep_ret:[2891.2796] ep_len:[1000]\n",
      "[Evaluate] [2/3] ep_ret:[2779.7826] ep_len:[1000]\n",
      "[Evaluate] step:[790000/1000000][79.0%] time:04:03:49.\n",
      "[Evaluate] [0/3] ep_ret:[3043.2459] ep_len:[1000]\n",
      "[Evaluate] [1/3] ep_ret:[3106.0058] ep_len:[1000]\n",
      "[Evaluate] [2/3] ep_ret:[3076.0967] ep_len:[1000]\n",
      "[Evaluate] step:[800000/1000000][80.0%] time:04:06:56.\n",
      "[Evaluate] [0/3] ep_ret:[2773.1440] ep_len:[1000]\n",
      "[Evaluate] [1/3] ep_ret:[3021.9524] ep_len:[1000]\n",
      "[Evaluate] [2/3] ep_ret:[3005.8814] ep_len:[1000]\n",
      "[Evaluate] step:[810000/1000000][81.0%] time:04:10:02.\n",
      "[Evaluate] [0/3] ep_ret:[3095.5546] ep_len:[1000]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Evaluate] [1/3] ep_ret:[3104.1164] ep_len:[1000]\n",
      "[Evaluate] [2/3] ep_ret:[3118.8945] ep_len:[1000]\n",
      "[Evaluate] step:[820000/1000000][82.0%] time:04:13:09.\n",
      "[Evaluate] [0/3] ep_ret:[3097.5389] ep_len:[1000]\n",
      "[Evaluate] [1/3] ep_ret:[3120.9309] ep_len:[1000]\n",
      "[Evaluate] [2/3] ep_ret:[3134.3907] ep_len:[1000]\n",
      "[Evaluate] step:[830000/1000000][83.0%] time:04:16:15.\n",
      "[Evaluate] [0/3] ep_ret:[3130.4420] ep_len:[1000]\n",
      "[Evaluate] [1/3] ep_ret:[3138.9736] ep_len:[1000]\n",
      "[Evaluate] [2/3] ep_ret:[3121.8303] ep_len:[1000]\n",
      "[Evaluate] step:[840000/1000000][84.0%] time:04:19:21.\n",
      "[Evaluate] [0/3] ep_ret:[3102.0770] ep_len:[1000]\n",
      "[Evaluate] [1/3] ep_ret:[3122.3888] ep_len:[1000]\n",
      "[Evaluate] [2/3] ep_ret:[3083.5287] ep_len:[1000]\n",
      "[Evaluate] step:[850000/1000000][85.0%] time:04:22:28.\n",
      "[Evaluate] [0/3] ep_ret:[3015.2237] ep_len:[1000]\n",
      "[Evaluate] [1/3] ep_ret:[3004.7884] ep_len:[1000]\n",
      "[Evaluate] [2/3] ep_ret:[3018.9179] ep_len:[1000]\n",
      "[Evaluate] step:[860000/1000000][86.0%] time:04:25:35.\n",
      "[Evaluate] [0/3] ep_ret:[2699.7040] ep_len:[1000]\n",
      "[Evaluate] [1/3] ep_ret:[2597.9569] ep_len:[1000]\n",
      "[Evaluate] [2/3] ep_ret:[1279.5998] ep_len:[1000]\n",
      "[Evaluate] step:[870000/1000000][87.0%] time:04:28:42.\n",
      "[Evaluate] [0/3] ep_ret:[3005.1034] ep_len:[1000]\n",
      "[Evaluate] [1/3] ep_ret:[3039.5923] ep_len:[1000]\n",
      "[Evaluate] [2/3] ep_ret:[2976.6859] ep_len:[1000]\n",
      "[Evaluate] step:[880000/1000000][88.0%] time:04:31:49.\n",
      "[Evaluate] [0/3] ep_ret:[2832.3516] ep_len:[1000]\n",
      "[Evaluate] [1/3] ep_ret:[2896.6123] ep_len:[1000]\n",
      "[Evaluate] [2/3] ep_ret:[2785.8768] ep_len:[1000]\n",
      "[Evaluate] step:[890000/1000000][89.0%] time:04:34:56.\n",
      "[Evaluate] [0/3] ep_ret:[2941.7177] ep_len:[1000]\n",
      "[Evaluate] [1/3] ep_ret:[2944.9556] ep_len:[1000]\n",
      "[Evaluate] [2/3] ep_ret:[2937.3235] ep_len:[1000]\n",
      "[Evaluate] step:[900000/1000000][90.0%] time:04:38:02.\n",
      "[Evaluate] [0/3] ep_ret:[2955.2834] ep_len:[1000]\n",
      "[Evaluate] [1/3] ep_ret:[2961.9337] ep_len:[1000]\n",
      "[Evaluate] [2/3] ep_ret:[2961.4896] ep_len:[1000]\n",
      "[Evaluate] step:[910000/1000000][91.0%] time:04:41:09.\n",
      "[Evaluate] [0/3] ep_ret:[2911.9512] ep_len:[1000]\n",
      "[Evaluate] [1/3] ep_ret:[848.6582] ep_len:[1000]\n",
      "[Evaluate] [2/3] ep_ret:[2245.3007] ep_len:[1000]\n",
      "[Evaluate] step:[920000/1000000][92.0%] time:04:44:16.\n",
      "[Evaluate] [0/3] ep_ret:[2932.3025] ep_len:[1000]\n",
      "[Evaluate] [1/3] ep_ret:[874.3360] ep_len:[1000]\n",
      "[Evaluate] [2/3] ep_ret:[1271.6672] ep_len:[1000]\n",
      "[Evaluate] step:[930000/1000000][93.0%] time:04:47:23.\n",
      "[Evaluate] [0/3] ep_ret:[2997.3545] ep_len:[1000]\n",
      "[Evaluate] [1/3] ep_ret:[3011.1314] ep_len:[1000]\n",
      "[Evaluate] [2/3] ep_ret:[3091.9446] ep_len:[1000]\n",
      "[Evaluate] step:[940000/1000000][94.0%] time:04:50:29.\n",
      "[Evaluate] [0/3] ep_ret:[3006.8954] ep_len:[1000]\n",
      "[Evaluate] [1/3] ep_ret:[3085.7447] ep_len:[1000]\n",
      "[Evaluate] [2/3] ep_ret:[2912.6144] ep_len:[1000]\n",
      "[Evaluate] step:[950000/1000000][95.0%] time:04:53:36.\n",
      "[Evaluate] [0/3] ep_ret:[3064.4234] ep_len:[1000]\n",
      "[Evaluate] [1/3] ep_ret:[3116.6788] ep_len:[1000]\n",
      "[Evaluate] [2/3] ep_ret:[3025.3281] ep_len:[1000]\n",
      "[Evaluate] step:[960000/1000000][96.0%] time:04:56:44.\n",
      "[Evaluate] [0/3] ep_ret:[993.8013] ep_len:[1000]\n",
      "[Evaluate] [1/3] ep_ret:[1515.1538] ep_len:[1000]\n",
      "[Evaluate] [2/3] ep_ret:[1500.1539] ep_len:[1000]\n",
      "[Evaluate] step:[970000/1000000][97.0%] time:04:59:51.\n",
      "[Evaluate] [0/3] ep_ret:[3101.6089] ep_len:[1000]\n",
      "[Evaluate] [1/3] ep_ret:[3072.0320] ep_len:[1000]\n",
      "[Evaluate] [2/3] ep_ret:[2993.3806] ep_len:[1000]\n",
      "[Evaluate] step:[980000/1000000][98.0%] time:05:02:58.\n",
      "[Evaluate] [0/3] ep_ret:[2969.9790] ep_len:[1000]\n",
      "[Evaluate] [1/3] ep_ret:[3108.9021] ep_len:[1000]\n",
      "[Evaluate] [2/3] ep_ret:[3076.9554] ep_len:[1000]\n",
      "[Evaluate] step:[990000/1000000][99.0%] time:05:06:05.\n",
      "[Evaluate] [0/3] ep_ret:[3128.3681] ep_len:[1000]\n",
      "[Evaluate] [1/3] ep_ret:[3104.4091] ep_len:[1000]\n",
      "[Evaluate] [2/3] ep_ret:[3147.6089] ep_len:[1000]\n",
      "[Evaluate] step:[1000000/1000000][100.0%] time:05:09:12.\n",
      "[Evaluate] [0/3] ep_ret:[3129.8969] ep_len:[1000]\n",
      "[Evaluate] [1/3] ep_ret:[3111.1036] ep_len:[1000]\n",
      "[Evaluate] [2/3] ep_ret:[3149.7378] ep_len:[1000]\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "o,ep_ret,ep_len = env.reset(),0,0\n",
    "for t in range(int(total_steps)):\n",
    "    zero_to_one = (t/total_steps)\n",
    "    one_to_zero = 1.0-zero_to_one\n",
    "    esec = time.time()-start_time\n",
    "    \n",
    "    # Get action \n",
    "    if t > start_steps: a = get_action(model,sess,o,deterministic=False)\n",
    "    else: a = env.action_space.sample()\n",
    "        \n",
    "    # Step the env\n",
    "    o2,r,d,_ = env.step(a)\n",
    "    ep_ret += r\n",
    "    ep_len += 1\n",
    "    d = False if ep_len==max_ep_len_train else d # ignore done if it maxed out \n",
    "    \n",
    "    # Store experience to replay buffers\n",
    "    replay_buffer.store(o, a, r, o2, d) # save obs, action, reward, next obs\n",
    "    replay_buffer_short.store(o, a, r, o2, d) # save obs, action, reward, next obs\n",
    "    o = o2 # easy to overlook\n",
    "    \n",
    "    # End of trajectory handling - reset env\n",
    "    if d or (ep_len == max_ep_len_train):\n",
    "        o, ep_ret, ep_len = env.reset(), 0, 0\n",
    "    \n",
    "    # Update\n",
    "    if (t>=start_steps) and (t%update_every == 0):\n",
    "        for _ in range(update_count):\n",
    "            batch = replay_buffer.sample_batch(batch_size//2) \n",
    "            batch_short = replay_buffer_short.sample_batch(batch_size//2) \n",
    "            feed_dict = {model['o_ph']: np.concatenate((batch['obs1'],batch_short['obs1'])),\n",
    "                         model['o2_ph']: np.concatenate((batch['obs2'],batch_short['obs2'])),\n",
    "                         model['a_ph']: np.concatenate((batch['acts'],batch_short['acts'])),\n",
    "                         model['r_ph']: np.concatenate((batch['rews'],batch_short['rews'])),\n",
    "                         model['d_ph']: np.concatenate((batch['done'],batch_short['done']))\n",
    "                        }\n",
    "            outs = sess.run(step_ops,feed_dict=feed_dict) # train \n",
    "            q1_val,q2_val = outs[3],outs[4]\n",
    "            \n",
    "    # Evaluate\n",
    "    if (((t+1)%evaluate_every) == 0): \n",
    "        print (\"[Evaluate] step:[%d/%d][%.1f%%] time:%s.\"%\n",
    "               (t+1,total_steps,zero_to_one*100,\n",
    "                time.strftime(\"%H:%M:%S\", time.gmtime(time.time()-start_time)))\n",
    "              )\n",
    "        for eval_idx in range(num_eval): \n",
    "            o,d,ep_ret,ep_len = test_env.reset(),False,0,0\n",
    "            _ = test_env.render(mode='human') \n",
    "            while not(d or (ep_len == max_ep_len_test)):\n",
    "                a = get_action(model,sess,o,deterministic=True)\n",
    "                o,r,d,_ = test_env.step(a)\n",
    "                _ = test_env.render(mode='human') \n",
    "                ep_ret += r # compute return \n",
    "                ep_len += 1\n",
    "            print (\"[Evaluate] [%d/%d] ep_ret:[%.4f] ep_len:[%d]\"\n",
    "                %(eval_idx,num_eval,ep_ret,ep_len))\n",
    "    \n",
    "print (\"Done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Close "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()\n",
    "test_env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[AntBulletEnv-v0] ready.\n",
      "[Evaluate] ep_ret:[2.0000] ep_len:[1000]\n"
     ]
    }
   ],
   "source": [
    "gym.logger.set_level(40)\n",
    "env_name = 'AntBulletEnv-v0'\n",
    "test_env = gym.make(env_name)\n",
    "_ = test_env.render(mode='human') # enable rendering on test_env\n",
    "_ = test_env.reset()\n",
    "for _ in range(3): # dummy run for proper rendering \n",
    "    a = test_env.action_space.sample()\n",
    "    o,r,d,_ = test_env.step(a)\n",
    "    time.sleep(0.01)\n",
    "print (\"[%s] ready.\"%(env_name))\n",
    "o,d,ep_ret,ep_len = test_env.reset(),False,0,0\n",
    "_ = test_env.render(mode='human') \n",
    "while not(d or (ep_len == max_ep_len_test)):\n",
    "    a = get_action(model,sess,o,deterministic=True)\n",
    "    o,r,d,_ = test_env.step(a)\n",
    "    _ = test_env.render(mode='human') \n",
    "    ep_ret += r # compute return \n",
    "    ep_len += 1\n",
    "print (\"[Evaluate] ep_ret:[%.4f] ep_len:[%d]\"\n",
    "    %(ep_ret,ep_len))\n",
    "test_env.close() # close env "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<video src=\"../vid/SAC_PyBullet_Ant.mp4\" controls  >\n",
       "      Your browser does not support the <code>video</code> element.\n",
       "    </video>"
      ],
      "text/plain": [
       "<IPython.core.display.Video object>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import Video\n",
    "Video('../vid/SAC_PyBullet_Ant.mp4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "_draft": {
   "nbviewer_url": "https://gist.github.com/3f583727383beb34adca11b6dfea5d4e"
  },
  "gist": {
   "data": {
    "description": "Self-contained SAC on PyBullet Ant",
    "public": true
   },
   "id": "3f583727383beb34adca11b6dfea5d4e"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
