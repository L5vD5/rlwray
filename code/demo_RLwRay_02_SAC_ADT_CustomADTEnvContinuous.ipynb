{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SAC with CustomADTEnvContinuous"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Packaged loaded. TF version is [1.15.0].\n"
     ]
    }
   ],
   "source": [
    "import datetime,gym,time,os,psutil,ray\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from util import gpu_sess,suppress_tf_warning,tic,toc\n",
    "from sac import ReplayBuffer,create_sac_model,create_sac_graph,\\\n",
    "    save_sac_model_and_buffers,restore_sac_model_and_buffers\n",
    "np.set_printoptions(precision=2)\n",
    "suppress_tf_warning() # suppress warning \n",
    "gym.logger.set_level(40) # gym logger \n",
    "\n",
    "from episci.environment_wrappers.tactical_action_adt_env_continuous import CustomADTEnvContinuous\n",
    "from episci.agents.utils.constants import Agents, RewardType\n",
    "print (\"Packaged loaded. TF version is [%s].\"%(tf.__version__))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rollout Worker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_env():\n",
    "    from episci.environment_wrappers.tactical_action_adt_env_continuous import CustomADTEnvContinuous\n",
    "    from episci.agents.utils.constants import Agents, RewardType\n",
    "    \n",
    "    red_distribution = {\n",
    "        Agents.SPOT_4G: 0.15,\n",
    "        Agents.SPOT_5G: 0.30,\n",
    "        Agents.SPOT_RANDOM: 0.45,\n",
    "        Agents.EXPERT_SYSTEM_TRIAL_2: 0.6,\n",
    "        Agents.EXPERT_SYSTEM_TRIAL_3_SCRIMMAGE_4: 0.75,\n",
    "        Agents.EXPERT_SYSTEM: 1.0\n",
    "    }\n",
    "    env_config = {\n",
    "        \"red_distribution\": red_distribution,\n",
    "        \"reward_type\": RewardType.SHAPED\n",
    "    }\n",
    "    return CustomADTEnvContinuous(env_config)\n",
    "\n",
    "def get_eval_env():\n",
    "    # from episci.environment_wrappers.tactical_action_adt_env_continuous import CustomADTEnvContinuous\n",
    "    # from episci.agents.utils.constants import Agents, RewardType\n",
    "    red_distribution = {\n",
    "        Agents.SPOT_4G: 0.15,\n",
    "        Agents.SPOT_5G: 0.30,\n",
    "        Agents.SPOT_RANDOM: 0.45,\n",
    "        Agents.EXPERT_SYSTEM_TRIAL_2: 0.6,\n",
    "        Agents.EXPERT_SYSTEM_TRIAL_3_SCRIMMAGE_4: 0.75,\n",
    "        Agents.EXPERT_SYSTEM: 1.0\n",
    "    }\n",
    "    env_config = {\n",
    "        \"red_distribution\": red_distribution,\n",
    "        \"reward_type\": RewardType.SHAPED\n",
    "    }\n",
    "    return CustomADTEnvContinuous(env_config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rollout worker classes (with and without RAY) ready.\n"
     ]
    }
   ],
   "source": [
    "class RolloutWorkerClass(object):\n",
    "    \"\"\"\n",
    "    Worker without RAY (for update purposes)\n",
    "    \"\"\"\n",
    "    def __init__(self,hdims=[256,256],actv=tf.nn.relu,\n",
    "                 lr=1e-3,gamma=0.99,alpha=0.1,polyak=0.995,seed=1):\n",
    "        self.seed = seed\n",
    "        # Each worker should maintain its own environment\n",
    "        import gym\n",
    "        from util import suppress_tf_warning\n",
    "        suppress_tf_warning() # suppress TF warnings\n",
    "        gym.logger.set_level(40) \n",
    "        self.env = get_eval_env()\n",
    "        odim,adim = self.env.observation_space.shape[0],self.env.action_space.shape[0]\n",
    "        self.odim = odim\n",
    "        self.adim = adim\n",
    "        _ = self.env.reset()\n",
    "        \n",
    "        # Create SAC model and computational graph \n",
    "        self.model,self.sess = create_sac_model(\n",
    "            odim=self.odim,adim=self.adim,hdims=hdims,actv=actv)\n",
    "        self.step_ops,self.target_init = \\\n",
    "            create_sac_graph(self.model,lr=lr,gamma=gamma,alpha=alpha,polyak=polyak)\n",
    "        \n",
    "        # Initialize model \n",
    "        tf.set_random_seed(self.seed)\n",
    "        np.random.seed(self.seed)\n",
    "        self.sess.run(tf.global_variables_initializer())\n",
    "        self.sess.run(self.target_init)\n",
    "    \n",
    "    def get_action(self,o,deterministic=False):\n",
    "        act_op = self.model['mu'] if deterministic else self.model['pi']\n",
    "        return self.sess.run(act_op, feed_dict={self.model['o_ph']:o.reshape(1,-1)})[0]\n",
    "\n",
    "    def get_weights(self):\n",
    "        \"\"\"\n",
    "        Get weights\n",
    "        \"\"\"\n",
    "        weight_vals = self.sess.run(self.model['main_vars'])\n",
    "        return weight_vals\n",
    "    \n",
    "@ray.remote\n",
    "class RayRolloutWorkerClass(object):\n",
    "    \"\"\"\n",
    "    Rollout Worker with RAY\n",
    "    \"\"\"\n",
    "    def __init__(self,worker_id=0,hdims=[256,256],actv=tf.nn.relu,\n",
    "                 ep_len_rollout=1000):\n",
    "        # Parse\n",
    "        self.worker_id = worker_id\n",
    "        self.ep_len_rollout = ep_len_rollout\n",
    "        # Each worker should maintain its own environment\n",
    "        import gym\n",
    "        from util import suppress_tf_warning\n",
    "        suppress_tf_warning() # suppress TF warnings\n",
    "        gym.logger.set_level(40) \n",
    "        self.env = get_env()\n",
    "        odim,adim = self.env.observation_space.shape[0],self.env.action_space.shape[0]\n",
    "        self.odim = odim\n",
    "        self.adim = adim\n",
    "        _ = self.env.reset()\n",
    "        \n",
    "        # Replay buffers to pass\n",
    "        self.o_buffer = np.zeros((self.ep_len_rollout,self.odim))\n",
    "        self.a_buffer = np.zeros((self.ep_len_rollout,self.adim))\n",
    "        self.r_buffer = np.zeros((self.ep_len_rollout))\n",
    "        self.o2_buffer = np.zeros((self.ep_len_rollout,self.odim))\n",
    "        self.d_buffer = np.zeros((self.ep_len_rollout))\n",
    "        \n",
    "        # Create SAC model\n",
    "        self.model,self.sess = create_sac_model(\n",
    "            odim=self.odim,adim=self.adim,hdims=hdims,actv=actv)\n",
    "        self.sess.run(tf.global_variables_initializer())\n",
    "        print (\"Ray Worker [%d] Ready.\"%(self.worker_id))\n",
    "        \n",
    "        # Flag to initialize assign operations for 'set_weights()'\n",
    "        self.FIRST_SET_FLAG = True\n",
    "        \n",
    "        # Flag to initialize rollout\n",
    "        self.FIRST_ROLLOUT_FLAG = True\n",
    "        \n",
    "    def get_action(self,o,deterministic=False):\n",
    "        act_op = self.model['mu'] if deterministic else self.model['pi']\n",
    "        return self.sess.run(act_op, feed_dict={self.model['o_ph']:o.reshape(1,-1)})[0]\n",
    "    \n",
    "    def set_weights(self,weight_vals):\n",
    "        \"\"\"\n",
    "        Set weights without memory leakage\n",
    "        \"\"\"\n",
    "        if self.FIRST_SET_FLAG:\n",
    "            self.FIRST_SET_FLAG = False\n",
    "            self.assign_placeholders = []\n",
    "            self.assign_ops = []\n",
    "            for w_idx,weight_tf_var in enumerate(self.model['main_vars']):\n",
    "                a = weight_tf_var\n",
    "                assign_placeholder = tf.placeholder(a.dtype, shape=a.get_shape())\n",
    "                assign_op = a.assign(assign_placeholder)\n",
    "                self.assign_placeholders.append(assign_placeholder)\n",
    "                self.assign_ops.append(assign_op)\n",
    "        for w_idx,weight_tf_var in enumerate(self.model['main_vars']):\n",
    "            # Memory-leakage-free assign (hopefully)\n",
    "            self.sess.run(self.assign_ops[w_idx],\n",
    "                          {self.assign_placeholders[w_idx]:weight_vals[w_idx]})\n",
    "            \n",
    "    def rollout(self):\n",
    "        \"\"\"\n",
    "        Rollout\n",
    "        \"\"\"\n",
    "        if self.FIRST_ROLLOUT_FLAG:\n",
    "            self.FIRST_ROLLOUT_FLAG = False\n",
    "            self.o = self.env.reset() # reset environment\n",
    "        # Loop\n",
    "        for t in range(ep_len_rollout):\n",
    "            self.a = self.get_action(self.o,deterministic=False) \n",
    "            self.o2,self.r,self.d,_ = self.env.step(self.a)\n",
    "            # Append\n",
    "            self.o_buffer[t,:] = self.o\n",
    "            self.a_buffer[t,:] = self.a\n",
    "            self.r_buffer[t] = self.r\n",
    "            self.o2_buffer[t,:] = self.o2\n",
    "            self.d_buffer[t] = self.d\n",
    "            # Save next state \n",
    "            self.o = self.o2\n",
    "            if self.d: self.o = self.env.reset() # reset when done \n",
    "        return self.o_buffer,self.a_buffer,self.r_buffer,self.o2_buffer,self.d_buffer\n",
    "    \n",
    "    def evaluate(self,red=None):\n",
    "        \"\"\"\n",
    "        Evaluate\n",
    "        \"\"\"\n",
    "        o,d,ep_ret,ep_len = self.env.reset(red=red),False,0,0\n",
    "        while not(d or (ep_len == max_ep_len_eval)):\n",
    "            a = self.get_action(o,deterministic=True)\n",
    "            o,r,d,_ = self.env.step(a)\n",
    "            ep_ret += r # compute return \n",
    "            ep_len += 1\n",
    "        blue_health,red_health = self.env.blue_health,self.env.red_health\n",
    "        eval_res = [ep_ret,ep_len,blue_health,red_health] # evaluation result \n",
    "        return eval_res\n",
    "    \n",
    "print (\"Rollout worker classes (with and without RAY) ready.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_cpu = 11\n",
    "n_workers = 10\n",
    "total_steps,evaluate_every,print_every = 50000,50,10\n",
    "ep_len_rollout = 50*30 # 30sec rollout\n",
    "hdims,actv = [64,32,16],tf.nn.relu # tf.nn.relu\n",
    "red_list = [Agents.SPOT_4G,Agents.SPOT_5G,Agents.SPOT_RANDOM,\n",
    "            Agents.EXPERT_SYSTEM_TRIAL_2,Agents.EXPERT_SYSTEM_TRIAL_3_SCRIMMAGE_4,\n",
    "            Agents.EXPERT_SYSTEM]\n",
    "num_eval,max_ep_len_eval = len(red_list),15e3 # evaluation \n",
    "# Learning hyp\n",
    "batch_size,update_count = 4096,50*30 # batchsize / number of updates\n",
    "lr = 1e-4 # 1e-3\n",
    "# SAC\n",
    "gamma = 0.99 # discount 0.99\n",
    "alpha = 0.01 # 0.1\n",
    "polyak = 0.99 # 0.995"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize Workers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-07-03 03:50:35,357\tWARNING services.py:586 -- setpgrp failed, processes may not be cleaned up properly: [Errno 1] Operation not permitted.\n",
      "2020-07-03 03:50:35,399\tINFO resource_spec.py:212 -- Starting Ray with 4.98 GiB memory available for workers and up to 10.0 GiB for objects. You can adjust these settings with ray.init(memory=<bytes>, object_store_memory=<bytes>).\n",
      "2020-07-03 03:50:35,769\tINFO services.py:1078 -- View the Ray dashboard at \u001b[1m\u001b[32mlocalhost:8266\u001b[39m\u001b[22m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RAY initialized with [11] cpus and [10] workers.\n"
     ]
    }
   ],
   "source": [
    "ray.init(num_cpus=n_cpu,\n",
    "         memory = 5*1024*1024*1024,\n",
    "         object_store_memory = 10*1024*1024*1024,\n",
    "         driver_object_store_memory = 5*1024*1024*1024)\n",
    "tf.reset_default_graph()\n",
    "R = RolloutWorkerClass(hdims=hdims,actv=actv,\n",
    "                       lr=lr,gamma=gamma,alpha=alpha,polyak=polyak,seed=0)\n",
    "workers = [RayRolloutWorkerClass.remote(worker_id=i,hdims=hdims,actv=actv,\n",
    "                                        ep_len_rollout=ep_len_rollout) \n",
    "           for i in range(n_workers)]\n",
    "print (\"RAY initialized with [%d] cpus and [%d] workers.\"%\n",
    "       (n_cpu,n_workers))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "time.sleep(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Replay Buffers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "replay_buffer_long = ReplayBuffer(odim=R.odim,adim=R.adim,size=int(1e5))\n",
    "replay_buffer_short = ReplayBuffer(odim=R.odim,adim=R.adim,size=int(1e5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=392)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=392)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=392)\u001b[0m      JSBSim Flight Dynamics Model v1.1.0.dev1 Jun 16 2020 00:16:24\n",
      "\u001b[2m\u001b[36m(pid=401)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=401)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=401)\u001b[0m      JSBSim Flight Dynamics Model v1.1.0.dev1 Jun 16 2020 00:16:24\n",
      "\u001b[2m\u001b[36m(pid=400)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=400)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=400)\u001b[0m      JSBSim Flight Dynamics Model v1.1.0.dev1 Jun 16 2020 00:16:24\n",
      "\u001b[2m\u001b[36m(pid=397)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=397)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=397)\u001b[0m      JSBSim Flight Dynamics Model v1.1.0.dev1 Jun 16 2020 00:16:24\n",
      "\u001b[2m\u001b[36m(pid=391)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=391)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=391)\u001b[0m      JSBSim Flight Dynamics Model v1.1.0.dev1 Jun 16 2020 00:16:24\n",
      "\u001b[2m\u001b[36m(pid=403)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=403)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=403)\u001b[0m      JSBSim Flight Dynamics Model v1.1.0.dev1 Jun 16 2020 00:16:24\n",
      "\u001b[2m\u001b[36m(pid=398)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=398)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=398)\u001b[0m      JSBSim Flight Dynamics Model v1.1.0.dev1 Jun 16 2020 00:16:24\n",
      "\u001b[2m\u001b[36m(pid=396)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=396)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=396)\u001b[0m      JSBSim Flight Dynamics Model v1.1.0.dev1 Jun 16 2020 00:16:24\n",
      "\u001b[2m\u001b[36m(pid=393)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=393)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=393)\u001b[0m      JSBSim Flight Dynamics Model v1.1.0.dev1 Jun 16 2020 00:16:24\n",
      "\u001b[2m\u001b[36m(pid=404)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=404)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=404)\u001b[0m      JSBSim Flight Dynamics Model v1.1.0.dev1 Jun 16 2020 00:16:24\n",
      "\u001b[2m\u001b[36m(pid=400)\u001b[0m Ray Worker [0] Ready.\n",
      "\u001b[2m\u001b[36m(pid=391)\u001b[0m Ray Worker [1] Ready.\n",
      "\u001b[2m\u001b[36m(pid=392)\u001b[0m Ray Worker [2] Ready.\n",
      "\u001b[2m\u001b[36m(pid=401)\u001b[0m Ray Worker [6] Ready.\n",
      "\u001b[2m\u001b[36m(pid=393)\u001b[0m Ray Worker [8] Ready.\n",
      "\u001b[2m\u001b[36m(pid=403)\u001b[0m Ray Worker [4] Ready.\n",
      "\u001b[2m\u001b[36m(pid=397)\u001b[0m Ray Worker [9] Ready.\n",
      "\u001b[2m\u001b[36m(pid=404)\u001b[0m Ray Worker [7] Ready.\n",
      "\u001b[2m\u001b[36m(pid=398)\u001b[0m Ray Worker [5] Ready.\n",
      "\u001b[2m\u001b[36m(pid=396)\u001b[0m Ray Worker [3] Ready.\n",
      "[1/50000] rollout:[34.8]s update:[15.9]s avg_q:[0.046].\n",
      "[Eval. start] step:[1/50000][0.0%] #step:[1.5e+04] time:[day:[01] 00:00:50] ram:[13.6%].\n",
      " [0/6] [spot_4g] ep_ret:[81.0898] ep_len:[15000]. blue health:[1.00] red health:[0.22]\n",
      " [1/6] [spot_5g] ep_ret:[110.1019] ep_len:[13397]. blue health:[1.00] red health:[0.00]\n",
      " [2/6] [spot_random] ep_ret:[93.2820] ep_len:[15000]. blue health:[1.00] red health:[0.10]\n",
      " [3/6] [es_trial2] ep_ret:[102.1672] ep_len:[6137]. blue health:[1.00] red health:[0.00]\n",
      " [4/6] [es_trial3_scrimmage4] ep_ret:[-103.6396] ep_len:[6450]. blue health:[0.00] red health:[1.00]\n",
      " [5/6] [expert_system] ep_ret:[101.5529] ep_len:[5034]. blue health:[1.00] red health:[0.00]\n",
      "[Eval. done] time:[day:[01] 00:02:59] ep_ret_avg:[64.092].\n",
      "[../data/net/adt_cont_tactic/model_and_buffers.npz] saved.\n",
      "[10/50000] rollout:[33.1]s update:[16.0]s avg_q:[0.444].\n",
      "[20/50000] rollout:[31.7]s update:[16.5]s avg_q:[1.023].\n",
      "[30/50000] rollout:[31.4]s update:[16.3]s avg_q:[1.089].\n",
      "[40/50000] rollout:[26.6]s update:[16.3]s avg_q:[1.010].\n",
      "[50/50000] rollout:[30.9]s update:[16.4]s avg_q:[0.492].\n",
      "[Eval. start] step:[50/50000][0.1%] #step:[7.5e+05] time:[day:[01] 00:41:08] ram:[15.1%].\n",
      " [0/6] [spot_4g] ep_ret:[3.0179] ep_len:[15000]. blue health:[1.00] red health:[1.00]\n",
      " [1/6] [spot_5g] ep_ret:[-0.6422] ep_len:[15000]. blue health:[1.00] red health:[1.00]\n",
      " [2/6] [spot_random] ep_ret:[6.1572] ep_len:[15000]. blue health:[1.00] red health:[1.00]\n",
      " [3/6] [es_trial2] ep_ret:[90.3365] ep_len:[15000]. blue health:[1.00] red health:[0.25]\n",
      " [4/6] [es_trial3_scrimmage4] ep_ret:[-9.8323] ep_len:[15000]. blue health:[1.00] red health:[1.00]\n",
      " [5/6] [expert_system] ep_ret:[-9.6100] ep_len:[15000]. blue health:[0.93] red health:[1.00]\n",
      "[Eval. done] time:[day:[01] 00:45:41] ep_ret_avg:[13.238].\n",
      "[../data/net/adt_cont_tactic/model_and_buffers.npz] saved.\n",
      "[60/50000] rollout:[21.8]s update:[16.0]s avg_q:[1.268].\n",
      "[70/50000] rollout:[32.1]s update:[16.3]s avg_q:[0.552].\n",
      "[80/50000] rollout:[30.7]s update:[16.3]s avg_q:[0.267].\n",
      "[90/50000] rollout:[31.7]s update:[16.3]s avg_q:[0.885].\n",
      "[100/50000] rollout:[29.0]s update:[16.3]s avg_q:[0.727].\n",
      "[Eval. start] step:[100/50000][0.2%] #step:[1.5e+06] time:[day:[01] 01:23:05] ram:[16.2%].\n",
      " [0/6] [spot_4g] ep_ret:[9.4417] ep_len:[15000]. blue health:[1.00] red health:[1.00]\n",
      " [1/6] [spot_5g] ep_ret:[1.0249] ep_len:[15000]. blue health:[0.99] red health:[1.00]\n",
      " [2/6] [spot_random] ep_ret:[6.9554] ep_len:[15000]. blue health:[1.00] red health:[1.00]\n",
      " [3/6] [es_trial2] ep_ret:[107.2248] ep_len:[9598]. blue health:[1.00] red health:[0.00]\n",
      " [4/6] [es_trial3_scrimmage4] ep_ret:[9.2978] ep_len:[15000]. blue health:[0.99] red health:[0.83]\n",
      " [5/6] [expert_system] ep_ret:[-38.1163] ep_len:[15000]. blue health:[0.62] red health:[1.00]\n",
      "[Eval. done] time:[day:[01] 01:27:11] ep_ret_avg:[15.971].\n",
      "[../data/net/adt_cont_tactic/model_and_buffers.npz] saved.\n",
      "[110/50000] rollout:[19.8]s update:[16.3]s avg_q:[1.338].\n",
      "[120/50000] rollout:[30.4]s update:[16.4]s avg_q:[1.075].\n",
      "[130/50000] rollout:[31.4]s update:[16.3]s avg_q:[0.955].\n",
      "[140/50000] rollout:[30.7]s update:[16.3]s avg_q:[1.362].\n",
      "[150/50000] rollout:[31.2]s update:[16.4]s avg_q:[0.480].\n",
      "[Eval. start] step:[150/50000][0.3%] #step:[2.2e+06] time:[day:[01] 02:05:19] ram:[17.3%].\n",
      " [0/6] [spot_4g] ep_ret:[17.6388] ep_len:[15000]. blue health:[1.00] red health:[1.00]\n",
      " [1/6] [spot_5g] ep_ret:[-0.4834] ep_len:[15000]. blue health:[1.00] red health:[1.00]\n",
      " [2/6] [spot_random] ep_ret:[-0.0228] ep_len:[15000]. blue health:[1.00] red health:[1.00]\n",
      " [3/6] [es_trial2] ep_ret:[104.1615] ep_len:[9696]. blue health:[1.00] red health:[0.00]\n",
      " [4/6] [es_trial3_scrimmage4] ep_ret:[-5.6100] ep_len:[6971]. blue health:[0.00] red health:[1.00]\n",
      " [5/6] [expert_system] ep_ret:[-100.5716] ep_len:[4280]. blue health:[0.00] red health:[1.00]\n",
      "[Eval. done] time:[day:[01] 02:08:16] ep_ret_avg:[2.519].\n",
      "[../data/net/adt_cont_tactic/model_and_buffers.npz] saved.\n",
      "[160/50000] rollout:[23.7]s update:[16.3]s avg_q:[1.007].\n",
      "[170/50000] rollout:[31.3]s update:[16.4]s avg_q:[0.492].\n",
      "[180/50000] rollout:[30.7]s update:[16.4]s avg_q:[-0.063].\n",
      "[190/50000] rollout:[19.8]s update:[16.3]s avg_q:[1.082].\n",
      "[200/50000] rollout:[32.4]s update:[16.3]s avg_q:[0.321].\n",
      "[Eval. start] step:[200/50000][0.4%] #step:[3.0e+06] time:[day:[01] 02:46:31] ram:[18.4%].\n",
      " [0/6] [spot_4g] ep_ret:[104.9289] ep_len:[5906]. blue health:[1.00] red health:[0.00]\n",
      " [1/6] [spot_5g] ep_ret:[2.1814] ep_len:[15000]. blue health:[1.00] red health:[0.99]\n",
      " [2/6] [spot_random] ep_ret:[10.4625] ep_len:[15000]. blue health:[1.00] red health:[0.90]\n",
      " [3/6] [es_trial2] ep_ret:[101.2924] ep_len:[6028]. blue health:[1.00] red health:[0.00]\n",
      " [4/6] [es_trial3_scrimmage4] ep_ret:[84.6749] ep_len:[12010]. blue health:[0.91] red health:[0.00]\n",
      " [5/6] [expert_system] ep_ret:[-104.2044] ep_len:[11975]. blue health:[0.00] red health:[1.00]\n",
      "[Eval. done] time:[day:[01] 02:49:44] ep_ret_avg:[33.223].\n",
      "[../data/net/adt_cont_tactic/model_and_buffers.npz] saved.\n",
      "[210/50000] rollout:[31.2]s update:[16.3]s avg_q:[0.661].\n",
      "[220/50000] rollout:[31.0]s update:[16.3]s avg_q:[0.510].\n",
      "[230/50000] rollout:[30.9]s update:[16.4]s avg_q:[0.989].\n",
      "[240/50000] rollout:[31.1]s update:[16.4]s avg_q:[0.517].\n",
      "[250/50000] rollout:[31.4]s update:[16.4]s avg_q:[0.405].\n",
      "[Eval. start] step:[250/50000][0.5%] #step:[3.8e+06] time:[day:[01] 03:28:47] ram:[19.4%].\n",
      " [0/6] [spot_4g] ep_ret:[10.2677] ep_len:[15000]. blue health:[1.00] red health:[1.00]\n",
      " [1/6] [spot_5g] ep_ret:[0.6205] ep_len:[15000]. blue health:[1.00] red health:[1.00]\n",
      " [2/6] [spot_random] ep_ret:[14.0829] ep_len:[15000]. blue health:[1.00] red health:[1.00]\n",
      " [3/6] [es_trial2] ep_ret:[100.3961] ep_len:[7423]. blue health:[1.00] red health:[0.00]\n",
      " [4/6] [es_trial3_scrimmage4] ep_ret:[-102.9168] ep_len:[14605]. blue health:[0.00] red health:[1.00]\n",
      " [5/6] [expert_system] ep_ret:[-101.8705] ep_len:[4214]. blue health:[0.00] red health:[1.00]\n",
      "[Eval. done] time:[day:[01] 03:32:49] ep_ret_avg:[-13.237].\n",
      "[../data/net/adt_cont_tactic/model_and_buffers.npz] saved.\n",
      "[260/50000] rollout:[31.2]s update:[16.4]s avg_q:[0.719].\n",
      "[270/50000] rollout:[30.7]s update:[16.4]s avg_q:[0.095].\n",
      "[280/50000] rollout:[30.6]s update:[16.3]s avg_q:[-1.417].\n",
      "[290/50000] rollout:[31.2]s update:[16.3]s avg_q:[-0.228].\n",
      "[300/50000] rollout:[29.3]s update:[16.4]s avg_q:[-0.187].\n",
      "[Eval. start] step:[300/50000][0.6%] #step:[4.5e+06] time:[day:[01] 04:12:12] ram:[20.5%].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [0/6] [spot_4g] ep_ret:[97.8311] ep_len:[13849]. blue health:[1.00] red health:[0.00]\n",
      " [1/6] [spot_5g] ep_ret:[-2.1569] ep_len:[15000]. blue health:[1.00] red health:[1.00]\n",
      " [2/6] [spot_random] ep_ret:[-2.9062] ep_len:[15000]. blue health:[1.00] red health:[1.00]\n",
      " [3/6] [es_trial2] ep_ret:[-54.7362] ep_len:[15000]. blue health:[0.47] red health:[1.00]\n",
      " [4/6] [es_trial3_scrimmage4] ep_ret:[-107.4982] ep_len:[10300]. blue health:[0.00] red health:[1.00]\n",
      " [5/6] [expert_system] ep_ret:[-12.4295] ep_len:[15000]. blue health:[0.94] red health:[1.00]\n",
      "[Eval. done] time:[day:[01] 04:16:49] ep_ret_avg:[-13.649].\n",
      "[../data/net/adt_cont_tactic/model_and_buffers.npz] saved.\n",
      "[310/50000] rollout:[30.9]s update:[16.2]s avg_q:[0.410].\n",
      "[320/50000] rollout:[33.6]s update:[15.9]s avg_q:[-0.575].\n",
      "[330/50000] rollout:[30.9]s update:[16.0]s avg_q:[-0.362].\n",
      "[340/50000] rollout:[32.8]s update:[15.8]s avg_q:[-0.858].\n",
      "[350/50000] rollout:[32.9]s update:[15.8]s avg_q:[-1.243].\n",
      "[Eval. start] step:[350/50000][0.7%] #step:[5.2e+06] time:[day:[01] 04:56:28] ram:[21.6%].\n",
      " [0/6] [spot_4g] ep_ret:[41.2894] ep_len:[15000]. blue health:[1.00] red health:[0.74]\n",
      " [1/6] [spot_5g] ep_ret:[-2.4452] ep_len:[15000]. blue health:[1.00] red health:[1.00]\n",
      " [2/6] [spot_random] ep_ret:[9.2356] ep_len:[15000]. blue health:[1.00] red health:[1.00]\n",
      " [3/6] [es_trial2] ep_ret:[15.2036] ep_len:[15000]. blue health:[1.00] red health:[0.94]\n",
      " [4/6] [es_trial3_scrimmage4] ep_ret:[-8.7038] ep_len:[15000]. blue health:[1.00] red health:[1.00]\n",
      " [5/6] [expert_system] ep_ret:[-102.3636] ep_len:[3870]. blue health:[0.00] red health:[1.00]\n",
      "[Eval. done] time:[day:[01] 05:01:09] ep_ret_avg:[-7.964].\n",
      "[../data/net/adt_cont_tactic/model_and_buffers.npz] saved.\n",
      "[360/50000] rollout:[25.3]s update:[15.7]s avg_q:[0.030].\n",
      "[370/50000] rollout:[30.2]s update:[15.8]s avg_q:[-0.395].\n",
      "[380/50000] rollout:[29.5]s update:[15.9]s avg_q:[0.286].\n",
      "[390/50000] rollout:[32.0]s update:[16.0]s avg_q:[-0.627].\n",
      "[400/50000] rollout:[28.3]s update:[15.9]s avg_q:[-0.063].\n",
      "[Eval. start] step:[400/50000][0.8%] #step:[6.0e+06] time:[day:[01] 05:39:57] ram:[22.7%].\n",
      " [0/6] [spot_4g] ep_ret:[101.7642] ep_len:[1659]. blue health:[1.00] red health:[0.00]\n",
      " [1/6] [spot_5g] ep_ret:[0.3455] ep_len:[15000]. blue health:[1.00] red health:[1.00]\n",
      " [2/6] [spot_random] ep_ret:[56.0664] ep_len:[15000]. blue health:[1.00] red health:[0.55]\n",
      " [3/6] [es_trial2] ep_ret:[-1.9360] ep_len:[15000]. blue health:[0.82] red health:[0.88]\n",
      " [4/6] [es_trial3_scrimmage4] ep_ret:[-14.9165] ep_len:[15000]. blue health:[0.77] red health:[1.00]\n",
      " [5/6] [expert_system] ep_ret:[-99.8468] ep_len:[7385]. blue health:[0.00] red health:[0.98]\n",
      "[Eval. done] time:[day:[01] 05:44:34] ep_ret_avg:[6.913].\n",
      "[../data/net/adt_cont_tactic/model_and_buffers.npz] saved.\n",
      "[410/50000] rollout:[30.8]s update:[15.8]s avg_q:[0.354].\n",
      "[420/50000] rollout:[33.8]s update:[15.9]s avg_q:[-1.043].\n",
      "[430/50000] rollout:[32.5]s update:[15.9]s avg_q:[-0.454].\n",
      "[440/50000] rollout:[32.9]s update:[15.8]s avg_q:[0.085].\n",
      "[450/50000] rollout:[32.9]s update:[15.8]s avg_q:[0.015].\n",
      "[Eval. start] step:[450/50000][0.9%] #step:[6.8e+06] time:[day:[01] 06:24:12] ram:[23.7%].\n",
      " [0/6] [spot_4g] ep_ret:[-0.2014] ep_len:[15000]. blue health:[1.00] red health:[1.00]\n",
      " [1/6] [spot_5g] ep_ret:[0.2241] ep_len:[15000]. blue health:[1.00] red health:[1.00]\n",
      " [2/6] [spot_random] ep_ret:[-0.9208] ep_len:[15000]. blue health:[1.00] red health:[1.00]\n",
      " [3/6] [es_trial2] ep_ret:[102.0728] ep_len:[8517]. blue health:[1.00] red health:[0.00]\n",
      " [4/6] [es_trial3_scrimmage4] ep_ret:[-5.9433] ep_len:[15000]. blue health:[1.00] red health:[1.00]\n",
      " [5/6] [expert_system] ep_ret:[-80.1610] ep_len:[12998]. blue health:[0.00] red health:[0.75]\n",
      "[Eval. done] time:[day:[01] 06:28:34] ep_ret_avg:[2.512].\n",
      "[../data/net/adt_cont_tactic/model_and_buffers.npz] saved.\n",
      "[460/50000] rollout:[31.9]s update:[15.8]s avg_q:[-0.706].\n",
      "[470/50000] rollout:[32.9]s update:[15.9]s avg_q:[-0.338].\n",
      "[480/50000] rollout:[32.6]s update:[15.8]s avg_q:[-0.208].\n",
      "[490/50000] rollout:[30.7]s update:[15.9]s avg_q:[-0.410].\n",
      "[500/50000] rollout:[26.0]s update:[15.7]s avg_q:[0.495].\n",
      "[Eval. start] step:[500/50000][1.0%] #step:[7.5e+06] time:[day:[01] 07:08:01] ram:[24.8%].\n",
      " [0/6] [spot_4g] ep_ret:[101.6051] ep_len:[9263]. blue health:[1.00] red health:[0.00]\n",
      " [1/6] [spot_5g] ep_ret:[-5.5025] ep_len:[15000]. blue health:[1.00] red health:[1.00]\n",
      " [2/6] [spot_random] ep_ret:[100.9195] ep_len:[3467]. blue health:[1.00] red health:[0.00]\n",
      " [3/6] [es_trial2] ep_ret:[4.6909] ep_len:[15000]. blue health:[1.00] red health:[1.00]\n",
      " [4/6] [es_trial3_scrimmage4] ep_ret:[-11.5218] ep_len:[15000]. blue health:[1.00] red health:[1.00]\n",
      " [5/6] [expert_system] ep_ret:[-101.4581] ep_len:[15000]. blue health:[0.10] red health:[1.00]\n",
      "[Eval. done] time:[day:[01] 07:12:35] ep_ret_avg:[14.789].\n",
      "[../data/net/adt_cont_tactic/model_and_buffers.npz] saved.\n",
      "[510/50000] rollout:[33.7]s update:[15.8]s avg_q:[-0.149].\n",
      "[520/50000] rollout:[28.6]s update:[15.8]s avg_q:[0.513].\n",
      "[530/50000] rollout:[32.6]s update:[15.8]s avg_q:[-0.592].\n",
      "[540/50000] rollout:[33.8]s update:[16.0]s avg_q:[0.016].\n",
      "[550/50000] rollout:[33.1]s update:[15.8]s avg_q:[0.345].\n",
      "[Eval. start] step:[550/50000][1.1%] #step:[8.2e+06] time:[day:[01] 07:51:47] ram:[25.8%].\n",
      " [0/6] [spot_4g] ep_ret:[17.9723] ep_len:[15000]. blue health:[1.00] red health:[1.00]\n",
      " [1/6] [spot_5g] ep_ret:[-1.5650] ep_len:[15000]. blue health:[1.00] red health:[1.00]\n",
      " [2/6] [spot_random] ep_ret:[20.3137] ep_len:[15000]. blue health:[1.00] red health:[0.91]\n",
      " [3/6] [es_trial2] ep_ret:[16.5833] ep_len:[15000]. blue health:[1.00] red health:[0.88]\n",
      " [4/6] [es_trial3_scrimmage4] ep_ret:[64.7872] ep_len:[13482]. blue health:[0.76] red health:[0.00]\n",
      " [5/6] [expert_system] ep_ret:[-9.8334] ep_len:[15000]. blue health:[1.00] red health:[1.00]\n",
      "[Eval. done] time:[day:[01] 07:56:32] ep_ret_avg:[18.043].\n",
      "[../data/net/adt_cont_tactic/model_and_buffers.npz] saved.\n",
      "[560/50000] rollout:[29.1]s update:[15.9]s avg_q:[0.809].\n",
      "[570/50000] rollout:[32.0]s update:[15.8]s avg_q:[0.147].\n",
      "[580/50000] rollout:[33.1]s update:[15.9]s avg_q:[0.374].\n",
      "[590/50000] rollout:[32.9]s update:[15.9]s avg_q:[0.675].\n",
      "[600/50000] rollout:[32.9]s update:[15.8]s avg_q:[0.982].\n",
      "[Eval. start] step:[600/50000][1.2%] #step:[9.0e+06] time:[day:[01] 08:36:38] ram:[26.6%].\n",
      " [0/6] [spot_4g] ep_ret:[1.3910] ep_len:[15000]. blue health:[1.00] red health:[1.00]\n",
      " [1/6] [spot_5g] ep_ret:[-1.6540] ep_len:[15000]. blue health:[1.00] red health:[1.00]\n",
      " [2/6] [spot_random] ep_ret:[65.6398] ep_len:[15000]. blue health:[1.00] red health:[0.51]\n",
      " [3/6] [es_trial2] ep_ret:[3.4475] ep_len:[15000]. blue health:[1.00] red health:[0.95]\n",
      " [4/6] [es_trial3_scrimmage4] ep_ret:[-111.4903] ep_len:[11349]. blue health:[0.00] red health:[1.00]\n",
      " [5/6] [expert_system] ep_ret:[3.9202] ep_len:[15000]. blue health:[1.00] red health:[1.00]\n",
      "[Eval. done] time:[day:[01] 08:41:23] ep_ret_avg:[-6.458].\n",
      "[../data/net/adt_cont_tactic/model_and_buffers.npz] saved.\n",
      "[610/50000] rollout:[32.5]s update:[15.9]s avg_q:[1.239].\n",
      "[620/50000] rollout:[32.3]s update:[15.8]s avg_q:[0.238].\n",
      "[630/50000] rollout:[25.8]s update:[15.8]s avg_q:[0.851].\n",
      "[640/50000] rollout:[31.1]s update:[15.8]s avg_q:[0.589].\n",
      "[650/50000] rollout:[33.1]s update:[15.8]s avg_q:[1.411].\n",
      "[Eval. start] step:[650/50000][1.3%] #step:[9.8e+06] time:[day:[01] 09:20:52] ram:[27.6%].\n",
      " [0/6] [spot_4g] ep_ret:[4.1513] ep_len:[15000]. blue health:[1.00] red health:[1.00]\n",
      " [1/6] [spot_5g] ep_ret:[4.2437] ep_len:[15000]. blue health:[1.00] red health:[1.00]\n",
      " [2/6] [spot_random] ep_ret:[4.9143] ep_len:[15000]. blue health:[1.00] red health:[1.00]\n",
      " [3/6] [es_trial2] ep_ret:[102.8082] ep_len:[7966]. blue health:[1.00] red health:[0.00]\n",
      " [4/6] [es_trial3_scrimmage4] ep_ret:[31.0579] ep_len:[15000]. blue health:[0.90] red health:[0.57]\n",
      " [5/6] [expert_system] ep_ret:[-102.7323] ep_len:[4568]. blue health:[0.00] red health:[1.00]\n",
      "[Eval. done] time:[day:[01] 09:25:09] ep_ret_avg:[7.407].\n",
      "[../data/net/adt_cont_tactic/model_and_buffers.npz] saved.\n",
      "[660/50000] rollout:[33.1]s update:[15.8]s avg_q:[-0.576].\n",
      "[670/50000] rollout:[32.5]s update:[15.8]s avg_q:[0.783].\n",
      "[680/50000] rollout:[31.8]s update:[15.8]s avg_q:[0.331].\n",
      "[690/50000] rollout:[30.6]s update:[15.8]s avg_q:[1.145].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[700/50000] rollout:[32.5]s update:[15.7]s avg_q:[0.024].\n",
      "[Eval. start] step:[700/50000][1.4%] #step:[1.0e+07] time:[day:[01] 10:04:02] ram:[28.7%].\n",
      " [0/6] [spot_4g] ep_ret:[9.1226] ep_len:[15000]. blue health:[1.00] red health:[1.00]\n",
      " [1/6] [spot_5g] ep_ret:[101.7784] ep_len:[5512]. blue health:[1.00] red health:[0.00]\n",
      " [2/6] [spot_random] ep_ret:[8.5826] ep_len:[15000]. blue health:[1.00] red health:[0.97]\n",
      " [3/6] [es_trial2] ep_ret:[105.4630] ep_len:[13176]. blue health:[1.00] red health:[0.00]\n",
      " [4/6] [es_trial3_scrimmage4] ep_ret:[-11.1424] ep_len:[8457]. blue health:[0.00] red health:[1.00]\n",
      " [5/6] [expert_system] ep_ret:[-50.9616] ep_len:[12297]. blue health:[0.00] red health:[1.00]\n",
      "[Eval. done] time:[day:[01] 10:08:13] ep_ret_avg:[27.140].\n",
      "[../data/net/adt_cont_tactic/model_and_buffers.npz] saved.\n",
      "[710/50000] rollout:[25.8]s update:[15.9]s avg_q:[0.388].\n",
      "[720/50000] rollout:[25.8]s update:[15.8]s avg_q:[0.181].\n",
      "[730/50000] rollout:[29.5]s update:[15.9]s avg_q:[0.329].\n",
      "[740/50000] rollout:[32.0]s update:[15.8]s avg_q:[-0.437].\n",
      "[750/50000] rollout:[26.1]s update:[15.9]s avg_q:[0.219].\n",
      "[Eval. start] step:[750/50000][1.5%] #step:[1.1e+07] time:[day:[01] 10:47:42] ram:[29.7%].\n",
      " [0/6] [spot_4g] ep_ret:[6.3679] ep_len:[15000]. blue health:[1.00] red health:[1.00]\n",
      " [1/6] [spot_5g] ep_ret:[104.9830] ep_len:[15000]. blue health:[1.00] red health:[0.04]\n",
      " [2/6] [spot_random] ep_ret:[6.0821] ep_len:[15000]. blue health:[1.00] red health:[0.93]\n",
      " [3/6] [es_trial2] ep_ret:[23.9665] ep_len:[15000]. blue health:[0.91] red health:[0.69]\n",
      " [4/6] [es_trial3_scrimmage4] ep_ret:[-85.2458] ep_len:[14655]. blue health:[0.00] red health:[0.81]\n",
      " [5/6] [expert_system] ep_ret:[-92.4252] ep_len:[12704]. blue health:[0.00] red health:[0.87]\n",
      "[Eval. done] time:[day:[01] 10:52:26] ep_ret_avg:[-6.045].\n",
      "[../data/net/adt_cont_tactic/model_and_buffers.npz] saved.\n",
      "[760/50000] rollout:[32.7]s update:[15.9]s avg_q:[0.019].\n",
      "[770/50000] rollout:[32.8]s update:[15.8]s avg_q:[0.892].\n",
      "[780/50000] rollout:[33.1]s update:[15.9]s avg_q:[-0.670].\n",
      "[790/50000] rollout:[25.6]s update:[15.9]s avg_q:[0.084].\n",
      "[800/50000] rollout:[33.4]s update:[15.9]s avg_q:[0.256].\n",
      "[Eval. start] step:[800/50000][1.6%] #step:[1.2e+07] time:[day:[01] 11:32:00] ram:[30.7%].\n",
      " [0/6] [spot_4g] ep_ret:[-2.4426] ep_len:[15000]. blue health:[1.00] red health:[1.00]\n",
      " [1/6] [spot_5g] ep_ret:[-3.9086] ep_len:[5692]. blue health:[0.00] red health:[1.00]\n",
      " [2/6] [spot_random] ep_ret:[-1.7656] ep_len:[15000]. blue health:[1.00] red health:[1.00]\n",
      " [3/6] [es_trial2] ep_ret:[11.4243] ep_len:[15000]. blue health:[1.00] red health:[1.00]\n",
      " [4/6] [es_trial3_scrimmage4] ep_ret:[-8.5613] ep_len:[15000]. blue health:[1.00] red health:[1.00]\n",
      " [5/6] [expert_system] ep_ret:[-102.6983] ep_len:[5939]. blue health:[0.00] red health:[1.00]\n",
      "[Eval. done] time:[day:[01] 11:36:37] ep_ret_avg:[-17.992].\n",
      "[../data/net/adt_cont_tactic/model_and_buffers.npz] saved.\n",
      "[810/50000] rollout:[33.3]s update:[15.9]s avg_q:[0.784].\n",
      "[820/50000] rollout:[32.7]s update:[15.9]s avg_q:[0.365].\n",
      "[830/50000] rollout:[27.3]s update:[16.0]s avg_q:[0.342].\n",
      "[840/50000] rollout:[32.6]s update:[15.8]s avg_q:[0.696].\n",
      "[850/50000] rollout:[32.6]s update:[15.8]s avg_q:[0.677].\n",
      "[Eval. start] step:[850/50000][1.7%] #step:[1.3e+07] time:[day:[01] 12:16:42] ram:[31.0%].\n",
      " [0/6] [spot_4g] ep_ret:[19.7956] ep_len:[15000]. blue health:[1.00] red health:[0.91]\n",
      " [1/6] [spot_5g] ep_ret:[46.0675] ep_len:[15000]. blue health:[1.00] red health:[0.63]\n",
      " [2/6] [spot_random] ep_ret:[32.1812] ep_len:[15000]. blue health:[1.00] red health:[0.77]\n",
      " [3/6] [es_trial2] ep_ret:[108.4493] ep_len:[13771]. blue health:[1.00] red health:[0.00]\n",
      " [4/6] [es_trial3_scrimmage4] ep_ret:[-92.2990] ep_len:[5535]. blue health:[0.00] red health:[0.89]\n",
      " [5/6] [expert_system] ep_ret:[-104.6260] ep_len:[8115]. blue health:[0.00] red health:[1.00]\n",
      "[Eval. done] time:[day:[01] 12:21:04] ep_ret_avg:[1.595].\n",
      "[../data/net/adt_cont_tactic/model_and_buffers.npz] saved.\n",
      "[860/50000] rollout:[33.1]s update:[15.9]s avg_q:[0.371].\n",
      "[870/50000] rollout:[32.3]s update:[15.9]s avg_q:[0.619].\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "n_env_step = 0 # number of environment steps\n",
    "for t in range(int(total_steps)):\n",
    "    esec = time.time()-start_time\n",
    "    \n",
    "    # Synchronize worker weights\n",
    "    weights = R.get_weights()\n",
    "    set_weights_list = [worker.set_weights.remote(weights) for worker in workers] \n",
    "    \n",
    "    # Make rollout and accumulate to Buffers\n",
    "    t_start = time.time()\n",
    "    ops = [worker.rollout.remote() for worker in workers]\n",
    "    rollout_vals = ray.get(ops)\n",
    "    for rollout_val in rollout_vals:\n",
    "        o_buffer,a_buffer,r_buffer,o2_buffer,d_buffer = rollout_val\n",
    "        for i in range(ep_len_rollout):\n",
    "            o,a,r,o2,d = o_buffer[i,:],a_buffer[i,:],r_buffer[i],o2_buffer[i,:],d_buffer[i]\n",
    "            replay_buffer_long.store(o, a, r, o2, d) \n",
    "            replay_buffer_short.store(o, a, r, o2, d) \n",
    "            n_env_step += 1\n",
    "    sec_rollout = time.time() - t_start\n",
    "    \n",
    "    # Update\n",
    "    t_start = time.time()\n",
    "    avg_qs = np.zeros(int(update_count))\n",
    "    for c_idx in range(int(update_count)):\n",
    "        batch_long = replay_buffer_long.sample_batch(batch_size//2) \n",
    "        batch_short = replay_buffer_short.sample_batch(batch_size//2) \n",
    "        feed_dict = {R.model['o_ph']: np.concatenate((batch_long['obs1'],batch_short['obs1'])),\n",
    "                     R.model['o2_ph']: np.concatenate((batch_long['obs2'],batch_short['obs2'])),\n",
    "                     R.model['a_ph']: np.concatenate((batch_long['acts'],batch_short['acts'])),\n",
    "                     R.model['r_ph']: np.concatenate((batch_long['rews'],batch_short['rews'])),\n",
    "                     R.model['d_ph']: np.concatenate((batch_long['done'],batch_short['done']))\n",
    "                    }\n",
    "        outs = R.sess.run(R.step_ops, feed_dict)\n",
    "        q1_vals,q2_vals = outs[3],outs[4]\n",
    "        avg_q = 0.5*np.mean(q1_vals)+0.5*np.mean(q2_vals)\n",
    "        avg_qs[c_idx] = avg_q\n",
    "    sec_update = time.time() - t_start\n",
    "    \n",
    "    # Synchronize worker weights (after update)\n",
    "    weights = R.get_weights()\n",
    "    set_weights_list = [worker.set_weights.remote(weights) for worker in workers] \n",
    "    \n",
    "    # Print\n",
    "    if (t == 0) or (((t+1)%print_every) == 0): \n",
    "        print (\"[%d/%d] rollout:[%.1f]s update:[%.1f]s avg_q:[%.3f].\"%\n",
    "               (t+1,total_steps,sec_rollout,sec_update,np.mean(avg_qs)))\n",
    "    \n",
    "    # Evaluate\n",
    "    if (t == 0) or (((t+1)%evaluate_every) == 0): \n",
    "        ram_percent = psutil.virtual_memory().percent # memory usage\n",
    "        print (\"[Eval. start] step:[%d/%d][%.1f%%] #step:[%.1e] time:[%s] ram:[%.1f%%].\"%\n",
    "               (t+1,total_steps,t/total_steps*100,\n",
    "                n_env_step,\n",
    "                time.strftime(\"day:[%d] %H:%M:%S\", time.gmtime(time.time()-start_time)),\n",
    "                ram_percent)\n",
    "              )\n",
    "        \n",
    "        LOCAL_EVAL = 0\n",
    "        if LOCAL_EVAL:\n",
    "            ep_ret_sum = 0\n",
    "            for eval_idx in range(num_eval): \n",
    "                red = red_list[eval_idx]\n",
    "                o,d,ep_ret,ep_len = R.env.reset(red=red),False,0,0\n",
    "                while not(d or (ep_len == max_ep_len_eval)):\n",
    "                    a = R.get_action(o,deterministic=True)\n",
    "                    o,r,d,_ = R.env.step(a)\n",
    "                    ep_ret += r # compute return\n",
    "                    ep_len += 1\n",
    "                ep_ret_sum += ep_ret\n",
    "                blue_health,red_health = R.env.blue_health,R.env.red_health\n",
    "                print (\" [%d/%d] [%s] ep_ret:[%.4f] ep_len:[%d]. blue health:[%.2f] red health:[%.2f]\"\n",
    "                    %(eval_idx,num_eval,red,ep_ret,ep_len, blue_health,red_health))\n",
    "            ep_ret_avg = ep_ret_sum / num_eval\n",
    "            print (\"[Eval. done] time:[%s] ep_ret_avg:[%.3f].\"%\n",
    "                   (time.strftime(\"day:[%d] %H:%M:%S\", time.gmtime(time.time()-start_time)),\n",
    "                    ep_ret_avg)\n",
    "                  )\n",
    "        else: # parallel evaluation with Ray\n",
    "            ops = []\n",
    "            for i_idx in range(num_eval):\n",
    "                worker,red = workers[i_idx],red_list[i_idx]\n",
    "                ops.append(worker.evaluate.remote(red=red))\n",
    "            eval_vals = ray.get(ops)\n",
    "            ep_ret_sum = 0\n",
    "            for i_idx in range(num_eval):\n",
    "                red,eval_val = red_list[i_idx],eval_vals[i_idx]\n",
    "                ep_ret,ep_len,blue_health,red_health = eval_val[0],eval_val[1],eval_val[2],eval_val[3]\n",
    "                ep_ret_sum += ep_ret\n",
    "                print (\" [%d/%d] [%s] ep_ret:[%.4f] ep_len:[%d]. blue health:[%.2f] red health:[%.2f]\"\n",
    "                    %(i_idx,len(eval_vals),red,ep_ret,ep_len,blue_health,red_health))\n",
    "            ep_ret_avg = ep_ret_sum / num_eval\n",
    "            print (\"[Eval. done] time:[%s] ep_ret_avg:[%.3f].\"%\n",
    "                   (time.strftime(\"day:[%d] %H:%M:%S\", time.gmtime(time.time()-start_time)),\n",
    "                    ep_ret_avg)\n",
    "                  )\n",
    "        \n",
    "        # Save current SAC model and replay buffers \n",
    "        npz_path = '../data/net/adt_cont_tactic/model_and_buffers.npz'\n",
    "        save_sac_model_and_buffers(npz_path,R,replay_buffer_long,replay_buffer_short,\n",
    "                                   VERBOSE=False,IGNORE_BUFFERS=True)\n",
    "\n",
    "print (\"Done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Close"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ray.shutdown()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save model weights and replay buffers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to save the npz file \n",
    "npz_path = '../data/net/adt_cont_tactic/model_and_buffers_final.npz'\n",
    "save_sac_model_and_buffers(npz_path,R,replay_buffer_long,replay_buffer_short,\n",
    "                           VERBOSE=False,IGNORE_BUFFERS=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Restore "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "npz_path = '../data/net/adt_cont_tactic/model_and_buffers_final.npz'\n",
    "restore_sac_model_and_buffers(npz_path,R,replay_buffer_long,replay_buffer_short,\n",
    "                              VERBOSE=False,IGNORE_BUFFERS=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "eval_env = get_eval_env()\n",
    "red_list = [Agents.SPOT_4G,Agents.SPOT_5G,Agents.SPOT_RANDOM,\n",
    "            Agents.EXPERT_SYSTEM_TRIAL_2,Agents.EXPERT_SYSTEM_TRIAL_3_SCRIMMAGE_4,\n",
    "            Agents.EXPERT_SYSTEM]\n",
    "red = red_list[0]\n",
    "o,d,ep_ret,ep_len = eval_env.reset(red=red),False,0,0\n",
    "while not(d or (ep_len == max_ep_len_eval)):\n",
    "    a = R.get_action(o,deterministic=True)\n",
    "    o,r,d,_ = eval_env.step(a)\n",
    "    ep_ret += r # compute return \n",
    "    ep_len += 1\n",
    "print (\"[Evaluate] ep_ret:[%.4f] ep_len:[%d]\"\n",
    "    %(eval_idx,ep_len))\n",
    "eval_env.close() # close env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
