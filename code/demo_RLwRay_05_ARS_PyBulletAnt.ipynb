{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Augmented Random Search (ARS) with PyBullet Ant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Packaged loaded. TF version is [1.15.0].\n"
     ]
    }
   ],
   "source": [
    "import datetime,gym,os,pybullet_envs,time,os,psutil,ray\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from ars import create_ars_model,get_noises_from_weights\n",
    "from util import gpu_sess,suppress_tf_warning\n",
    "np.set_printoptions(precision=2)\n",
    "suppress_tf_warning() # suppress warning \n",
    "gym.logger.set_level(40) # gym logger \n",
    "print (\"Packaged loaded. TF version is [%s].\"%(tf.__version__))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rollout Worker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "RENDER_ON_EVAL = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_env():\n",
    "    import pybullet_envs,gym\n",
    "    gym.logger.set_level(40) # gym logger \n",
    "    return gym.make('AntBulletEnv-v0')\n",
    "\n",
    "def get_eval_env():\n",
    "    import pybullet_envs,gym\n",
    "    gym.logger.set_level(40) # gym logger\n",
    "    eval_env = gym.make('AntBulletEnv-v0')\n",
    "    if RENDER_ON_EVAL:\n",
    "        _ = eval_env.render(mode='human') # enable rendering\n",
    "    _ = eval_env.reset()\n",
    "    for _ in range(3): # dummy run for proper rendering \n",
    "        a = eval_env.action_space.sample()\n",
    "        o,r,d,_ = eval_env.step(a)\n",
    "        time.sleep(0.01)\n",
    "    return eval_env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RolloutWorkerClass(object):\n",
    "    \"\"\"\n",
    "    Worker without RAY (for update purposes)\n",
    "    \"\"\"\n",
    "    def __init__(self,seed=1):\n",
    "        self.seed = seed\n",
    "        from util import suppress_tf_warning\n",
    "        suppress_tf_warning() # suppress TF warnings\n",
    "        self.env = get_env()\n",
    "        odim,adim = self.env.observation_space.shape[0],self.env.action_space.shape[0]\n",
    "        self.odim = odim\n",
    "        self.adim = adim\n",
    "        # ARS model \n",
    "        self.model,self.sess = create_ars_model(\n",
    "            odim=self.odim,adim=self.adim,hdims=[128],\n",
    "            actv=tf.nn.relu,out_actv=tf.nn.tanh)\n",
    "        # Initialize model \n",
    "        tf.set_random_seed(self.seed)\n",
    "        np.random.seed(self.seed)\n",
    "        self.sess.run(tf.global_variables_initializer())\n",
    "        # Flag to initialize assign operations for 'set_weights()'\n",
    "        self.FIRST_SET_FLAG = True\n",
    "        \n",
    "    def get_action(self,o):\n",
    "        return self.sess.run(\n",
    "            self.model['mu'],feed_dict={self.model['o_ph']:o.reshape(1,-1)})[0]\n",
    "    \n",
    "    def get_weights(self):\n",
    "        \"\"\"\n",
    "        Get weights\n",
    "        \"\"\"\n",
    "        weight_vals = self.sess.run(self.model['main_vars'])\n",
    "        return weight_vals\n",
    "    \n",
    "    def set_weights(self,weight_vals):\n",
    "        \"\"\"\n",
    "        Set weights without memory leakage\n",
    "        \"\"\"\n",
    "        if self.FIRST_SET_FLAG:\n",
    "            self.FIRST_SET_FLAG = False\n",
    "            self.assign_placeholders = []\n",
    "            self.assign_ops = []\n",
    "            for w_idx,weight_tf_var in enumerate(self.model['main_vars']):\n",
    "                a = weight_tf_var\n",
    "                assign_placeholder = tf.placeholder(a.dtype, shape=a.get_shape())\n",
    "                assign_op = a.assign(assign_placeholder)\n",
    "                self.assign_placeholders.append(assign_placeholder)\n",
    "                self.assign_ops.append(assign_op)\n",
    "        for w_idx,weight_tf_var in enumerate(self.model['main_vars']):\n",
    "            self.sess.run(self.assign_ops[w_idx],\n",
    "                          {self.assign_placeholders[w_idx]:weight_vals[w_idx]})\n",
    "            \n",
    "@ray.remote\n",
    "class RayRolloutWorkerClass(object):\n",
    "    \"\"\"\n",
    "    Rollout Worker with RAY\n",
    "    \"\"\"\n",
    "    def __init__(self,worker_id=0,ep_len_rollout=1000):\n",
    "        self.worker_id = worker_id\n",
    "        self.ep_len_rollout = ep_len_rollout\n",
    "        from util import suppress_tf_warning\n",
    "        suppress_tf_warning() # suppress TF warnings\n",
    "        self.env = get_env()\n",
    "        odim,adim = self.env.observation_space.shape[0],self.env.action_space.shape[0]\n",
    "        self.odim = odim\n",
    "        self.adim = adim\n",
    "        # ARS model \n",
    "        self.model,self.sess = create_ars_model(\n",
    "            odim=self.odim,adim=self.adim,hdims=[128],\n",
    "            actv=tf.nn.relu,out_actv=tf.nn.tanh)\n",
    "        \n",
    "        # Flag to initialize assign operations for 'set_weights()'\n",
    "        self.FIRST_SET_FLAG = True\n",
    "        \n",
    "        # Flag to initialize rollout\n",
    "        self.FIRST_ROLLOUT_FLAG = True\n",
    "        \n",
    "    def get_action(self,o):\n",
    "        return self.sess.run(\n",
    "            self.model['mu'],feed_dict={self.model['o_ph']:o.reshape(1,-1)})[0]\n",
    "    \n",
    "    def set_weights(self,weight_vals,noise_vals,noise_sign=+1):\n",
    "        \"\"\"\n",
    "        Set weights without memory leakage\n",
    "        \"\"\"\n",
    "        if self.FIRST_SET_FLAG:\n",
    "            self.FIRST_SET_FLAG = False\n",
    "            self.assign_placeholders = []\n",
    "            self.assign_ops = []\n",
    "            for w_idx,weight_tf_var in enumerate(self.model['main_vars']):\n",
    "                a = weight_tf_var\n",
    "                assign_placeholder = tf.placeholder(a.dtype, shape=a.get_shape())\n",
    "                assign_op = a.assign(assign_placeholder)\n",
    "                self.assign_placeholders.append(assign_placeholder)\n",
    "                self.assign_ops.append(assign_op)\n",
    "        for w_idx,weight_tf_var in enumerate(self.model['main_vars']):\n",
    "            self.sess.run(self.assign_ops[w_idx],\n",
    "                          {self.assign_placeholders[w_idx]:\n",
    "                           weight_vals[w_idx]+noise_sign*noise_vals[w_idx]}) \n",
    "            \n",
    "    def rollout(self):\n",
    "        \"\"\"\n",
    "        Rollout\n",
    "        \"\"\"\n",
    "        if self.FIRST_ROLLOUT_FLAG:\n",
    "            self.FIRST_ROLLOUT_FLAG = False\n",
    "            self.o = self.env.reset() # reset environment\n",
    "            \n",
    "        # Loop\n",
    "        self.o = self.env.reset() # reset always\n",
    "        r_sum,step = 0,0\n",
    "        for t in range(self.ep_len_rollout):\n",
    "            self.a = self.get_action(self.o) \n",
    "            self.o2,self.r,self.d,_ = self.env.step(self.a)\n",
    "            # Save next state \n",
    "            self.o = self.o2\n",
    "            # Accumulate reward\n",
    "            r_sum += self.r\n",
    "            step += 1\n",
    "            if self.d: break\n",
    "        return r_sum,step\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize Env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment Ready. odim:[28] adim:[8].\n"
     ]
    }
   ],
   "source": [
    "eval_env = get_eval_env()\n",
    "adim,odim = eval_env.action_space.shape[0],eval_env.observation_space.shape[0]\n",
    "print (\"Environment Ready. odim:[%d] adim:[%d].\"%(odim,adim))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyper-parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_cpu = n_workers = 30\n",
    "total_steps,evaluate_every = 2000,50 \n",
    "ep_len_rollout = 1000\n",
    "num_eval,max_ep_len_eval = 3,1e3\n",
    "n_env_step = 0\n",
    "alpha,nu,b = 0.01,0.02,n_workers//2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize Workers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-07-12 10:55:15,598\tINFO resource_spec.py:212 -- Starting Ray with 143.02 GiB memory available for workers and up to 65.29 GiB for objects. You can adjust these settings with ray.init(memory=<bytes>, object_store_memory=<bytes>).\n",
      "2020-07-12 10:55:15,716\tWARNING services.py:923 -- Redis failed to start, retrying now.\n",
      "2020-07-12 10:55:15,946\tWARNING services.py:923 -- Redis failed to start, retrying now.\n",
      "2020-07-12 10:55:16,172\tINFO services.py:1165 -- View the Ray dashboard at \u001b[1m\u001b[32mlocalhost:8267\u001b[39m\u001b[22m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RAY initialized with [30] cpus and [30] workers.\n"
     ]
    }
   ],
   "source": [
    "ray.init(num_cpus=n_cpu)\n",
    "tf.reset_default_graph()\n",
    "R = RolloutWorkerClass(seed=0)\n",
    "workers = [RayRolloutWorkerClass.remote(worker_id=i,ep_len_rollout=ep_len_rollout) \n",
    "           for i in range(n_workers)]\n",
    "print (\"RAY initialized with [%d] cpus and [%d] workers.\"%\n",
    "       (n_cpu,n_workers))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "time.sleep(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Evaluate] step:[1/2000][0.0%] #step:[1.1e+04] time:[00:00:03] ram:[17.3%].\n",
      "[Evaluate] [0/3] ep_ret:[48.9050] ep_len:[62]\n",
      "[Evaluate] [1/3] ep_ret:[25.8082] ep_len:[38]\n",
      "[Evaluate] [2/3] ep_ret:[26.2239] ep_len:[39]\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "for t in range(int(total_steps)):\n",
    "    \n",
    "    # Distribute worker weights\n",
    "    weights = R.get_weights()\n",
    "    noises_list = []\n",
    "    for _ in range(n_workers):\n",
    "        noises_list.append(get_noises_from_weights(weights,nu=nu))\n",
    "        \n",
    "    # Positive rollouts (noise_sign=+1)\n",
    "    set_weights_list = [worker.set_weights.remote(weights,noises,noise_sign=+1) \n",
    "                        for worker,noises in zip(workers,noises_list)] \n",
    "    ops = [worker.rollout.remote() for worker in workers]\n",
    "    res_pos = ray.get(ops)\n",
    "    rollout_pos_vals,r_idx = np.zeros(n_workers),0\n",
    "    for rew,eplen in res_pos:\n",
    "        rollout_pos_vals[r_idx] = rew\n",
    "        r_idx = r_idx + 1\n",
    "        n_env_step += eplen\n",
    "    \n",
    "    # Negative rollouts (noise_sign=-1)\n",
    "    set_weights_list = [worker.set_weights.remote(weights,noises,noise_sign=-1) \n",
    "                        for worker,noises in zip(workers,noises_list)] \n",
    "    ops = [worker.rollout.remote() for worker in workers]\n",
    "    res_neg = ray.get(ops)\n",
    "    rollout_neg_vals,r_idx = np.zeros(n_workers),0\n",
    "    for rew,eplen in res_neg:\n",
    "        rollout_neg_vals[r_idx] = rew\n",
    "        r_idx = r_idx + 1\n",
    "        n_env_step += eplen\n",
    "    \n",
    "    # Reward \n",
    "    rollout_concat_vals = np.concatenate((rollout_pos_vals,rollout_neg_vals))\n",
    "    rollout_delta_vals = rollout_pos_vals - rollout_neg_vals \n",
    "    rollout_max_vals = np.maximum(rollout_pos_vals,rollout_neg_vals)\n",
    "    \n",
    "    # Sort\n",
    "    sort_idx = np.argsort(-rollout_max_vals)\n",
    "    \n",
    "    # Update\n",
    "    sigma_R = np.std(rollout_concat_vals)\n",
    "    weights_updated = []\n",
    "    for w_idx,weight in enumerate(weights): # for each weight \n",
    "        delta_weight_sum = np.zeros_like(weight)\n",
    "        for k in range(b):\n",
    "            idx = sort_idx[k] # sorted index\n",
    "            rollout_delta = rollout_delta_vals[idx]\n",
    "            noises_k = noises_list[idx]\n",
    "            noise_k = noises_k[w_idx] # noise for current weight\n",
    "            delta_weight_sum += rollout_delta*noise_k\n",
    "        delta_weight = (alpha/(b*sigma_R))*delta_weight_sum\n",
    "        weight = weight + delta_weight\n",
    "        weights_updated.append(weight) \n",
    "    \n",
    "    # Set weight\n",
    "    R.set_weights(weights_updated)\n",
    "    \n",
    "    # Evaluate\n",
    "    if (t == 0) or (((t+1)%evaluate_every) == 0) or (t == (total_steps-1)): \n",
    "        ram_percent = psutil.virtual_memory().percent # memory usage\n",
    "        print (\"[Evaluate] step:[%d/%d][%.1f%%] #step:[%.1e] time:[%s] ram:[%.1f%%].\"%\n",
    "               (t+1,total_steps,t/total_steps*100,\n",
    "                n_env_step,\n",
    "                time.strftime(\"%H:%M:%S\", time.gmtime(time.time()-start_time)),\n",
    "                ram_percent)\n",
    "              )\n",
    "        for eval_idx in range(num_eval): \n",
    "            o,d,ep_ret,ep_len = eval_env.reset(),False,0,0\n",
    "            if RENDER_ON_EVAL:\n",
    "                _ = eval_env.render(mode='human') \n",
    "            while not(d or (ep_len == max_ep_len_eval)):\n",
    "                a = R.get_action(o)\n",
    "                o,r,d,_ = eval_env.step(a)\n",
    "                if RENDER_ON_EVAL:\n",
    "                    _ = eval_env.render(mode='human') \n",
    "                ep_ret += r # compute return \n",
    "                ep_len += 1\n",
    "            print (\"[Evaluate] [%d/%d] ep_ret:[%.4f] ep_len:[%d]\"\n",
    "                %(eval_idx,num_eval,ep_ret,ep_len)) \n",
    "    \n",
    "print (\"Done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
