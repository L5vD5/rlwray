{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PPO with ADT Continuous Env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Packaged loaded. TF version is [1.15.0].\n"
     ]
    }
   ],
   "source": [
    "import datetime,gym,time,os,psutil,ray\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from util import open_txt,write_txt\n",
    "from ppo import PPOBuffer,create_ppo_model,create_ppo_graph,update_ppo,\\\n",
    "    save_ppo_model,restore_ppo_model\n",
    "from episci.environment_wrappers.tactical_action_adt_env_continuous \\\n",
    "    import CustomADTEnvContinuous\n",
    "from episci.agents.utils.constants import Agents,RewardType,StateInfo\n",
    "print (\"Packaged loaded. TF version is [%s].\"%(tf.__version__))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Worker\n",
    "exp_name = 'ppo_adt_cont'\n",
    "n_cpu = 31\n",
    "n_workers = 30\n",
    "\n",
    "# Environment\n",
    "action_length = 5 # 50/5 = 10HZ\n",
    "# Red agent distribution for training\n",
    "red_list_train = [\n",
    "    Agents.ZOMBIE,\n",
    "    Agents.SPOT_RANDOM,\n",
    "    Agents.BUD_FSM,\n",
    "    Agents.EXPERT_SYSTEM\n",
    "]\n",
    "# Red agent distribution for evaluation\n",
    "red_list_eval = [\n",
    "    Agents.ZOMBIE, \n",
    "    Agents.ROSIE, \n",
    "    Agents.BUD, \n",
    "    Agents.BUD_FSM, \n",
    "    Agents.EXPERT_SYSTEM\n",
    "]*n_workers\n",
    "red_list_eval = red_list_eval[:n_workers]\n",
    "num_eval = len(red_list_eval) # evaluation\n",
    "\n",
    "# Steps\n",
    "total_steps,evaluate_every,print_every = 5000,5,5\n",
    "ep_len_rollout = 3000 # 15,000/5\n",
    "buffer_size = int(3000*len(red_list_train))\n",
    "batch_size = int(2**15) \n",
    "\n",
    "# Network configuration\n",
    "hdims = [64,32,16]\n",
    "clip_ratio = 0.2\n",
    "pi_lr = 1e-5 # 3e-4\n",
    "vf_lr = 1e-4 # 1e-3\n",
    "epsilon = 1e-5 # 1e-2\n",
    "# Buffer\n",
    "gamma = 0.99 # 0.99\n",
    "lam = 0.95 # 0.95\n",
    "# Update\n",
    "train_pi_iters = 1000\n",
    "train_v_iters = 1000\n",
    "target_kl = 0.005 # 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_env(red_distribution=None):\n",
    "    from episci.environment_wrappers.tactical_action_adt_env_continuous \\\n",
    "        import CustomADTEnvContinuous\n",
    "    from episci.agents.utils.constants import Agents, RewardType\n",
    "    env_config = {\n",
    "        \"red_distribution\": red_distribution,\n",
    "        \"reward_type\": RewardType.SHAPED\n",
    "    }\n",
    "    return CustomADTEnvContinuous(env_config,action_length=action_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[../report/log/ppo_adt_cont] created.\n",
      "[../report/log/ppo_adt_cont/log_Aug-16-2020-03:15:41.txt] created.\n"
     ]
    }
   ],
   "source": [
    "txt_path = '../report/log/%s/log_%s.txt'%(\n",
    "    exp_name,\n",
    "    datetime.datetime.now().strftime(\"%b-%d-%Y-%H:%M:%S\"))\n",
    "f = open_txt(txt_path)\n",
    "print (\"[%s] created.\"%(txt_path))\n",
    "time.sleep(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rollout Workers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RolloutWorkerClass(object):\n",
    "    \"\"\"\n",
    "    Worker without RAY (for update purposes)\n",
    "    \"\"\"\n",
    "    def __init__(self,seed=1):\n",
    "        self.seed = seed\n",
    "        # Each worker should maintain its own environment\n",
    "        import gym\n",
    "        from util import suppress_tf_warning\n",
    "        suppress_tf_warning() # suppress TF warnings\n",
    "        gym.logger.set_level(40) # gym logger \n",
    "        self.env = get_env()\n",
    "        odim,adim = self.env.observation_space.shape[0],self.env.action_space.shape[0]\n",
    "        self.odim = odim\n",
    "        self.adim = adim\n",
    "        _ = self.env.reset(red=Agents.SPOT_RANDOM)\n",
    "        \n",
    "        # Initialize PPO\n",
    "        self.model,self.sess = create_ppo_model(env=self.env,hdims=hdims,output_actv=tf.nn.tanh)\n",
    "        self.graph = create_ppo_graph(self.model,\n",
    "                                      clip_ratio=clip_ratio,pi_lr=pi_lr,vf_lr=vf_lr,epsilon=epsilon)\n",
    "        # Initialize model \n",
    "        tf.set_random_seed(self.seed)\n",
    "        np.random.seed(self.seed)\n",
    "        self.sess.run(tf.global_variables_initializer())\n",
    "        \n",
    "        # Flag to initialize assign operations for 'set_weights()'\n",
    "        self.FIRST_SET_FLAG = True\n",
    "        \n",
    "    def get_action(self,o,deterministic=False):\n",
    "        act_op = self.model['mu'] if deterministic else self.model['pi']\n",
    "        return self.sess.run(act_op, feed_dict={self.model['o_ph']:o.reshape(1,-1)})[0]\n",
    "    \n",
    "    def get_weights(self):\n",
    "        \"\"\"\n",
    "        Get weights\n",
    "        \"\"\"\n",
    "        weight_vals = self.sess.run(self.model['pi_vars']+self.model['v_vars'])\n",
    "        return weight_vals\n",
    "    \n",
    "    def set_weights(self,weight_vals):\n",
    "        \"\"\"\n",
    "        Set weights without memory leakage\n",
    "        \"\"\"\n",
    "        if self.FIRST_SET_FLAG:\n",
    "            self.FIRST_SET_FLAG = False\n",
    "            self.assign_placeholders = []\n",
    "            self.assign_ops = []\n",
    "            for w_idx,weight_tf_var in enumerate(self.model['pi_vars']+self.model['v_vars']):\n",
    "                a = weight_tf_var\n",
    "                assign_placeholder = tf.placeholder(a.dtype, shape=a.get_shape())\n",
    "                assign_op = a.assign(assign_placeholder)\n",
    "                self.assign_placeholders.append(assign_placeholder)\n",
    "                self.assign_ops.append(assign_op)\n",
    "        for w_idx,weight_tf_var in enumerate(self.model['pi_vars']+self.model['v_vars']):\n",
    "            self.sess.run(self.assign_ops[w_idx],\n",
    "                          {self.assign_placeholders[w_idx]:weight_vals[w_idx]})    \n",
    "    \n",
    "@ray.remote\n",
    "class RayRolloutWorkerClass(object):\n",
    "    \"\"\"\n",
    "    Rollout Worker with RAY\n",
    "    \"\"\"\n",
    "    def __init__(self,worker_id=0,ep_len_rollout=1000,buffer_size=1000):\n",
    "        # Parse\n",
    "        self.worker_id = worker_id\n",
    "        self.ep_len_rollout = ep_len_rollout\n",
    "        self.buffer_size = buffer_size\n",
    "        # Each worker should maintain its own environment\n",
    "        import gym\n",
    "        from util import suppress_tf_warning\n",
    "        suppress_tf_warning() # suppress TF warnings\n",
    "        gym.logger.set_level(40) # gym logger \n",
    "        self.env = get_env()\n",
    "        odim,adim = self.env.observation_space.shape[0],self.env.action_space.shape[0]\n",
    "        self.odim = odim\n",
    "        self.adim = adim\n",
    "        _ = self.env.reset(red=Agents.SPOT_RANDOM)\n",
    "        \n",
    "        # Replay buffers to pass\n",
    "        self.o_buffer = np.zeros((self.buffer_size,self.odim))\n",
    "        self.a_buffer = np.zeros((self.buffer_size,self.adim))\n",
    "        self.r_buffer = np.zeros((self.buffer_size))\n",
    "        self.v_t_buffer = np.zeros((self.buffer_size))\n",
    "        self.logp_t_buffer = np.zeros((self.buffer_size))\n",
    "        # Create PPO model\n",
    "        self.model,self.sess = create_ppo_model(env=self.env,hdims=hdims,output_actv=tf.nn.tanh)\n",
    "        # Initialize model \n",
    "        self.sess.run(tf.global_variables_initializer())\n",
    "        # Buffer\n",
    "        self.buf = PPOBuffer(odim=self.odim,adim=self.adim,\n",
    "                             size=buffer_size,gamma=gamma,lam=lam)\n",
    "        print (\"Ray Worker [%d] Ready.\"%(self.worker_id))\n",
    "        \n",
    "        # Flag to initialize assign operations for 'set_weights()'\n",
    "        self.FIRST_SET_FLAG = True\n",
    "        \n",
    "        # Flag to initialize rollout\n",
    "        self.FIRST_ROLLOUT_FLAG = True\n",
    "        \n",
    "    def get_action(self,o,deterministic=False):\n",
    "        act_op = self.model['mu'] if deterministic else self.model['pi']\n",
    "        return self.sess.run(act_op, feed_dict={self.model['o_ph']:o.reshape(1,-1)})[0]\n",
    "    \n",
    "    def set_weights(self,weight_vals):\n",
    "        \"\"\"\n",
    "        Set weights without memory leakage\n",
    "        \"\"\"\n",
    "        if self.FIRST_SET_FLAG:\n",
    "            self.FIRST_SET_FLAG = False\n",
    "            self.assign_placeholders = []\n",
    "            self.assign_ops = []\n",
    "            for w_idx,weight_tf_var in enumerate(self.model['pi_vars']+self.model['v_vars']):\n",
    "                a = weight_tf_var\n",
    "                assign_placeholder = tf.placeholder(a.dtype, shape=a.get_shape())\n",
    "                assign_op = a.assign(assign_placeholder)\n",
    "                self.assign_placeholders.append(assign_placeholder)\n",
    "                self.assign_ops.append(assign_op)\n",
    "        for w_idx,weight_tf_var in enumerate(self.model['pi_vars']+self.model['v_vars']):\n",
    "            self.sess.run(self.assign_ops[w_idx],\n",
    "                          {self.assign_placeholders[w_idx]:weight_vals[w_idx]})    \n",
    "        \n",
    "    def rollout(self,red_list=[Agents.SPOT_RANDOM,Agents.EXPERT_SYSTEM]):\n",
    "        \"\"\"\n",
    "        Rollout\n",
    "        \"\"\"\n",
    "        if self.FIRST_ROLLOUT_FLAG:\n",
    "            self.FIRST_ROLLOUT_FLAG = False\n",
    "            self.o = self.env.reset(red=Agents.SPOT_RANDOM) # reset environment\n",
    "        # Loop\n",
    "        r_sum,cnt = 0,0\n",
    "        for r_idx,red in enumerate(red_list): # for each red policy\n",
    "            self.o = self.env.reset(red=red) # reset environment\n",
    "            for t in range(self.ep_len_rollout):\n",
    "                a,v_t,logp_t = self.sess.run(\n",
    "                    self.model['get_action_ops'],\n",
    "                    feed_dict={self.model['o_ph']:self.o.reshape(1,-1)})\n",
    "                o2, r, d, _ = self.env.step(a[0])\n",
    "                r_sum += r\n",
    "                cnt += 1\n",
    "                # save and log\n",
    "                self.buf.store(self.o,a,r,v_t,logp_t)\n",
    "                # Update obs (critical!)\n",
    "                self.o = o2\n",
    "                if d:\n",
    "                    self.buf.finish_path(last_val=0.0)\n",
    "                    self.o = self.env.reset(red=Agents.SPOT_RANDOM) # reset when done \n",
    "            last_val = self.sess.run(self.model['v'],\n",
    "                                     feed_dict={self.model['o_ph']:self.o.reshape(1,-1)})\n",
    "            self.buf.finish_path(last_val) # finish path buffer\n",
    "        r_avg = r_sum / cnt\n",
    "        return self.buf.get(),r_avg # obs_buf, act_buf, adv_buf, ret_buf, logp_buf\n",
    "    \n",
    "    def evaluate(self,red=None):\n",
    "        \"\"\"\n",
    "        Evaluate\n",
    "        \"\"\"\n",
    "        o,d,ep_ret,ep_len = self.env.reset(red=red),False,0,0\n",
    "        while not(d or (ep_len == self.ep_len_rollout)):\n",
    "            a = self.get_action(o,deterministic=True)\n",
    "            o,r,d,_ = self.env.step(a)\n",
    "            ep_ret += r # compute return \n",
    "            ep_len += 1\n",
    "        blue_health,red_health = self.env.blue_health,self.env.red_health\n",
    "        eval_res = [ep_ret,ep_len,blue_health,red_health] # evaluation result \n",
    "        return eval_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize Workers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-08-16 03:15:42,946\tINFO resource_spec.py:212 -- Starting Ray with 165.58 GiB memory available for workers and up to 74.97 GiB for objects. You can adjust these settings with ray.init(memory=<bytes>, object_store_memory=<bytes>).\n",
      "2020-08-16 03:15:43,182\tWARNING services.py:923 -- Redis failed to start, retrying now.\n",
      "2020-08-16 03:15:43,396\tINFO services.py:1165 -- View the Ray dashboard at \u001b[1m\u001b[32mlocalhost:8265\u001b[39m\u001b[22m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RAY initialized with [31] cpus and [30] workers.\n"
     ]
    }
   ],
   "source": [
    "ray.init(num_cpus=n_cpu)\n",
    "tf.reset_default_graph()\n",
    "R = RolloutWorkerClass(seed=0)\n",
    "workers = [RayRolloutWorkerClass.remote(\n",
    "    worker_id=i,ep_len_rollout=ep_len_rollout,buffer_size=buffer_size) \n",
    "           for i in range(n_workers)]\n",
    "print (\"RAY initialized with [%d] cpus and [%d] workers.\"%\n",
    "       (n_cpu,n_workers))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=54195)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=54195)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=54195)\u001b[0m      JSBSim Flight Dynamics Model v1.1.0.dev1 Jul 11 2020 05:35:14\n",
      "\u001b[2m\u001b[36m(pid=54171)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=54171)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=54171)\u001b[0m      JSBSim Flight Dynamics Model v1.1.0.dev1 Jul 11 2020 05:35:14\n",
      "\u001b[2m\u001b[36m(pid=54196)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=54196)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=54196)\u001b[0m      JSBSim Flight Dynamics Model v1.1.0.dev1 Jul 11 2020 05:35:14\n",
      "\u001b[2m\u001b[36m(pid=54182)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=54182)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=54182)\u001b[0m      JSBSim Flight Dynamics Model v1.1.0.dev1 Jul 11 2020 05:35:14\n",
      "\u001b[2m\u001b[36m(pid=54173)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=54173)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=54173)\u001b[0m      JSBSim Flight Dynamics Model v1.1.0.dev1 Jul 11 2020 05:35:14\n",
      "\u001b[2m\u001b[36m(pid=54174)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=54174)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=54174)\u001b[0m      JSBSim Flight Dynamics Model v1.1.0.dev1 Jul 11 2020 05:35:14\n",
      "\u001b[2m\u001b[36m(pid=54189)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=54189)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=54189)\u001b[0m      JSBSim Flight Dynamics Model v1.1.0.dev1 Jul 11 2020 05:35:14\n",
      "\u001b[2m\u001b[36m(pid=54193)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=54193)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=54193)\u001b[0m      JSBSim Flight Dynamics Model v1.1.0.dev1 Jul 11 2020 05:35:14\n",
      "\u001b[2m\u001b[36m(pid=54190)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=54190)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=54190)\u001b[0m      JSBSim Flight Dynamics Model v1.1.0.dev1 Jul 11 2020 05:35:14\n",
      "\u001b[2m\u001b[36m(pid=54185)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=54185)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=54185)\u001b[0m      JSBSim Flight Dynamics Model v1.1.0.dev1 Jul 11 2020 05:35:14\n",
      "\u001b[2m\u001b[36m(pid=54175)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=54175)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=54175)\u001b[0m      JSBSim Flight Dynamics Model v1.1.0.dev1 Jul 11 2020 05:35:14\n",
      "\u001b[2m\u001b[36m(pid=54172)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=54172)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=54172)\u001b[0m      JSBSim Flight Dynamics Model v1.1.0.dev1 Jul 11 2020 05:35:14\n",
      "\u001b[2m\u001b[36m(pid=54177)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=54177)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=54177)\u001b[0m      JSBSim Flight Dynamics Model v1.1.0.dev1 Jul 11 2020 05:35:14\n",
      "\u001b[2m\u001b[36m(pid=54180)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=54180)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=54180)\u001b[0m      JSBSim Flight Dynamics Model v1.1.0.dev1 Jul 11 2020 05:35:14\n",
      "\u001b[2m\u001b[36m(pid=54184)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=54184)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=54184)\u001b[0m      JSBSim Flight Dynamics Model v1.1.0.dev1 Jul 11 2020 05:35:14\n",
      "\u001b[2m\u001b[36m(pid=54188)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=54188)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=54188)\u001b[0m      JSBSim Flight Dynamics Model v1.1.0.dev1 Jul 11 2020 05:35:14\n",
      "\u001b[2m\u001b[36m(pid=54178)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=54178)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=54178)\u001b[0m      JSBSim Flight Dynamics Model v1.1.0.dev1 Jul 11 2020 05:35:14\n",
      "\u001b[2m\u001b[36m(pid=54186)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=54186)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=54186)\u001b[0m      JSBSim Flight Dynamics Model v1.1.0.dev1 Jul 11 2020 05:35:14\n",
      "\u001b[2m\u001b[36m(pid=54176)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=54176)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=54176)\u001b[0m      JSBSim Flight Dynamics Model v1.1.0.dev1 Jul 11 2020 05:35:14\n",
      "\u001b[2m\u001b[36m(pid=54198)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=54198)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=54198)\u001b[0m      JSBSim Flight Dynamics Model v1.1.0.dev1 Jul 11 2020 05:35:14\n",
      "\u001b[2m\u001b[36m(pid=54199)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=54199)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=54199)\u001b[0m      JSBSim Flight Dynamics Model v1.1.0.dev1 Jul 11 2020 05:35:14\n",
      "\u001b[2m\u001b[36m(pid=54194)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=54194)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=54194)\u001b[0m      JSBSim Flight Dynamics Model v1.1.0.dev1 Jul 11 2020 05:35:14\n",
      "\u001b[2m\u001b[36m(pid=54201)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=54201)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=54201)\u001b[0m      JSBSim Flight Dynamics Model v1.1.0.dev1 Jul 11 2020 05:35:14\n",
      "\u001b[2m\u001b[36m(pid=54192)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=54192)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=54192)\u001b[0m      JSBSim Flight Dynamics Model v1.1.0.dev1 Jul 11 2020 05:35:14\n",
      "\u001b[2m\u001b[36m(pid=54179)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=54179)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=54179)\u001b[0m      JSBSim Flight Dynamics Model v1.1.0.dev1 Jul 11 2020 05:35:14\n",
      "\u001b[2m\u001b[36m(pid=54187)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=54187)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=54187)\u001b[0m      JSBSim Flight Dynamics Model v1.1.0.dev1 Jul 11 2020 05:35:14\n",
      "\u001b[2m\u001b[36m(pid=54181)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=54181)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=54181)\u001b[0m      JSBSim Flight Dynamics Model v1.1.0.dev1 Jul 11 2020 05:35:14\n",
      "\u001b[2m\u001b[36m(pid=54183)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=54183)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=54183)\u001b[0m      JSBSim Flight Dynamics Model v1.1.0.dev1 Jul 11 2020 05:35:14\n",
      "\u001b[2m\u001b[36m(pid=54191)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=54191)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=54191)\u001b[0m      JSBSim Flight Dynamics Model v1.1.0.dev1 Jul 11 2020 05:35:14\n",
      "\u001b[2m\u001b[36m(pid=54200)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=54200)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=54200)\u001b[0m      JSBSim Flight Dynamics Model v1.1.0.dev1 Jul 11 2020 05:35:14\n",
      "\u001b[2m\u001b[36m(pid=54195)\u001b[0m Ray Worker [4] Ready.\n",
      "\u001b[2m\u001b[36m(pid=54175)\u001b[0m Ray Worker [16] Ready.\n",
      "\u001b[2m\u001b[36m(pid=54177)\u001b[0m Ray Worker [9] Ready.\n",
      "\u001b[2m\u001b[36m(pid=54182)\u001b[0m Ray Worker [19] Ready.\n",
      "\u001b[2m\u001b[36m(pid=54184)\u001b[0m Ray Worker [28] Ready.\n",
      "\u001b[2m\u001b[36m(pid=54174)\u001b[0m Ray Worker [24] Ready.\n",
      "\u001b[2m\u001b[36m(pid=54176)\u001b[0m Ray Worker [29] Ready.\n",
      "\u001b[2m\u001b[36m(pid=54189)\u001b[0m Ray Worker [23] Ready.\n",
      "\u001b[2m\u001b[36m(pid=54193)\u001b[0m Ray Worker [10] Ready.\n",
      "\u001b[2m\u001b[36m(pid=54190)\u001b[0m Ray Worker [25] Ready.\n",
      "\u001b[2m\u001b[36m(pid=54185)\u001b[0m Ray Worker [13] Ready.\n",
      "\u001b[2m\u001b[36m(pid=54172)\u001b[0m Ray Worker [7] Ready.\n",
      "\u001b[2m\u001b[36m(pid=54180)\u001b[0m Ray Worker [14] Ready.\n",
      "\u001b[2m\u001b[36m(pid=54171)\u001b[0m Ray Worker [17] Ready.\n",
      "\u001b[2m\u001b[36m(pid=54188)\u001b[0m Ray Worker [26] Ready.\n",
      "\u001b[2m\u001b[36m(pid=54178)\u001b[0m Ray Worker [12] Ready.\n",
      "\u001b[2m\u001b[36m(pid=54187)\u001b[0m Ray Worker [18] Ready.\n",
      "\u001b[2m\u001b[36m(pid=54173)\u001b[0m Ray Worker [21] Ready.\n",
      "\u001b[2m\u001b[36m(pid=54186)\u001b[0m Ray Worker [27] Ready.\n",
      "\u001b[2m\u001b[36m(pid=54198)\u001b[0m Ray Worker [3] Ready.\n",
      "\u001b[2m\u001b[36m(pid=54196)\u001b[0m Ray Worker [22] Ready.\n",
      "\u001b[2m\u001b[36m(pid=54194)\u001b[0m Ray Worker [2] Ready.\n",
      "\u001b[2m\u001b[36m(pid=54200)\u001b[0m Ray Worker [0] Ready.\n",
      "\u001b[2m\u001b[36m(pid=54201)\u001b[0m Ray Worker [1] Ready.\n",
      "\u001b[2m\u001b[36m(pid=54199)\u001b[0m Ray Worker [5] Ready.\n",
      "\u001b[2m\u001b[36m(pid=54179)\u001b[0m Ray Worker [6] Ready.\n",
      "\u001b[2m\u001b[36m(pid=54181)\u001b[0m Ray Worker [11] Ready.\n",
      "\u001b[2m\u001b[36m(pid=54191)\u001b[0m Ray Worker [20] Ready.\n",
      "\u001b[2m\u001b[36m(pid=54192)\u001b[0m Ray Worker [8] Ready.\n",
      "\u001b[2m\u001b[36m(pid=54183)\u001b[0m Ray Worker [15] Ready.\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "n_env_step = 0 # number of environment steps\n",
    "for t in range(int(total_steps)):\n",
    "    esec = time.time()-start_time\n",
    "    \n",
    "    # 1. Synchronize worker weights\n",
    "    weights = R.get_weights()\n",
    "    set_weights_list = [worker.set_weights.remote(weights) for worker in workers] \n",
    "    \n",
    "    # 2. Make rollout and accumulate to Buffers\n",
    "    t_start = time.time()\n",
    "    ops = [worker.rollout.remote(\n",
    "        red_list=red_list_train # <= with the list of pre-defined red agent policies\n",
    "    ) for worker in workers]\n",
    "    rollout_vals = ray.get(ops)\n",
    "    sec_rollout = time.time() - t_start\n",
    "    \n",
    "    # Get stats before update\n",
    "    t_start = time.time() # tic\n",
    "\n",
    "    # 3. Update the PPO model \n",
    "    r_sum = 0\n",
    "    for r_idx,rval in enumerate(rollout_vals): # concat all buffers from workers\n",
    "        obs_buf,act_buf,adv_buf,ret_buf,logp_buf,r_rollout_avg = \\\n",
    "            rval[0][0],rval[0][1],rval[0][2],rval[0][3],rval[0][4],rval[1]\n",
    "        if r_idx == 0:\n",
    "            obs_bufs,act_bufs,adv_bufs,ret_bufs,logp_bufs = \\\n",
    "                obs_buf,act_buf,adv_buf,ret_buf,logp_buf\n",
    "        else:\n",
    "            obs_bufs = np.concatenate((obs_bufs,obs_buf),axis=0)\n",
    "            act_bufs = np.concatenate((act_bufs,act_buf),axis=0)\n",
    "            adv_bufs = np.concatenate((adv_bufs,adv_buf),axis=0)\n",
    "            ret_bufs = np.concatenate((ret_bufs,ret_buf),axis=0)\n",
    "            logp_bufs = np.concatenate((logp_bufs,logp_buf),axis=0)\n",
    "        r_sum += r_rollout_avg\n",
    "    r_avg = r_sum / len(rollout_vals)\n",
    "    n_val_total = obs_bufs.shape[0] # total buffer size \n",
    "    for pi_iter in range(train_pi_iters): # update actor\n",
    "        rand_idx = np.random.permutation(n_val_total)[:batch_size]\n",
    "        buf_batches = [obs_bufs[rand_idx],act_bufs[rand_idx],adv_bufs[rand_idx],\n",
    "                       ret_bufs[rand_idx],logp_bufs[rand_idx]]\n",
    "        feeds = {k:v for k,v in zip(R.model['all_phs'],buf_batches)}\n",
    "        _,kl,pi_loss,ent = R.sess.run([R.graph['train_pi'],R.graph['approx_kl'],\n",
    "                               R.graph['pi_loss'],R.graph['approx_ent']],\n",
    "                           feed_dict=feeds)        \n",
    "        if kl > 1.5 * target_kl:\n",
    "            # print (\"  pi_iter:[%d] kl(%.3f) is higher than 1.5x(%.3f)\"%(pi_iter,kl,target_kl))\n",
    "            break\n",
    "    for _ in range(train_v_iters): # update critic\n",
    "        rand_idx = np.random.permutation(n_val_total)[:batch_size]\n",
    "        buf_batches = [obs_bufs[rand_idx],act_bufs[rand_idx],adv_bufs[rand_idx],\n",
    "                       ret_bufs[rand_idx],logp_bufs[rand_idx]]\n",
    "        feeds = {k:v for k,v in zip(R.model['all_phs'],buf_batches)}\n",
    "        R.sess.run(R.graph['train_v'],feed_dict=feeds)\n",
    "    sec_update = time.time() - t_start # toc\n",
    "    \n",
    "    # 4. Synchronize worker weights (after update)\n",
    "    weights = R.get_weights()\n",
    "    set_weights_list = [worker.set_weights.remote(weights) for worker in workers] \n",
    "    \n",
    "    # Print\n",
    "    if (t == 0) or (((t+1)%print_every) == 0): \n",
    "        print (\"[%d/%d] rollout:[%.1f]s pi_iter:[%d/%d] update:[%.1f]s kl:[%.4f] target_kl:[%.4f].\"%\n",
    "               (t+1,total_steps,sec_rollout,pi_iter,train_pi_iters,sec_update,kl,target_kl))\n",
    "        print (\"   pi_loss:[%.4f], entropy:[%.4f], r_avg[%.4f]\"%\n",
    "               (pi_loss,ent,r_avg))\n",
    "        \n",
    "    # 5. Evaluate\n",
    "    if (t == 0) or (((t+1)%evaluate_every) == 0): \n",
    "        ram_percent = psutil.virtual_memory().percent # memory usage\n",
    "        print (\"[Eval. start] step:[%d/%d][%.1f%%] #step:[%.1e] time:[%s] ram:[%.1f%%].\"%\n",
    "               (t+1,total_steps,t/total_steps*100,\n",
    "                n_env_step,\n",
    "                time.strftime(\"day:[%d] %H:%M:%S\", time.gmtime(time.time()-start_time)),\n",
    "                ram_percent)\n",
    "              )\n",
    "        ops = []\n",
    "        for i_idx in range(num_eval):\n",
    "            worker,red = workers[i_idx],red_list_eval[i_idx]\n",
    "            ops.append(worker.evaluate.remote(red=red))\n",
    "        eval_vals = ray.get(ops)\n",
    "        ep_ret_sum = 0\n",
    "        for i_idx in range(num_eval):\n",
    "            red,eval_val = red_list_eval[i_idx],eval_vals[i_idx]\n",
    "            ep_ret,ep_len,blue_health,red_health = eval_val[0],eval_val[1],eval_val[2],eval_val[3]\n",
    "            ep_ret_sum += ep_ret\n",
    "            print (\" [%d/%d] [%s] ep_ret:[%.4f] ep_len:[%d]. blue health:[%.2f] red health:[%.2f]\"\n",
    "                %(i_idx,len(eval_vals),red,ep_ret,ep_len,blue_health,red_health))\n",
    "        ep_ret_avg = ep_ret_sum / num_eval\n",
    "        print (\"[Eval. done] time:[%s] ep_ret_avg:[%.3f].\"%\n",
    "               (time.strftime(\"day:[%d] %H:%M:%S\", time.gmtime(time.time()-start_time)),ep_ret_avg))\n",
    "        write_txt(f,\"%.2f, r_train:%.4f, ret_eval:%.4f\"%(time.time()-start_time, r_avg, ep_ret_avg),\n",
    "                  ADD_NEWLINE=True,DO_PRINT=False)\n",
    "        # Save current PPO model\n",
    "        npz_path = '../report/net/%s/model_%d.npz'%(exp_name,t+1)\n",
    "        save_ppo_model(npz_path,R,VERBOSE=False)\n",
    "        \n",
    "\n",
    "print (\"Done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
