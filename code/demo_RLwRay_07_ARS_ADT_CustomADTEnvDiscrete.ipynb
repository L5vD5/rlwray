{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ARS with CustomADTEnvDiscrete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime,gym,time,os,psutil,ray\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from util import gpu_sess,suppress_tf_warning,tic,toc,open_txt,write_txt,OnlineMeanVariance,\\\n",
    "    arr2idx,idx2arr\n",
    "from ars import create_ars_model,get_noises_from_weights,save_ars_model,restore_ars_model\n",
    "np.set_printoptions(precision=2)\n",
    "suppress_tf_warning() # suppress warning \n",
    "gym.logger.set_level(40) # gym logger \n",
    "\n",
    "from episci.environment_wrappers.tactical_action_adt_env_discrete import CustomADTEnvDiscrete\n",
    "from episci.agents.utils.constants import Agents,RewardType,StateInfo\n",
    "print (\"Packaged loaded. TF version is [%s].\"%(tf.__version__))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_name = 'ars_adt_disc'\n",
    "n_cpu = 91\n",
    "n_workers = 90\n",
    "total_steps,evaluate_every,print_every = 5000,5,1\n",
    "ep_len_rollout = 50*300 # 50*300 = 15000 \n",
    "ep_len_rollout_eval = 15000 \n",
    "hdims,actv,out_actv = [64,64],tf.nn.relu,None\n",
    "# alpha:stepsize / nu:exploration std / b: elite set size\n",
    "alpha,nu,b = 0.005,0.005,(n_workers//20)\n",
    "seed = 0\n",
    "# Environment\n",
    "action_length = 1 # 50/1 = 50HZ\n",
    "md_info = np.array([4,4,3,3]) # multi discrete information\n",
    "# Train\n",
    "red_list_train = [\n",
    "    Agents.ZOMBIE,\n",
    "    Agents.SPOT_RANDOM,\n",
    "    Agents.BUD_FSM,\n",
    "    Agents.EXPERT_SYSTEM\n",
    "]\n",
    "# Evaluation\n",
    "red_list_eval = [\n",
    "    Agents.ZOMBIE, \n",
    "    Agents.ROSIE, \n",
    "    Agents.BUD, \n",
    "    Agents.BUD_FSM, \n",
    "    Agents.EXPERT_SYSTEM_TRIAL_2, \n",
    "    Agents.EXPERT_SYSTEM_TRIAL_3_SCRIMMAGE_4, \n",
    "    Agents.EXPERT_SYSTEM\n",
    "]*n_workers\n",
    "red_list_eval = red_list_eval[:n_workers]\n",
    "num_eval,max_ep_len_eval = len(red_list_eval),15e3 # evaluation\n",
    "# Restore\n",
    "npz_path = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "txt_path = '../log/%s/log_%s.txt'%(\n",
    "    exp_name,\n",
    "    datetime.datetime.now().strftime(\"%b-%d-%Y-%H:%M:%S\"))\n",
    "f = open_txt(txt_path)\n",
    "print (\"[%s] created.\"%(txt_path))\n",
    "time.sleep(1) # wait "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_env(red_distribution=None):\n",
    "    from episci.environment_wrappers.tactical_action_adt_env_discrete import CustomADTEnvDiscrete\n",
    "    from episci.agents.utils.constants import Agents, RewardType\n",
    "    env_config = {\n",
    "        \"red_distribution\": red_distribution,\n",
    "        \"reward_type\": RewardType.SHAPED\n",
    "    }\n",
    "    return CustomADTEnvDiscrete(env_config,action_length=action_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Worker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RolloutWorkerClass(object):\n",
    "    \"\"\"\n",
    "    Worker without RAY (for update purposes)\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 hdims=[64]*2,actv=tf.nn.relu,out_actv=tf.nn.tanh,\n",
    "                 seed=1):\n",
    "        self.seed = seed\n",
    "        from util import suppress_tf_warning\n",
    "        suppress_tf_warning() # suppress TF warnings\n",
    "        self.env = get_env()\n",
    "        odim,adim = self.env.observation_space.shape[0],self.env.action_space.shape[0]\n",
    "        adim = np.sum(md_info)\n",
    "        self.odim,self.adim = odim,adim\n",
    "        # Observation normalization\n",
    "        self.obs_mu = np.zeros(self.odim)\n",
    "        self.obs_std = np.ones(self.odim)\n",
    "        # ARS model \n",
    "        self.model,self.sess = create_ars_model(\n",
    "            odim=self.odim,adim=self.adim,hdims=hdims,\n",
    "            actv=actv,out_actv=out_actv)\n",
    "        # Initialize model \n",
    "        tf.set_random_seed(self.seed)\n",
    "        np.random.seed(self.seed)\n",
    "        self.sess.run(tf.global_variables_initializer())\n",
    "        # Flag to initialize assign operations for 'set_weights()'\n",
    "        self.FIRST_SET_FLAG = True\n",
    "    def set_observation_stats(self,obs_mu,obs_std):\n",
    "        self.obs_mu = obs_mu\n",
    "        self.obs_std = obs_std\n",
    "    def get_action(self,o):\n",
    "        obs_std = self.obs_std\n",
    "        obs_std[obs_std<1e-6] = np.inf\n",
    "        nzd_o = (o-self.obs_mu)/obs_std\n",
    "        a = self.sess.run(\n",
    "            self.model['mu'],feed_dict={self.model['o_ph']:nzd_o.reshape(1,-1)})[0]\n",
    "        \n",
    "        # Convert to multi-discrete array\n",
    "        md_arr = np.zeros(len(md_info))\n",
    "        md_cumsum = np.cumsum(md_info)\n",
    "        for a_idx in range(len(md_info)): # for each discrete action\n",
    "            if a_idx == 0:\n",
    "                fr_idx = 0\n",
    "            else:\n",
    "                fr_idx = md_cumsum[a_idx-1]\n",
    "            to_idx = md_cumsum[a_idx]\n",
    "            md_arr[a_idx] = np.argmax(a[fr_idx:to_idx]) # argmax\n",
    "        md_arr = md_arr.astype(np.int) # to integer\n",
    "        \n",
    "        \n",
    "        return md_arr\n",
    "    def get_weights(self):\n",
    "        weight_vals = self.sess.run(self.model['main_vars'])\n",
    "        return weight_vals\n",
    "    def set_weights(self,weight_vals):\n",
    "        if self.FIRST_SET_FLAG:\n",
    "            self.FIRST_SET_FLAG = False\n",
    "            self.assign_placeholders = []\n",
    "            self.assign_ops = []\n",
    "            for w_idx,weight_tf_var in enumerate(self.model['main_vars']):\n",
    "                a = weight_tf_var\n",
    "                assign_placeholder = tf.placeholder(a.dtype, shape=a.get_shape())\n",
    "                assign_op = a.assign(assign_placeholder)\n",
    "                self.assign_placeholders.append(assign_placeholder)\n",
    "                self.assign_ops.append(assign_op)\n",
    "        for w_idx,weight_tf_var in enumerate(self.model['main_vars']):\n",
    "            self.sess.run(self.assign_ops[w_idx],\n",
    "                          {self.assign_placeholders[w_idx]:weight_vals[w_idx]})\n",
    "            \n",
    "@ray.remote\n",
    "class RayRolloutWorkerClass(object):\n",
    "    \"\"\"\n",
    "    Rollout Worker with RAY\n",
    "    \"\"\"\n",
    "    def __init__(self,worker_id=0,\n",
    "                 hdims=[128],actv=tf.nn.relu,out_actv=tf.nn.tanh,\n",
    "                 ep_len_rollout=15000,ep_len_rollout_eval=15000):\n",
    "        self.worker_id = worker_id\n",
    "        self.ep_len_rollout = ep_len_rollout\n",
    "        self.ep_len_rollout_eval = ep_len_rollout_eval\n",
    "        from util import suppress_tf_warning\n",
    "        suppress_tf_warning() # suppress TF warnings\n",
    "        self.env = get_env()\n",
    "        odim,adim = self.env.observation_space.shape[0],self.env.action_space.shape[0]\n",
    "        adim = np.sum(md_info)\n",
    "        self.odim,self.adim = odim,adim\n",
    "        # Observation normalization\n",
    "        self.obs_mu = np.zeros(self.odim)\n",
    "        self.obs_std = np.ones(self.odim)\n",
    "        # ARS model \n",
    "        self.model,self.sess = create_ars_model(\n",
    "            odim=self.odim,adim=self.adim,hdims=hdims,\n",
    "            actv=actv,out_actv=out_actv)\n",
    "        # Flag to initialize assign operations for 'set_weights()'\n",
    "        self.FIRST_SET_FLAG = True        \n",
    "    def set_observation_stats(self,obs_mu,obs_std):\n",
    "        self.obs_mu = np.copy(obs_mu) # call by value\n",
    "        self.obs_std = np.copy(obs_std) # call by value\n",
    "    def get_action(self,o):\n",
    "        obs_std = self.obs_std\n",
    "        obs_std[obs_std<1e-6] = np.inf\n",
    "        nzd_o = (o-self.obs_mu)/obs_std # use whitened observation \n",
    "        a = self.sess.run(\n",
    "            self.model['mu'],feed_dict={self.model['o_ph']:nzd_o.reshape(1,-1)})[0]\n",
    "        \n",
    "        # Convert to multi-discrete array\n",
    "        md_arr = np.zeros(len(md_info))\n",
    "        md_cumsum = np.cumsum(md_info)\n",
    "        for a_idx in range(len(md_info)): # for each discrete action\n",
    "            if a_idx == 0:\n",
    "                fr_idx = 0\n",
    "            else:\n",
    "                fr_idx = md_cumsum[a_idx-1]\n",
    "            to_idx = md_cumsum[a_idx]\n",
    "            md_arr[a_idx] = np.argmax(a[fr_idx:to_idx]) # argmax\n",
    "        md_arr = md_arr.astype(np.int) # to integer\n",
    "        \n",
    "        return md_arr\n",
    "    def set_weights(self,weight_vals,noise_vals,noise_sign=+1):\n",
    "        if self.FIRST_SET_FLAG:\n",
    "            self.FIRST_SET_FLAG = False\n",
    "            self.assign_placeholders = []\n",
    "            self.assign_ops = []\n",
    "            for w_idx,weight_tf_var in enumerate(self.model['main_vars']):\n",
    "                a = weight_tf_var\n",
    "                assign_placeholder = tf.placeholder(a.dtype, shape=a.get_shape())\n",
    "                assign_op = a.assign(assign_placeholder)\n",
    "                self.assign_placeholders.append(assign_placeholder)\n",
    "                self.assign_ops.append(assign_op)\n",
    "        for w_idx,weight_tf_var in enumerate(self.model['main_vars']):\n",
    "            self.sess.run(self.assign_ops[w_idx],\n",
    "                          {self.assign_placeholders[w_idx]:\n",
    "                           weight_vals[w_idx]+noise_sign*noise_vals[w_idx]})\n",
    "    def rollout(self,\n",
    "                red_list=[Agents.SPOT_RANDOM,Agents.EXPERT_SYSTEM]):\n",
    "        \"\"\"\n",
    "        Rollout\n",
    "        \"\"\"\n",
    "        obs_buffer,obs_cnt = np.zeros((len(red_list)*self.ep_len_rollout,self.odim)),0\n",
    "        r_sum,n_step = 0,0 # reward sum and total steps\n",
    "        for r_idx,red in enumerate(red_list): # for each red policy\n",
    "            # Specify red policy\n",
    "            self.o = self.env.reset(red=red) # reset env with specified a red agent\n",
    "            for t in range(self.ep_len_rollout):\n",
    "                self.a = self.get_action(self.o) \n",
    "                self.o2,self.r,self.d,_ = self.env.step(self.a)\n",
    "                # Save next state \n",
    "                self.o = self.o2\n",
    "                # Accumulate reward\n",
    "                r_sum += self.r\n",
    "                n_step += 1\n",
    "                # Stack observation\n",
    "                obs_buffer[obs_cnt,:] = self.o\n",
    "                obs_cnt += 1\n",
    "                if self.d: \n",
    "                    break \n",
    "        # Compute the average return and steps \n",
    "        r_avg = r_sum / len(red_list)\n",
    "        n_step_avg = n_step / len(red_list)\n",
    "        obs_buffer = obs_buffer[:obs_cnt,:] # trim observation buffer \n",
    "        return r_avg,n_step_avg,obs_buffer\n",
    "    \n",
    "    def evaluate(self,red=None):\n",
    "        o,d,ep_ret,ep_len = self.env.reset(red=red),False,0,0\n",
    "        while not(d or (ep_len == self.ep_len_rollout_eval)):\n",
    "            a = self.get_action(o)\n",
    "            o,r,d,_ = self.env.step(a)\n",
    "            ep_ret += r # compute return \n",
    "            ep_len += 1\n",
    "        blue_health,red_health = self.env.blue_health,self.env.red_health\n",
    "        # Other infos\n",
    "        blue_height = self.env.manager._blue.state[0]\n",
    "        red_height = self.env.manager._red.state[0]\n",
    "        \n",
    "        # return / length / blue health / red health / blue height / red height\n",
    "        eval_res = [ep_ret,ep_len,blue_health,red_health,blue_height,red_height]\n",
    "        return eval_res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = get_env()\n",
    "adim,odim = env.action_space.shape[0],env.observation_space.shape[0]\n",
    "adim = np.sum(md_info)\n",
    "print (\"Environment Ready. odim:[%d] adim:[%d].\"%(odim,adim))\n",
    "write_txt(f,\"Environment Ready. odim:[%d] adim:[%d].\"%(odim,adim),\n",
    "          ADD_NEWLINE=True,DO_PRINT=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observation online normalizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mv = OnlineMeanVariance()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize Workers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ray.init(num_cpus=n_cpu)\n",
    "tf.reset_default_graph()\n",
    "R = RolloutWorkerClass(hdims=hdims,actv=actv,out_actv=out_actv,seed=seed)\n",
    "workers = [RayRolloutWorkerClass.remote(\n",
    "    worker_id=i,hdims=hdims,actv=actv,out_actv=out_actv,\n",
    "    ep_len_rollout=ep_len_rollout,ep_len_rollout_eval=ep_len_rollout_eval)\n",
    "           for i in range(n_workers)]\n",
    "print (\"RAY initialized with [%d] cpus and [%d] workers.\"%\n",
    "       (n_cpu,n_workers))\n",
    "write_txt(f,\"RAY initialized with [%d] cpus and [%d] workers.\"%(n_cpu,n_workers),\n",
    "          ADD_NEWLINE=True,DO_PRINT=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Restore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if npz_path:\n",
    "    restore_ars_model(npz_path,R,VERBOSE=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "n_env_step = 0 # number of environment steps\n",
    "for t in range(int(total_steps)): # for all steps \n",
    "    esec = time.time()-start_time\n",
    "    \n",
    "    # 1. Distribute the central weights to distributed workers\n",
    "    weights = R.get_weights() # weights of the central worker \n",
    "    noises_list = []\n",
    "    for _ in range(n_workers):\n",
    "        noises_list.append(get_noises_from_weights(weights,nu=nu))\n",
    "    \n",
    "    # 2. Positive rollouts using distributed workers\n",
    "    set_weights_list = [worker.set_weights.remote(weights,noises,noise_sign=+1) \n",
    "                        for worker,noises in zip(workers,noises_list)] # set weights\n",
    "    rollout_ops = [worker.rollout.remote(\n",
    "        red_list=red_list_train\n",
    "    )\n",
    "           for worker in workers] # do positive rollouts\n",
    "    res_pos_rollout = ray.get(rollout_ops) # get positive rollout results\n",
    "    pos_rets,r_idx = np.zeros(n_workers),0\n",
    "    for pos_ret,ep_len,obs_buffer in res_pos_rollout:\n",
    "        pos_rets[r_idx] = pos_ret # return\n",
    "        r_idx = r_idx + 1\n",
    "        n_env_step += ep_len # accumulate episode length\n",
    "        for obs in obs_buffer: mv.include(obs) # update observation mean and std\n",
    "    \n",
    "    # 3. Negative rollouts using distributed workers\n",
    "    set_weights_list = [worker.set_weights.remote(weights,noises,noise_sign=-1) \n",
    "                        for worker,noises in zip(workers,noises_list)] # set weights\n",
    "    rollout_ops = [worker.rollout.remote(\n",
    "        red_list=red_list_train\n",
    "    )\n",
    "           for worker in workers] # do negative rollouts\n",
    "    res_neg_rollout = ray.get(rollout_ops) # get negative rollout results\n",
    "    neg_rets,r_idx = np.zeros(n_workers),0\n",
    "    for neg_ret,ep_len,obs_buffer in res_neg_rollout:\n",
    "        neg_rets[r_idx] = neg_ret # return\n",
    "        r_idx = r_idx + 1\n",
    "        n_env_step += ep_len # accumulate episode length\n",
    "        for obs in obs_buffer: mv.include(obs) # update observation mean and std\n",
    "    \n",
    "    # 4. Compute return statistics and Update\n",
    "    concat_rets = np.concatenate((pos_rets,neg_rets)) # concatenated returns [2*n_workers]\n",
    "    ret_deltas = pos_rets - neg_rets # return difference [n_workers]\n",
    "    max_rets = np.maximum(pos_rets,neg_rets) # maximum returns [n_workers]\n",
    "    max_ret = np.max(max_rets) # maximum return [1]\n",
    "    max_ret_delta = np.max(np.abs(ret_deltas)) # maximum return diff [1]\n",
    "    avg_ret = np.mean(max_rets) # average return [1]\n",
    "    sort_idx = np.argsort(-max_rets) # sort for resampling\n",
    "    sigma_R = np.std(concat_rets)\n",
    "    weights_updated = []\n",
    "    for w_idx,weight in enumerate(weights): # for each weight \n",
    "        delta_weight_sum = np.zeros_like(weight)\n",
    "        for k in range(b):\n",
    "            idx_k = sort_idx[k] # sorted index\n",
    "            ret_delta_k,noises_k = ret_deltas[idx_k],noises_list[idx_k]\n",
    "            noise_k = (1/nu)*noises_k[w_idx] # noise for current weight\n",
    "            delta_weight_sum += ret_delta_k*noise_k\n",
    "        delta_weight = (alpha/(b*sigma_R))*delta_weight_sum\n",
    "        weight = weight + delta_weight\n",
    "        weights_updated.append(weight) \n",
    "    \n",
    "    # 5. Set weights of the central worker \n",
    "    R.set_weights(weights_updated)\n",
    "    \n",
    "    # 6. Distribute the central weights to the distributed workers\n",
    "    weights = R.get_weights() # get the updated weights from the central worker\n",
    "    zero_noises_list = []\n",
    "    for _ in range(n_workers):\n",
    "        zero_noises_list.append(get_noises_from_weights(weights,nu=0))\n",
    "    set_weights_list = [worker.set_weights.remote(weights,zero_noises,noise_sign=0) \n",
    "                        for worker,zero_noises in zip(workers,zero_noises_list)] \n",
    "    \n",
    "    # Print\n",
    "    if (t == 0) or (((t+1)%print_every) == 0):\n",
    "        print (\"[%d/%d] time:[%s] max_ret:[%.2f] avg_ret:[%.2f] sigma_R:[%.2f] \"%\n",
    "               (t,total_steps,time.strftime(\"day:[%d] %H:%M:%S\", time.gmtime(time.time()-start_time)),\n",
    "               max_ret,avg_ret,sigma_R))\n",
    "        write_txt(f,\n",
    "                  \"[%d/%d] time:[%s] max_ret:[%.2f] avg_ret:[%.2f] sigma_R:[%.2f] \"%\n",
    "                  (t,total_steps,time.strftime(\"day:[%d] %H:%M:%S\", time.gmtime(time.time()-start_time)),\n",
    "                   max_ret,avg_ret,sigma_R),\n",
    "                  ADD_NEWLINE=True,DO_PRINT=False)\n",
    "    \n",
    "    # Evaluate\n",
    "    if (t == 0) or (((t+1)%evaluate_every) == 0): \n",
    "        ram_percent = psutil.virtual_memory().percent # memory usage\n",
    "        print (\"[Eval. start] step:[%d/%d][%.1f%%] #step:[%.1e] time:[%s] ram:[%.1f%%].\"%\n",
    "               (t+1,total_steps,t/total_steps*100,n_env_step,\n",
    "                time.strftime(\"day:[%d] %H:%M:%S\", time.gmtime(time.time()-start_time)),\n",
    "                ram_percent)\n",
    "              )\n",
    "        write_txt(f,\n",
    "                  \"[Eval. start] step:[%d/%d][%.1f%%] #step:[%.1e] time:[%s] ram:[%.1f%%].\"%\n",
    "                  (t+1,total_steps,t/total_steps*100,\n",
    "                   n_env_step,\n",
    "                   time.strftime(\"day:[%d] %H:%M:%S\", time.gmtime(time.time()-start_time)),\n",
    "                   ram_percent),\n",
    "                  ADD_NEWLINE=True,DO_PRINT=False)\n",
    "        ops = []\n",
    "        for i_idx in range(num_eval):\n",
    "            worker,red = workers[i_idx],red_list_eval[i_idx]\n",
    "            ops.append(worker.evaluate.remote(red=red))\n",
    "        eval_vals = ray.get(ops)\n",
    "        \n",
    "        ep_ret_sum = 0\n",
    "        for i_idx in range(num_eval):\n",
    "            red,eval_val = red_list_eval[i_idx],eval_vals[i_idx]\n",
    "            ep_ret,ep_len,blue_health,red_health = eval_val[0],eval_val[1],eval_val[2],eval_val[3]\n",
    "            blue_height,red_height = eval_val[4],eval_val[5]\n",
    "            ep_ret_sum += ep_ret\n",
    "            print (\" [%d/%d][%s] Ret:[%.2f] Len:[%d]. Health B:[%.2f] R:[%.2f] Height B:[%.1e] R:[%.1e]\"\n",
    "                %(i_idx,len(eval_vals),red,ep_ret,ep_len,blue_health,red_health,blue_height,red_height))\n",
    "            write_txt(f,\n",
    "                      \" [%d/%d][%s] Ret:[%.2f] Len:[%d]. Health B:[%.2f] R:[%.2f] Height B:[%.1e] R:[%.1e]\"\n",
    "                      %(i_idx,len(eval_vals),red,ep_ret,ep_len,blue_health,red_health,blue_height,red_height),\n",
    "                      ADD_NEWLINE=True,DO_PRINT=False)\n",
    "        ep_ret_avg = ep_ret_sum / num_eval\n",
    "        print (\"[Eval. done] Time:[%s] Ret_Avg:[%.3f].\\n\"%\n",
    "               (time.strftime(\"day:[%d] %H:%M:%S\", time.gmtime(time.time()-start_time)),\n",
    "                ep_ret_avg)\n",
    "              )\n",
    "        write_txt(f,\n",
    "                  \"[Eval. done] Time:[%s] Ret_Avg:[%.3f].\\n\"%\n",
    "                  (time.strftime(\"day:[%d] %H:%M:%S\", time.gmtime(time.time()-start_time)),\n",
    "                   ep_ret_avg),\n",
    "                  ADD_NEWLINE=True,DO_PRINT=False)\n",
    "        # Save\n",
    "        npz_path = '../data/net/%s/model_%d.npz'%(exp_name,t+1)\n",
    "        save_ars_model(npz_path,R,mv=mv,VERBOSE=False)\n",
    "        write_txt(f,\n",
    "                  \" [%s] saved.\"%npz_path,\n",
    "                  ADD_NEWLINE=True,DO_PRINT=False)\n",
    "    \n",
    "    # 7. Distribute observation mean and std to workers (after evaluation)\n",
    "    obs_mean,obs_std = mv.mean,mv.std\n",
    "    sef_obs_list= [worker.set_observation_stats.remote(obs_mean,obs_std) \n",
    "                   for worker in workers] # set observation mean and std\n",
    "    \n",
    "    # Loop \n",
    "    # break # for debugging \n",
    "    \n",
    "print (\"Done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
