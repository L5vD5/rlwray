{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation of SAC agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime,gym,time,os,psutil,ray\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from util import gpu_sess,suppress_tf_warning,tic,toc,open_txt,write_txt\n",
    "from sac import ReplayBuffer,create_sac_model,create_sac_graph,\\\n",
    "    save_sac_model_and_buffers,restore_sac_model_and_buffers\n",
    "np.set_printoptions(precision=2)\n",
    "suppress_tf_warning() # suppress warning \n",
    "gym.logger.set_level(40) # gym logger \n",
    "\n",
    "from episci.environment_wrappers.tactical_action_adt_env_continuous import CustomADTEnvContinuous\n",
    "from episci.agents.utils.constants import Agents, RewardType\n",
    "print (\"Packaged loaded. TF version is [%s].\"%(tf.__version__))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ray\n",
    "n_cpu = 30\n",
    "n_workers = 30\n",
    "# Restore\n",
    "npz_path_restore = '../data/net/sac_adt_cont_a/model_and_buffers_1550.npz' \n",
    "# Environment\n",
    "# action_length,action_length_eval = 5,5 # 50/5=10HZ\n",
    "action_length,action_length_eval = 10,10 # 50/1=50HZ\n",
    "ema = 0.9 # exponential moving average of actions\n",
    "# Evaluation\n",
    "red_list_eval = [\n",
    "    Agents.ZOMBIE, \n",
    "    Agents.ROSIE, \n",
    "    Agents.BUD, \n",
    "    Agents.BUD_FSM, \n",
    "    Agents.EXPERT_SYSTEM_TRIAL_2, \n",
    "    Agents.EXPERT_SYSTEM_TRIAL_3_SCRIMMAGE_4, \n",
    "    Agents.EXPERT_SYSTEM\n",
    "    ]*4\n",
    "num_eval,max_ep_len_eval = len(red_list_eval),15e3 # evaluation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Rollouts\n",
    "total_steps,burnin_steps = 50000,5\n",
    "evaluate_every,print_every = 50,5\n",
    "\n",
    "ep_len_rollout = 10*150 # 150sec rollout\n",
    "hdims,actv = [128,128],tf.nn.relu\n",
    "red_list_train = {\n",
    "    Agents.SPOT_4G: 0.15,\n",
    "    Agents.SPOT_5G: 0.30,\n",
    "    Agents.SPOT_RANDOM: 0.45,\n",
    "    Agents.EXPERT_SYSTEM_TRIAL_2: 0.6,\n",
    "    Agents.EXPERT_SYSTEM_TRIAL_3_SCRIMMAGE_4: 0.75,\n",
    "    Agents.EXPERT_SYSTEM: 1.0\n",
    "    }\n",
    "# Learning hyp\n",
    "batch_size,update_count = 2**16,500 # batchsize / number of updates\n",
    "lr = 1e-4\n",
    "epsilon = 1e-2\n",
    "# SAC\n",
    "gamma = 0.99 # discount 0.99\n",
    "alpha_q,alpha_pi = 0.05,0.5\n",
    "polyak = 0.995 # 0.995\n",
    "# Buffer\n",
    "buffer_sz_long,buffer_sz_short = 1e5,1e4\n",
    "# Temperature % epsilon greediness \n",
    "temp_min,temp_max = 1.0,1.0\n",
    "eps_greedy = 0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Environments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Environments\n",
    "def get_env():\n",
    "    from episci.environment_wrappers.tactical_action_adt_env_continuous import CustomADTEnvContinuous\n",
    "    from episci.agents.utils.constants import Agents, RewardType\n",
    "    red_distribution = red_list_train\n",
    "    env_config = {\n",
    "        \"red_distribution\": red_distribution,\n",
    "        \"reward_type\": RewardType.SHAPED\n",
    "    }\n",
    "    return CustomADTEnvContinuous(env_config,action_length=action_length)\n",
    "\n",
    "def get_eval_env():\n",
    "    from episci.environment_wrappers.tactical_action_adt_env_continuous import CustomADTEnvContinuous\n",
    "    from episci.agents.utils.constants import Agents, RewardType\n",
    "    red_distribution = red_list_train\n",
    "    env_config = {\n",
    "        \"red_distribution\": red_distribution,\n",
    "        \"reward_type\": RewardType.SHAPED\n",
    "    }\n",
    "    return CustomADTEnvContinuous(env_config,action_length=action_length_eval)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Workers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rollout Worker\n",
    "class RolloutWorkerClass(object):\n",
    "    \"\"\"\n",
    "    Worker without RAY (for update purposes)\n",
    "    \"\"\"\n",
    "    def __init__(self,hdims=[256,256],actv=tf.nn.relu,\n",
    "                 lr=1e-3,gamma=0.99,alpha_q=0.1,alpha_pi=0.1,polyak=0.995,epsilon=1e-2,\n",
    "                 seed=1):\n",
    "        self.seed = seed\n",
    "        # Each worker should maintain its own environment\n",
    "        import gym\n",
    "        from util import suppress_tf_warning\n",
    "        suppress_tf_warning() # suppress TF warnings\n",
    "        gym.logger.set_level(40) \n",
    "        self.env = get_eval_env()\n",
    "        odim,adim = self.env.observation_space.shape[0],self.env.action_space.shape[0]\n",
    "        self.odim = odim\n",
    "        self.adim = adim\n",
    "        _ = self.env.reset()\n",
    "\n",
    "        # Create SAC model and computational graph \n",
    "        self.model,self.sess = create_sac_model(\n",
    "            odim=self.odim,adim=self.adim,hdims=hdims,actv=actv)\n",
    "        self.step_ops,self.target_init = \\\n",
    "            create_sac_graph(self.model,lr=lr,gamma=gamma,alpha_q=alpha_q,alpha_pi=alpha_pi,\n",
    "                             polyak=polyak,epsilon=epsilon)\n",
    "\n",
    "        # Initialize model \n",
    "        self.FIRST_SET_FLAG = True\n",
    "        tf.set_random_seed(self.seed)\n",
    "        np.random.seed(self.seed)\n",
    "        self.sess.run(tf.global_variables_initializer())\n",
    "        self.sess.run(self.target_init)\n",
    "\n",
    "    def get_action(self,o,deterministic=False):\n",
    "        act_op = self.model['mu'] if deterministic else self.model['pi']\n",
    "        return self.sess.run(act_op, feed_dict={self.model['o_ph']:o.reshape(1,-1)})[0]\n",
    "\n",
    "    def get_weights(self):\n",
    "        \"\"\"\n",
    "        Get weights\n",
    "        \"\"\"\n",
    "        weight_vals = self.sess.run(self.model['main_vars'])\n",
    "        return weight_vals\n",
    "\n",
    "    def set_weights(self,weight_vals):\n",
    "        \"\"\"\n",
    "        Set weights without memory leakage\n",
    "        \"\"\"\n",
    "        if self.FIRST_SET_FLAG:\n",
    "            self.FIRST_SET_FLAG = False\n",
    "            self.assign_placeholders = []\n",
    "            self.assign_ops = []\n",
    "            for w_idx,weight_tf_var in enumerate(self.model['main_vars']):\n",
    "                a = weight_tf_var\n",
    "                assign_placeholder = tf.placeholder(a.dtype, shape=a.get_shape())\n",
    "                assign_op = a.assign(assign_placeholder)\n",
    "                self.assign_placeholders.append(assign_placeholder)\n",
    "                self.assign_ops.append(assign_op)\n",
    "        for w_idx,weight_tf_var in enumerate(self.model['main_vars']):\n",
    "            # Memory-leakage-free assign (hopefully)\n",
    "            self.sess.run(self.assign_ops[w_idx],\n",
    "                          {self.assign_placeholders[w_idx]:weight_vals[w_idx]})\n",
    "\n",
    "@ray.remote\n",
    "class RayRolloutWorkerClass(object):\n",
    "    \"\"\"\n",
    "    Rollout Worker with RAY\n",
    "    \"\"\"\n",
    "    def __init__(self,worker_id=0,hdims=[256,256],actv=tf.nn.relu,\n",
    "                 ep_len_rollout=1000,max_ep_len_eval=1000):\n",
    "        # Parse\n",
    "        self.worker_id = worker_id\n",
    "        self.ep_len_rollout = ep_len_rollout\n",
    "        self.max_ep_len_eval = max_ep_len_eval\n",
    "        # Each worker should maintain its own environment\n",
    "        import gym\n",
    "        from util import suppress_tf_warning\n",
    "        suppress_tf_warning() # suppress TF warnings\n",
    "        gym.logger.set_level(40) \n",
    "        self.env = get_env()\n",
    "        odim,adim = self.env.observation_space.shape[0],self.env.action_space.shape[0]\n",
    "        self.odim = odim\n",
    "        self.adim = adim\n",
    "        _ = self.env.reset()\n",
    "\n",
    "        # Replay buffers to pass\n",
    "        self.o_buffer = np.zeros((self.ep_len_rollout,self.odim))\n",
    "        self.a_buffer = np.zeros((self.ep_len_rollout,self.adim))\n",
    "        self.r_buffer = np.zeros((self.ep_len_rollout))\n",
    "        self.o2_buffer = np.zeros((self.ep_len_rollout,self.odim))\n",
    "        self.d_buffer = np.zeros((self.ep_len_rollout))\n",
    "\n",
    "        # Create SAC model\n",
    "        self.model,self.sess = create_sac_model(\n",
    "            odim=self.odim,adim=self.adim,hdims=hdims,actv=actv)\n",
    "        self.sess.run(tf.global_variables_initializer())\n",
    "        print (\"Ray Worker [%d] Ready.\"%(self.worker_id))\n",
    "\n",
    "        # Flag to initialize assign operations for 'set_weights()'\n",
    "        self.FIRST_SET_FLAG = True\n",
    "\n",
    "        # Flag to initialize rollout\n",
    "        self.FIRST_ROLLOUT_FLAG = True\n",
    "    \n",
    "    def get_action(self,o,deterministic=False,temperature=1.0):\n",
    "        \"\"\"\n",
    "        Get action (if temperature is 0, it becomes deterministic)\n",
    "        \"\"\"\n",
    "        a_mu = self.sess.run(self.model['mu'],\n",
    "                             feed_dict={self.model['o_ph']:o.reshape(1,-1)})[0]\n",
    "        a_pi = self.sess.run(self.model['pi'],\n",
    "                             feed_dict={self.model['o_ph']:o.reshape(1,-1)})[0]\n",
    "        if deterministic:\n",
    "            a = a_mu\n",
    "        else:\n",
    "            a = temperature*a_pi + (1-temperature)*a_mu\n",
    "        return a\n",
    "\n",
    "    def set_weights(self,weight_vals):\n",
    "        \"\"\"\n",
    "        Set weights without memory leakage\n",
    "        \"\"\"\n",
    "        if self.FIRST_SET_FLAG:\n",
    "            self.FIRST_SET_FLAG = False\n",
    "            self.assign_placeholders = []\n",
    "            self.assign_ops = []\n",
    "            for w_idx,weight_tf_var in enumerate(self.model['main_vars']):\n",
    "                a = weight_tf_var\n",
    "                assign_placeholder = tf.placeholder(a.dtype, shape=a.get_shape())\n",
    "                assign_op = a.assign(assign_placeholder)\n",
    "                self.assign_placeholders.append(assign_placeholder)\n",
    "                self.assign_ops.append(assign_op)\n",
    "        for w_idx,weight_tf_var in enumerate(self.model['main_vars']):\n",
    "            # Memory-leakage-free assign (hopefully)\n",
    "            self.sess.run(self.assign_ops[w_idx],\n",
    "                          {self.assign_placeholders[w_idx]:weight_vals[w_idx]})\n",
    "\n",
    "    def rollout(self,temperature=1.0,eps_greedy=0.0):\n",
    "        \"\"\"\n",
    "        Rollout\n",
    "        \"\"\"\n",
    "        if self.FIRST_ROLLOUT_FLAG:\n",
    "            self.FIRST_ROLLOUT_FLAG = False\n",
    "            self.o = self.env.reset() # reset environment\n",
    "        # Loop\n",
    "        r_sum = 0\n",
    "        for t in range(self.ep_len_rollout):\n",
    "            if np.random.rand() < eps_greedy:\n",
    "                self.a = self.env.action_space.sample() # random sample \n",
    "            else:\n",
    "                self.a = self.get_action(self.o,deterministic=False,temperature=temperature)\n",
    "            self.o2,self.r,self.d,_ = self.env.step(self.a)\n",
    "            r_sum += self.r\n",
    "            # Append\n",
    "            self.o_buffer[t,:] = self.o\n",
    "            self.a_buffer[t,:] = self.a\n",
    "            self.r_buffer[t] = self.r\n",
    "            self.o2_buffer[t,:] = self.o2\n",
    "            self.d_buffer[t] = self.d\n",
    "            # Save next state \n",
    "            self.o = self.o2\n",
    "            if self.d: \n",
    "                self.o = self.env.reset() # reset when done \n",
    "        r_avg = r_sum / self.ep_len_rollout\n",
    "        return self.o_buffer,self.a_buffer,self.r_buffer,self.o2_buffer,self.d_buffer,r_avg\n",
    "\n",
    "    def evaluate(self,red=None):\n",
    "        \"\"\"\n",
    "        Evaluate\n",
    "        \"\"\"\n",
    "        o,d,ep_ret,ep_len = self.env.reset(red=red),False,0,0\n",
    "        while not(d or (ep_len == self.max_ep_len_eval)):\n",
    "            a = self.get_action(o,deterministic=True)\n",
    "            \n",
    "            if ep_len == 0:\n",
    "                a_prev = a\n",
    "            else:\n",
    "                a = (ema)*a + (1-ema)*a_prev\n",
    "                a_prev = a\n",
    "            \n",
    "            o,r,d,_ = self.env.step(a) # set\n",
    "            ep_ret += r # accumulate reward\n",
    "            ep_len += 1 # length\n",
    "        blue_health,red_health = self.env.blue_health,self.env.red_health\n",
    "        eval_res = [ep_ret,ep_len,blue_health,red_health] # evaluation result \n",
    "        return eval_res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ray.init(num_cpus=n_cpu)\n",
    "tf.reset_default_graph()\n",
    "R = RolloutWorkerClass(hdims=hdims,actv=actv,\n",
    "                       lr=lr,gamma=gamma,alpha_q=alpha_q,alpha_pi=alpha_pi,\n",
    "                       polyak=polyak,epsilon=epsilon,\n",
    "                       seed=0)\n",
    "workers = [RayRolloutWorkerClass.remote(worker_id=i,hdims=hdims,actv=actv,\n",
    "                                        ep_len_rollout=ep_len_rollout,\n",
    "                                        max_ep_len_eval=max_ep_len_eval)\n",
    "           for i in range(n_workers)]\n",
    "\n",
    "# Replay Buffers\n",
    "replay_buffer_long = ReplayBuffer(odim=R.odim,adim=R.adim,size=int(buffer_sz_long))\n",
    "replay_buffer_short = ReplayBuffer(odim=R.odim,adim=R.adim,size=int(buffer_sz_short))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Restore, if necessary\n",
    "if npz_path_restore:\n",
    "    restore_sac_model_and_buffers(npz_path_restore,R,replay_buffer_long,replay_buffer_short,\n",
    "                                  VERBOSE=False,IGNORE_BUFFERS=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Synchronize worker weights\n",
    "weights = R.get_weights()\n",
    "set_weights_list = [worker.set_weights.remote(weights) for worker in workers] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "ops = []\n",
    "for i_idx in range(num_eval):\n",
    "    worker,red = workers[i_idx],red_list_eval[i_idx]\n",
    "    ops.append(worker.evaluate.remote(red=red)) # eval operation\n",
    "eval_vals = ray.get(ops) # do evaluation here \n",
    "ep_ret_sum,n_eval_sum = 0,0\n",
    "n_outer_loop = 1 # 1 \n",
    "for o_idx in range(n_outer_loop):\n",
    "    for i_idx in range(num_eval):\n",
    "        red,eval_val = red_list_eval[i_idx],eval_vals[i_idx]\n",
    "        ep_ret,ep_len,blue_health,red_health = eval_val[0],eval_val[1],eval_val[2],eval_val[3]\n",
    "        ep_ret_sum += ep_ret\n",
    "        n_eval_sum += 1\n",
    "        print (\" [%d/%d][%d/%d] [%s] ep_ret:[%.4f] ep_len:[%d]. blue health:[%.2f] red health:[%.2f]\"\n",
    "            %(o_idx,n_outer_loop,i_idx,len(eval_vals),red,ep_ret,ep_len,blue_health,red_health))\n",
    "ep_ret_avg = ep_ret_sum / n_eval_sum\n",
    "print (\"[Eval. done] time:[%s] ep_ret_avg:[%.3f].\\n\"%\n",
    "       (time.strftime(\"day:[%d] %H:%M:%S\", time.gmtime(time.time()-start_time)),\n",
    "        ep_ret_avg)\n",
    "      )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Close"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ray.shutdown()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
